{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14b20508",
   "metadata": {},
   "source": [
    "# Exercises: Fitting a Linear Model with Gradient Descent\n",
    "\n",
    "This notebook is a very important one.\n",
    "We will return to our favorite problem: modeling the relationship between an NBA player's height and his wingspan using a linear model. \n",
    "\n",
    "Before, we found that we could exactly solve for the model parameters (i.e. the slope and y-intercept) that minimize the squared-residuals between our recorded data and our model's predictions. Now, we will act as if no such analytic solution exists, since this will almost always be the case in \"real world\" problems. Instead, we will use gradient descent to tune the parameters of our linear model, and we will do this by leveraging MyGrad's autodiff capabilities to compute the relevant gradients for this optimization process. The procedure that we exercise here will turn out to be almost exactly identical to the process for \"training a neural network\" using \"supervised learning\", which are concepts that we will dive into ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9c4ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to import the necessary libraries\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import mygrad as mg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8fb979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the NetCDF-4 file `./data/nba_draft_measurements.nc` as an xarray-dataset\n",
    "# (refer to the previous exercise notebook if you need a refresher on this)\n",
    "draft_data = xr.load_dataset(Path.cwd() / \"data\" / \"nba_draft_measurements.nc\")  # <COGSTUB>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861383be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A scatter plot that shows wingspan versus height (without shoes) for the players\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "draft_data.plot.scatter(x=\"height_no_shoes\", y=\"wingspan\")\n",
    "ax.grid()\n",
    "ax.set_xlabel(\"Height (without shoes) [inches]\")\n",
    "ax.set_ylabel(\"Wingspan [inches]\");\n",
    "ax.set_title(\"Wingspan vs Height for NBA Draftees from 2019\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f388f773",
   "metadata": {},
   "source": [
    "## Reviewing Our Modeling Problem\n",
    "\n",
    "Based on the relationship between height and wingspan that we visualized above, we want to define a **linear mathematical model that can predict an individual's wingspan based on their height**.\n",
    "\n",
    "Our recorded data consists of $N$ measurements: $\\big(x_n, y^{\\mathrm{(true)}}_n\\big)_{n=0}^{N-1}$.\n",
    "Datum-$i$, $\\big(x_i, y^{\\mathrm{(true)}}_i\\big)$,  is the height and wingspan of player-$i$.\n",
    "We use the \"true\" superscript label here in anticipation of the fact that we will need to distinguish these measured wingspans from our model's predicted wingspans.\n",
    "Supposing we have some values picked out for our model's parameters, $m$ and $b$, then our model's predicted wingspan for player-$i$ is\n",
    "\n",
    "\\begin{equation}\n",
    "y^{\\mathrm{(pred)}}_i = F(m, b; x_i) = m x_i + b\n",
    "\\end{equation}\n",
    "\n",
    "Our goal is to find appropriate values for our model's parameters, $m$ and $b$, such that $F(m, b; x)$ will make reliable predictions about the wingspans of players whose measurements are not in our dataset.\n",
    "The way that we will measure the quality of our model's predictions is via the loss function (a.k.a objective function) that computes the mean squared-error (a.k.a squared residuals) of our predictions compared to our recorded data.\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathscr{L}_{\\mathrm{MSE}} = \\frac{1}{N}\\sum_{n=0}^{N-1}{\\big(y^{\\mathrm{(true)}}_n - y^{\\mathrm{(pred)}}_n\\big)^2}\n",
    "\\end{equation}\n",
    "\n",
    "**To find the values of** $m$ **and** $b$ **that minimize** $\\mathscr{L}_{\\mathrm{MSE}}$ **is to produce the best-fit linear model - the one that minimizes empirical risk - for this dataset.**\n",
    "We will denote these optimal model parameter values as $m^*$ and $b^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b4f708",
   "metadata": {},
   "source": [
    "The following function computes values of $m^*$ and $b^*$ – the ideal slope and y-intercept for our linear model – based on the particular values of our dataset, $\\big(x_n, y^{\\mathrm{(true)}}_n\\big)_{n=0}^{N-1}$.\n",
    "\n",
    "We will be making use of this analytical solution so that we can precisely measure how far our approximate solution is away from the ideal; keep in mind that for most problems we will not have access to an exact solution like this.\n",
    "You already coded up `ordinary_least_squares` in the Data Exploration notebook; copy & paste your solution from that notebook below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8143b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordinary_least_squares(x, y):\n",
    "    \"\"\"\n",
    "    Computes the slope and y-intercept for the line that minimizes\n",
    "    the sum of squared residuals of mx + b and y, for the observed data\n",
    "    (x, y).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : numpy.ndarray, shape-(N,)\n",
    "        The independent data. At least two distinct pieces of data\n",
    "        are required.\n",
    "\n",
    "    y : numpy.ndarray, shape-(N,)\n",
    "        The dependent data in correspondence with ``x``.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (m, b) : Tuple[float, float]\n",
    "        The optimal values for the slope and y-intercept\n",
    "    \"\"\"\n",
    "    # Refer to the Data Exploration notebook. Copy/paste your solution here\n",
    "    # <COGINST>\n",
    "    N = x.size\n",
    "    m = (x @ y - x.sum() * y.sum() / N) / (x @ x - (1 / N) * x.sum() ** 2)\n",
    "    b = y.mean() - m * x.mean()\n",
    "    return m, b\n",
    "    # </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a6726f",
   "metadata": {},
   "source": [
    "Using this function, compute the parameters, $m^*$ and $b^*$, that minimize the MSE of a linear model for our dataset, where $x$ corresponds to \"height\" (without shoes) and $y$ corresponds to wingspan.\n",
    "We will want to access the underlying NumPy arrays from the xarray data so that you can work with the \"raw data\" conveniently here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6141f8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute m* and b* that best fits the wingspan vs height (no shoes) data\n",
    "\n",
    "# Access the underlying numpy arrays  (recall that `.data` returns the numpy array of an xarray)\n",
    "height = draft_data.height_no_shoes.data  # <COGSTUB> access the players' heights (no shoes) from `draft_data`\n",
    "wingspan = draft_data.wingspan.data  # <COGSTUB> access the player's wingspans from `draft_data`\n",
    "\n",
    "\n",
    "# Compute the ideal parameter values for our linear model, based on this data\n",
    "m, b = ordinary_least_squares(height, wingspan) # <COGSTUB>  compute m* and w* for our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0071a6",
   "metadata": {},
   "source": [
    "Next, plot the data for this problem along with the resulting fitted linear model.\n",
    "Use `ax.scatter` to plot the original data and `ax.plot` to draw the model line; \n",
    "you will want to specify a distinct color for your linear model.\n",
    "Label your axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca6bb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot wingspan vs height (no shoes) as a scatter plot\n",
    "# Plot the line: m* x + b*\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(height, wingspan)  # plots our raw data\n",
    "\n",
    "min_height = height.min()  # <COGSTUB> compute the smallest height\n",
    "max_height = height.max()  # <COGSTUB> compute the largest height\n",
    "\n",
    "\n",
    "x_values = np.linspace(min_height, max_height, 1000)  # <COGSTUB> use np.linspace to make a shape-(1,000) array of values evenly spaces across [min_height, max_height] \n",
    "y_values = m * x_values + b  # <COGSTUB> use `x_values`, `m`, and `b`, and compute the corresponding y-values for our line\n",
    "\n",
    "ax.plot(x_values, y_values, c=\"red\")\n",
    "\n",
    "ax.grid(True)\n",
    "ax.set_xlabel(\"Height [inches]\")\n",
    "ax.set_ylabel(\"Wingspan [inches]\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1683b89",
   "metadata": {},
   "source": [
    "The following function creates a surface plot of $\\mathscr{L}_{\\mathrm{MSE}}(m, b)$ over a range of $(m, b)$ values, given a user-specified data set.\n",
    "Take a moment to read its docstring and then run this cell to define the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73bd96e",
   "metadata": {},
   "source": [
    "**You don't need to modify the following function at all. Just read through the docstring and run this cell**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be2ee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You don't need to modify this function at all. Just read\n",
    "# through the docstring and run this cell \n",
    "\n",
    "def graph_linear_regression_mse(\n",
    "    x,\n",
    "    y,\n",
    "    trajectory=None,\n",
    "    m_scale=10,\n",
    "    b_scale=10,\n",
    "    sample_density=500,\n",
    "):\n",
    "    \"\"\"\n",
    "    Given the data `x, y`, plots the mean squared-error (MSE) surface on the space of \n",
    "    possible slope and y-intercept values for a linear regression model.\n",
    "\n",
    "    The plot is automatically centered at the optimal parameter values (m*, b*); this\n",
    "    point is demarcated with a black dot.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray, shape-(N,)\n",
    "        The x data from your dataset\n",
    "\n",
    "    y : np.ndarray, shape-(N,)\n",
    "        The y data from your dataset\n",
    "\n",
    "    trajectory : Optional[np.ndarray], shape-(T, 2) or shape-(N, T, 2)\n",
    "        One or more length-T sequence of (slope, intercept) points to superimpose over the surface.\n",
    "        This can be used to display a \"trajectory\" of parameter values.\n",
    "\n",
    "    m_scale : int, optional (default=10)\n",
    "        The size of the range of slopes that are plotted in each direction\n",
    "\n",
    "    b_scale : int, optional (default=10)\n",
    "        The size of the range of y-intercepts that are plotted in each direction\n",
    "\n",
    "    sample_density : int, optional (default=500)\n",
    "        The number of samples to calculate along each axis. Decreasing this speeds\n",
    "        up the plot at the cost of visual quality.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[Figure, Axis]\n",
    "        Returns the matplotlib figure and axis that was created so\n",
    "        that the plot can be further manipulated or saved. \n",
    "    \"\"\"\n",
    "\n",
    "    def mse(x, y, m, b):\n",
    "        \"\"\"Computes the mean squared-error (MSE)\"\"\"\n",
    "        m = np.atleast_1d(m)\n",
    "        b = np.atleast_1d(b)\n",
    "        return ((x * m[None] + b[None] - y) ** 2).mean(axis=1)\n",
    "\n",
    "    # find least squares solution\n",
    "    A = np.vstack([x, np.ones(len(x))]).T\n",
    "    m_opt, b_opt = np.linalg.lstsq(A, y, rcond=None)[0]\n",
    "    l_opt = mse(x, y, m_opt, b_opt)\n",
    "\n",
    "    center_m = m_opt\n",
    "    center_b = b_opt\n",
    "\n",
    "    # Creates the plot figure\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "    # Plot the local minimum of the MSE surface as a black dot\n",
    "    ax.plot(\n",
    "        [m_opt],\n",
    "        [b_opt],\n",
    "        l_opt,\n",
    "        c=\"black\",\n",
    "        marker=\"o\",\n",
    "        zorder=3,\n",
    "        markersize=7,\n",
    "    )\n",
    "\n",
    "    # Define quadratic surface of MSE landscape over m and b\n",
    "    m_series = np.linspace(center_m - m_scale, center_m + m_scale, sample_density)\n",
    "    b_series = np.linspace(\n",
    "        center_b - b_scale, center_b + b_scale, sample_density\n",
    "    ).reshape(-1, 1)\n",
    "\n",
    "    Z = (b_series + x.reshape(-1, 1, 1) * m_series) - y.reshape(-1, 1, 1)\n",
    "\n",
    "    Z = np.mean(Z ** 2, axis=0)\n",
    "\n",
    "    # make surface plot\n",
    "    m_series, b_series = np.meshgrid(m_series, b_series)\n",
    "    ax.set_xlabel(\"Slope: m\")\n",
    "    ax.set_ylabel(\"Intercept: b\")\n",
    "    ax.set_zlabel(\"MSE Loss\")\n",
    "    ax.ticklabel_format(style=\"sci\", scilimits=(-1, 2))\n",
    "    ax.dist = 11\n",
    "    surf = ax.plot_surface(m_series, b_series, Z, cmap=plt.get_cmap(\"GnBu\"))\n",
    "\n",
    "    # Graphs one or more trajectories on the loss londscape\n",
    "    if trajectory is not None:\n",
    "        trajectories = np.atleast_2d(trajectory)\n",
    "        if trajectories.ndim == 2:\n",
    "            trajectories = trajectories[np.newaxis]\n",
    "        for trajectory in trajectories:\n",
    "            m_values, b_values = trajectory.T\n",
    "            l_values = ((x * m_values[:, None] + b_values[:, None] - y) ** 2).mean(\n",
    "                axis=1\n",
    "            )\n",
    "            ax.plot(\n",
    "                m_values,\n",
    "                b_values,\n",
    "                l_values,\n",
    "                marker=\"*\",\n",
    "                zorder=3,\n",
    "                markersize=7,\n",
    "            )\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1099dc0",
   "metadata": {},
   "source": [
    "Here we use `graph_linear_regression_mse` for our dataset.\n",
    "\n",
    "\n",
    "The ideal solution will appear as a black dot, and our model's slope/intercept values will be a orange start.\n",
    "Because we used `ordinary_least_squares` to compute `m` and `b`, the orange star should match the black dot perfectly.\n",
    "\n",
    "Note that you can click-and-drag your cursor over the plot to rotate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1da5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "\n",
    "# plotting the linear regression MSE surface with your solution point included\n",
    "\n",
    "graph_linear_regression_mse(height, wingspan, trajectory=[m, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf99ae3",
   "metadata": {},
   "source": [
    "This surface is our so-called **loss landscape**.\n",
    "It represents the values of $\\mathscr{L}_{\\mathrm{MSE}}$ over a continuum of $(m, b)$ parameter values.\n",
    "That is, given our fixed dataset, this depicts how well (or poorly) the various linear models of differing slopes and y-intercepts would fit the data.\n",
    "The ideal model is the one whose parameter values reside at the bottom of this loss-landscape.\n",
    "Typically we will use gradient descent to search for these ideal values that minimize $\\mathscr{L}$; as such, that is what we will be doing in the rest of this notebook.\n",
    "\n",
    "Before we proceed, note that our loss landscape looks much more like a taco shell (or a half pipe) than it does like a bowl, even though our equation for $\\mathscr{L}_{\\mathrm{MSE}}$ is quadratic in $m$ and $b$ (and 2D quadratic surfaces generally look like bowls).\n",
    "Even though the landscape looks flat along $b$, it is actually sloping upward away from the black dot; the slope is simply far more mild along the $b$ direction than it is along the $m$ direction.\n",
    "How might this affect the search for the minimum on this surface via gradient descent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc3622b",
   "metadata": {},
   "source": [
    "## Estimating Optimal Model Parameters Using Gradient Descent\n",
    "\n",
    "In keeping with our [discussion of gradient-based learning](https://rsokl.github.io/CogWeb/Video/Gradient_Descent.html) we will arrive at near-optimal values for our model's parameters by utilizing gradient descent.\n",
    "\n",
    "We will start this process by crudely drawing (small) random values for $m$ and $b$; these will determine where we first reside on the loss landscape.\n",
    "\n",
    "\\begin{align}\n",
    "m_{old}, b_{old} \\leftarrow \\mathrm{randomly\\;sampled\\;values}\n",
    "\\end{align}\n",
    "\n",
    "Next we'll descend this loss landscape by iteratively updating the values for $m$ and $b$ with the gradient-based step\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{bmatrix}m_\\text{new} \\\\ b_\\text{new} \\end{bmatrix} &= \\begin{bmatrix} m_\\text{old} \\\\ b_\\text{old} \\end{bmatrix} - \\delta \\vec{\\nabla} \\mathscr{L}_{MSE} (m_{\\text{old}}, b_{\\text{old}})\\\\\n",
    "   &\\Downarrow(\\mathrm{equivalent\\;to...})\\\\\n",
    "   m_\\text{new} &= m_\\text{old} - \\delta \\frac{\\mathrm{d}\\mathscr{L}_{MSE}}{\\mathrm{d} m}\\big|_{m_{old}, b_{old}}\\\\\n",
    "   b_\\text{new} &= b_\\text{old} - \\delta \\frac{\\mathrm{d}\\mathscr{L}_{MSE}}{\\mathrm{d} b}\\big|_{m_{old}, b_{old}}\\\\\n",
    "\\end{align}\n",
    "\n",
    "where $\\delta$ is the learning rate - a single, positive valued number that we have to pick.\n",
    "\n",
    "Keep in mind that we will be able to leverage automatic differentiation, via MyGrad, to compute $\\vec{\\nabla} \\mathscr{L}_{MSE} (m_{\\text{old}}, b_{\\text{old}})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57ef349",
   "metadata": {},
   "source": [
    "Complete the following function that performs a gradient-step on all of the supplied parameters.\n",
    "Note that this was a reading comprehension task - \"Writing a Generic Gradient-Update Function\" - in [a previous section](https://rsokl.github.io/CogWeb/Video/Automatic_Differentiation.html#Gradient-Descent-with-MyGrad).\n",
    "You can refer back to the solution at the bottom of the page for assistance with this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8904a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and paste the solution that you wrote in Autodiff & Grad_Descent\n",
    "\n",
    "def gradient_step(tensors, learning_rate):\n",
    "    \"\"\"\n",
    "    Performs gradient-step in-place on each of the provides tensors \n",
    "    according to the standard formulation of gradient descent.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tensors : Union[Tensor, Iterable[Tensors]]\n",
    "        A single tensor, or an iterable of an arbitrary number of tensors.\n",
    "\n",
    "        If a `tensor.grad` is `None`for a specific tensor, the update on\n",
    "        that tensor is skipped.\n",
    "\n",
    "    learning_rate : float\n",
    "        The \"learning rate\" factor for each descent step. A positive number.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The gradient-steps performed by this function occur in-place on each tensor,\n",
    "    thus this function does not return anything\n",
    "    \"\"\"\n",
    "    # <COGINST>\n",
    "    if isinstance(tensors, mg.Tensor):\n",
    "        # Only one tensor was provided. Pack\n",
    "        # it into a list so it can be accessed via\n",
    "        # iteration\n",
    "        tensors = [tensors]\n",
    "\n",
    "    for t in tensors:\n",
    "        if t.grad is not None:\n",
    "            t.data -= learning_rate * t.grad\n",
    "    # </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d343dd9",
   "metadata": {},
   "source": [
    "Now let's code up $\\mathscr{L}_{MSE}$ in MyGrad. Recall that the equation for $\\mathscr{L}_{MSE}$ is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathscr{L}_{\\mathrm{MSE}} = \\frac{1}{N}\\sum_{n=0}^{N-1}{\\big(y^{\\mathrm{(true)}}_n - y^{\\mathrm{(pred)}}_n\\big)^2}\n",
    "\\end{equation}\n",
    "\n",
    "We want to leverage [vectorized math functions](http://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/VectorizedOperations.html) to ensure that this function runs efficiently. Consider referencing [these functions](https://mygrad.readthedocs.io/en/latest/math.html#sums-products-differences) in particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84987da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the following function\n",
    "\n",
    "import mygrad as mg\n",
    "import numpy as np\n",
    "\n",
    "def mean_squared_error_mygrad(y_pred, y_true):\n",
    "    \"\"\" Computers the mean-squared error for a collection of predictions\n",
    "    and corresponding true values.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred : mygrad.Tensor, shape-(N,)\n",
    "        A tensor of N predictions.\n",
    "\n",
    "    y_true : array_like, shape-(N,)\n",
    "        An array of N corresponding true values\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mse : mygrad.Tensor, shape-()\n",
    "        A scalar-tensor containing the mean squared-error.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> pred = mg.tensor([1., 2., 3.])\n",
    "    >>> true = mg.tensor([1., 1., 3.])\n",
    "    >>> mean_squared_error_mygrad(pred, true)\n",
    "    Tensor(0.33333333)\n",
    "    \"\"\"\n",
    "    # This code should only use vectorized operations\n",
    "\n",
    "    differences = (y_pred - y_true)  # <COGSTUB> Compute the difference between the predicted values and true values\n",
    "    squared_differences = differences ** 2 # <COGSTUB> Square each difference\n",
    "    mean_of_squared_differences = np.mean(squared_differences) # <COGSTUB> Compute mean of the squared-differences\n",
    "    \n",
    "    # Return mean_of_squared_differences\n",
    "    return mean_of_squared_differences  # <COGLINE> # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8d663e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your code\n",
    "\n",
    "# Create a test case where `y_pred` and `y_true` match perfectly.\n",
    "# What should `mean_squared_error_mygrad` return in this case?\n",
    "# Check that `mean_squared_error_mygrad` performs as-expected\n",
    "\n",
    "y_pred = mg.tensor([1.0, 2.0, 3.0])  # <COGSTUB>\n",
    "y_true = y_pred  # <COGSTUB>\n",
    "\n",
    "print(\"MSE for perfect match:\", mean_squared_error_mygrad(y_pred, y_true))\n",
    "\n",
    "# Create a test case where:\n",
    "# - `y_true` is tensor([0., 0.])\n",
    "# - `y_pred` is tensor([1.0, -1.0])\n",
    "#\n",
    "# What should `mean_squared_error_mygrad` return in this case? Compute it by-hand.\n",
    "# Check that `mean_squared_error_mygrad` performs as-expected\n",
    "\n",
    "y_pred = mg.tensor([0.0, 0.0])  # <COGSTUB>\n",
    "y_true = mg.tensor([1.0, -1.0])  # <COGSTUB>\n",
    "\n",
    "print(\"MSE for [0, 0] vs [1, -1]:\", mean_squared_error_mygrad(y_pred, y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cad7949",
   "metadata": {},
   "source": [
    "## Defining Our Linear Model\n",
    "\n",
    "We will now [create a class](https://www.pythonlikeyoumeanit.com/module_4.html) used to encapsulate the parameters and functionality of our linear model.\n",
    "Although this might seem to be excessive here, this will prime us for creating more sophisticated models (e.g. neural networks) later on.\n",
    "\n",
    "This class will be responsible for:\n",
    "\n",
    "- drawing random values to initialize `m` and `b`; we will use the [`uniform`](https://mygrad.readthedocs.io/en/latest/generated/mygrad.nnet.initializers.uniform.html) function to draw these values.\n",
    "- storing our model parameters `m` and `b` and making them easily accessible\n",
    "- defining the so-called \"forward pass\" of our model: passing data to it and returning predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae56a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use `uniform` to draw random initial values for our model's parameters\n",
    "# After running this cell, use <SHIFT>+<TAB> to look up the documentation\n",
    "# for `uniform`\n",
    "from mygrad.nnet.initializers import uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0f36c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel:\n",
    "    \"\"\"\n",
    "    A linear model with parameters `self.m` and `self.b`\n",
    "    \"\"\"\n",
    "    \n",
    "    def initialize_params(self):\n",
    "        \"\"\"\n",
    "        Uses `mygrad.nnet.initializers.uniform` to draw tensor\n",
    "        values for both `self.m` and `self.b` from the uniform \n",
    "        distribution [-10, 10].\n",
    "        \n",
    "        Both parameters should be shape-(1,) tensors; the call:\n",
    "        \n",
    "           uniform(1, lower_bound=-10, upper_bound=10)\n",
    "        \n",
    "        will draw a shape-(1,) tensor from this distribution\n",
    "        \"\"\"\n",
    "        # Using the function `uniform`, draw a shape-(1,) tensor from \n",
    "        # the domain [-10, 10].\n",
    "        # Do this for both m and b, respectively.\n",
    "        self.m = uniform(1, lower_bound=-10, upper_bound=10)  # <COGSTUB>\n",
    "        self.b = uniform(1, lower_bound=-10, upper_bound=10)  # <COGSTUB>\n",
    "        \n",
    "    def __init__(self, m=None, b=None):\n",
    "        \"\"\" Accepts initial values for m and b. If either are not\n",
    "        specified, uses `self.initialize_params()` to draw them\n",
    "        randomly\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        m : Optional[mygrad.Tensor], shape-(1,)\n",
    "            The slope for the linear model. If `None`, a random\n",
    "            value is drawn.\n",
    "            \n",
    "        b : Optional[mygrad.Tensor], shape-(1,)\n",
    "            The y-intercept for the linear model. If `None`, a random\n",
    "            value is drawn.\n",
    "        \"\"\"\n",
    "        \n",
    "        #\n",
    "        # If either parameter is provided as an input to this method, \n",
    "        # use that specified value to overwrite the randomly drawn value\n",
    "        #\n",
    "        # `self.m` and `self.b` must be defined by this method\n",
    "                \n",
    "        # Use `self.initialize_params()` to draw random values for `self.m` and `self.b`. \n",
    "        self.initialize_params()  # <COGLINE>\n",
    "        \n",
    "        # If `m` is not `None, then assign it to `self.m`\n",
    "        # If `b` is not `None, then assign it to `self.b`\n",
    "        # <COGINST>\n",
    "        if m is not None:\n",
    "            self.m = m\n",
    "\n",
    "        if b is not None:\n",
    "            self.b = b\n",
    "        # </COGINST>\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Performs: m * x + b\n",
    "        \n",
    "        This is known as a 'forward pass' through the model\n",
    "        on the specified data. I.e. uses the linear model to\n",
    "        make a prediction based on the input `x`.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : array_like, shape-(N,)\n",
    "            An array or tensor of N observations.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        prediction : mygrad.Tensor, shape-(N,)\n",
    "            A corresponding tensor of N predictions based on\n",
    "            the linear model.\n",
    "        \"\"\"\n",
    "        # Using your model's parameters (`self.m` and `self.b`) and x, compute \n",
    "        # the line's output y\n",
    "        \n",
    "        # <COGINST>\n",
    "        y = self.m * x + self.b\n",
    "        return y\n",
    "        # </COGINS>\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        \"\"\" Returns a tuple of the tensors associated with the model's\n",
    "        parameters.\n",
    "        \n",
    "        This is accessed as an attribute, via `model.parameters`\n",
    "        *not* as a method (i.e. not as `model.parameters()`)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[Tensor, Tuple]\n",
    "            A tuple containing all of the learnable parameters for our model.\n",
    "            \n",
    "            This should return a tuple containing the slope and y-intercept \n",
    "            associated with the model.\n",
    "        \n",
    "        Examples\n",
    "        --------\n",
    "        >>> model = LinearModel()\n",
    "        >>> model.parameters\n",
    "        (Tensor([-7.714269], dtype=float32), Tensor([-6.770146], dtype=float32))\n",
    "        \"\"\"\n",
    "        # return `self.m` and `self.b` as a tuple\n",
    "        return (self.m, self.b)  # <COGLINE>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe40c2fb",
   "metadata": {},
   "source": [
    "Try initializing your model, `model = LinearModel()` and check that:\n",
    "\n",
    "- `model.m` and `model.b` are both defined and are shape-(1,) mygrad tensors\n",
    "- `model.parameters` returns both tensors\n",
    "- Calling `model(1.)` returns a mygrad tensor corresponding to: m * x + b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d10c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking that your model class is working as-expected\n",
    "\n",
    "# <COGINST>\n",
    "model = LinearModel()  # Initialize your linear model without specifying any input parameters.\n",
    "\n",
    "assert isinstance(model.m, mg.Tensor) and model.m.shape == (1,)\n",
    "\n",
    "assert isinstance(model.b, mg.Tensor) and model.b.shape == (1,)\n",
    "\n",
    "assert model.parameters == (model.m, model.b)\n",
    "\n",
    "output = model(1)\n",
    "assert isinstance(output, mg.Tensor) and output.size == 1\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae46193",
   "metadata": {},
   "source": [
    "## \"Training\" Our Model\n",
    "\n",
    "We will now use gradient descent to optimize our model's parameter values based on our recorded data.\n",
    "In the parlance of modern machine learning, this process is typically described as us \"training\" our model.\n",
    "And to introduce more terminology, the pattern of machine learning that we are about to invoke is called **supervised learning**: it is the process of training (updating) our model based off of collected data where we have access to the desired predictions that we want our model to make. That is, for each recorded height that we are going to feed to our model, we have an associated true wingspan that we measured and that we want our model to predict.\n",
    "\n",
    "### Using Un-Normalized Data (This Won't Work Well)\n",
    "\n",
    "To start off, we will attempt to search for ideal model parameters by processing our raw data (we will find that this works only moderately well and that we ought to pre-process our data before using it to train our model).   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faf0da9",
   "metadata": {},
   "source": [
    "**Run the following cell without modifying it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b45b3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell defines a convenience function for measuring the\n",
    "# distance between (m, b) and (m*, b*)\n",
    " \n",
    "true_params = np.array(ordinary_least_squares(height, wingspan))\n",
    "\n",
    "def dist_from_true(model_params, true_params) -> float:\n",
    "    \"\"\" Computes sqrt[(m - m*)^2 + (b - b*)^2]\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model_params : Tuple[Tensor, Tensor]\n",
    "        m and b\n",
    "    \n",
    "    true_params : numpy.ndarray, shape-(2,)\n",
    "        m* and b*\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The L2 distance between the parameters\"\"\"\n",
    "    params = np.array([i.item() for i in model_params])\n",
    "    return np.sqrt(np.sum((true_params - params) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990b2ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "heights = draft_data.height_no_shoes.data  # our observed data: x [inches]\n",
    "wingspans = draft_data.wingspan.data       # our observed data: y [inches]\n",
    "\n",
    "model = LinearModel()  # <COGSTUB> Initialize your linear model without specifying any input parameters.\n",
    "\n",
    "\n",
    "# `trajectory` is a list that will store the sequence of estimated \n",
    "# model parameters that we compute during gradient descent:\n",
    "#           [(m0, b0), (m1, b1), ... ]\n",
    "# Keep this as an empty list, it will be updated later.\n",
    "trajectory = []  \n",
    "\n",
    "\n",
    "# This is the number of times we will process the observed data\n",
    "# and perform an update our model's parameters\n",
    "#\n",
    "# An \"epoch\" denotes our having processed the dataset in its\n",
    "# entirety once. Thus we will train our model by processing \n",
    "# our dataset in full`num_epoch` times.\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "# The learning rate used for gradient descent. \n",
    "#\n",
    "# This is a value that we need to choose. The following is simply\n",
    "# an educated guess of a good learning rate; there is a whole art\n",
    "# to making educated guesses and well-informed choices for learning\n",
    "# rates, which we will discuss later. The key thing to note here is that\n",
    "# there was no principled reason behind our picking this *exact* value\n",
    "learning_rate = 1E-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9804bd1",
   "metadata": {},
   "source": [
    "The following cell initialized a plot using the [noggin library](https://github.com/rsokl/noggin).\n",
    "This library is capable of logging measurements taken from an experiment and plot them in real time.\n",
    "Here, we tell noggin that we want to track four measured \"metrics\"\n",
    "\n",
    "- `\"loss\"`: the value of $\\mathscr{L}_{MSE}$ for our current model parameters\n",
    "- `\"m:`: the current slope of our model\n",
    "- `\"b:`: the current y-intercept of our model\n",
    "- `\"dist_from_target:`: the Euclidean distance of our model's $(m, b)$ from the optimal $(m^*, b^*)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8b1044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The plot created here will update in real time as we run our experiment\n",
    "# Run this cell\n",
    "\n",
    "from noggin import create_plot\n",
    "\n",
    "# Four \"metrics\" will be tracked by the plotter\n",
    "plotter, fig, ax = create_plot([\"loss\", \"m\", \"b\", \"dist_from_target\"], ncols=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f86c493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code in this cell will be responsible for training our model\n",
    "\n",
    "# Fill out the following code blocks and then run this cell.\n",
    "# View the noggin plot above to see the recorded metrics\n",
    "\n",
    "for n in range(num_epochs):\n",
    "    # Perform a \"forward pass\" with your model on the *full* set of heights\n",
    "    # I.e. feed the model all N heights to produce N corresponding predictions\n",
    "    # using your linear model\n",
    "    y_pred = model(heights)  # <COGSTUB>\n",
    "    \n",
    "    # Compute the mean-squared error of your model's predictions compared\n",
    "    # to the true wingspans, using your loss function. \n",
    "    # (Recall that you defined this function earlier).\n",
    "    loss = mean_squared_error_mygrad(y_pred, wingspans) # <COGSTUB>\n",
    "    \n",
    "    # Invoke \"back-propagation\" from the computed loss.\n",
    "    # This will use mygrad's auto-differentiation abilities to compute the derivatives\n",
    "    # of the loss with respect to your model's parameters.\n",
    "\n",
    "    loss.backward()  # <COGLINE>\n",
    "    \n",
    "    \n",
    "    # This feeds our four measured metrics to noggin to be logged and plotted\n",
    "    # DON'T CHANGE THIS\n",
    "    plotter.set_train_batch(\n",
    "        dict(\n",
    "            loss=loss,\n",
    "            m=model.m.item(),\n",
    "            b=model.b.item(),\n",
    "            dist_from_target=dist_from_true(model.parameters, true_params),\n",
    "        ),\n",
    "        batch_size=len(y_pred),\n",
    "    )\n",
    "    \n",
    "    # Appends the current model params to the \"trajectory\" list\n",
    "    # Don't change this.\n",
    "    trajectory.append((model.m.item(), model.b.item()))\n",
    "    \n",
    "    # Perform a single update to your model's parameters using gradient descent.\n",
    "    #\n",
    "    # Access your model's parameters as a tuple (refer back to `LinearModel` to see how to do this).\n",
    "    # Pass these parameters and the learning rate to `gradient_step`.\n",
    "    # Recall that `gradient_step` doesn't return anything; it updates your model parameters in-place.\n",
    "\n",
    "    gradient_step(model.parameters, learning_rate=learning_rate)  # <COGLINE>\n",
    "    \n",
    "    # Now your model's parameters have been \"nudged\" slightly, via the gradient-descent step so as \n",
    "    # to improve its predictions on this dataset (i.e. to help *minimize* the loss).\n",
    "\n",
    "# This ensures that noggin plots any lingering measurements\n",
    "plotter.plot()\n",
    "\n",
    "# When you run this cell, the noggin plot above will update in real time.\n",
    "# Study the plot: \n",
    "#  - Does the loss go down?\n",
    "#  - Do the values of `m` and `b` converge?\n",
    "#  - Does the distance from the target go to 0? (it likely won't this time.. we'll see why)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785341a1",
   "metadata": {},
   "source": [
    "What does the graph of `dist_from_target` tell us about our approximate solution?\n",
    "Let's print out the last value of this metric associated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4b9aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints the distance between our approximate solution and the exact one\n",
    "train_metrics = plotter.to_xarray(\"train\").batch\n",
    "\n",
    "print(\"distance from target:\", round(train_metrics.dist_from_target[-1].item(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf338266",
   "metadata": {},
   "source": [
    "Let's visualize the \"trajectory\" of our model's parameter values – depicting how they evolved throughout \"training\".\n",
    "Does your the terminus of the trajectory end near the optimal solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78ea59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_linear_regression_mse(height, wingspan, trajectory=trajectory);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7b18a2",
   "metadata": {},
   "source": [
    "Let's see how well our learned model matches our data.\n",
    "The following plot will compare your learned model against the ideal model, derived from the closed-form solution to the least-squares problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c2f00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(height, wingspan)\n",
    "m, b = ordinary_least_squares(height, wingspan)\n",
    "\n",
    "x = np.linspace(height.min(), height.max(), 1000)\n",
    "\n",
    "ax.plot(x, model.m * x + model.b, c=\"orange\", label=\"Learned Fit\")\n",
    "ax.plot(x, m * x + b, c=\"red\", ls=\"--\", label=\"Ideal Fit\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "ax.set_xlabel(\"Height [inches]\")\n",
    "ax.set_ylabel(\"Wingspan [inches]\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed43ba1",
   "metadata": {},
   "source": [
    "You might see that the learned model does not match the least-squares solutions so closely (if it does match, then you got lucky! Try training the model again).\n",
    "It may match the model near the center of the data, but extrapolating outward would reveal discrepancies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe5d3e2",
   "metadata": {},
   "source": [
    "Let's see just how varied our models will be.\n",
    "The following will train an \"ensemble\" of linear models in identical fashions - but with different randomly-drawn parameters.\n",
    "It will then plot the trajectory associated with each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664c44ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an ensemble of linear models on our data\n",
    "# and plot their trajectories.\n",
    "#\n",
    "# Run this cell\n",
    "\n",
    "height = draft_data.height_no_shoes.data\n",
    "wingspan = draft_data.wingspan.data\n",
    "\n",
    "num_models = 10\n",
    "trajectories = [[] for i in range(num_models)]\n",
    "models = [LinearModel() for i in range(num_models)]\n",
    "\n",
    "num_epochs = 10\n",
    "step_size = 1e-4\n",
    "\n",
    "for n in range(num_epochs):\n",
    "    for model_id, model in enumerate(models):\n",
    "        y_pred = model(height)\n",
    "        loss = mean_squared_error_mygrad(y_pred, wingspan)\n",
    "        loss.backward()\n",
    "\n",
    "        trajectories[model_id].append((model.m.item(), model.b.item()))\n",
    "        gradient_step(model.parameters, learning_rate=learning_rate)\n",
    "\n",
    "trajectories = np.array(trajectories)\n",
    "graph_linear_regression_mse(height, wingspan, trajectory=trajectories)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(height, wingspan)\n",
    "m, b = ordinary_least_squares(height, wingspan)\n",
    "\n",
    "x = np.linspace(height.min(), height.max(), 1000)\n",
    "\n",
    "for n, model in enumerate(models):\n",
    "    ax.plot(x, model.m * x + model.b, alpha=0.5)\n",
    "ax.plot(x, m * x + b, c=\"red\", ls=\"--\", label=\"Ideal Fit\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "ax.set_xlabel(\"Height [inches]\")\n",
    "ax.set_ylabel(\"Wingspan [inches]\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e5aac2",
   "metadata": {},
   "source": [
    "What is causing this stagnation in our optimization procedure?\n",
    "What is it about the shape of the \"landscape\" of $\\mathscr{L}_{MSE}$ that appears to keep our model from learning parameters that fit more closely to $(m^*, b^*)$?\n",
    "Consider these questions in light of the fact that we use the same learning rate for updating both $m$ and $b$.\n",
    "\n",
    "If you are working with others, discuss this with a neighbor and note your theories here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a9414b",
   "metadata": {},
   "source": [
    "<COGINST>\n",
    "The loss landscape reveals that $\\mathscr{L}_{MSE}$ is far more sensitive to changes in $m$ than it is in $b$.\n",
    "This means that a step along the $m$ axis can have a substantial affect on $\\mathscr{L}_{MSE}$, but a step of the same size along the $b$ access makes only a minute impact.\n",
    "    \n",
    "Put another way, the optimal parameter configuration $(m^*, b^*)$ lies on a \"loss landscape\" that is shaped like a valley that is flat along $b$.\n",
    "Thus $\\frac{\\partial \\mathscr{L}_{MSE}}{\\partial b}\\big|_{m=m^{*}} \\approx 0 $ in the neighborhood of the optimum.\n",
    "Because we use the same learning rate for updates to $m$ and $b$, it is hard for us to make substantial updates to $b$, which would require a large learning rate, without making disruptive changes to $m$.\n",
    "</COGINST>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d59ed2",
   "metadata": {},
   "source": [
    "### Using Normalized Data\n",
    "\n",
    "The intense sensitivity of $\\mathscr{L}_{MSE}$ to changes in $m$ in comparison to changes in $b$ occurs because our data is centered far from the origin $(x=0, y=0)$.\n",
    "Thus, minute adjustments to $m$ cause a dramatic change to the predictions produced by our model near $(x=80, y=85)$, whereas changes to $b$ that are comparable in magnitude have a much less significant impact on the prediction quality.\n",
    "This is why $\\mathscr{L}_{MSE}$ looks like a flat valley along the $b$ axis compared to its steep slopes along the $m$ axis.\n",
    "Take sometime to reflect on this and test this statement if it isn't making sense at first.\n",
    "\n",
    "To remedy this we will want to **normalize our data** so that the normalized height and wingspan values both have a mean of $0$ and a standard deviation of $1$.\n",
    "Using this normalized data will help to produce a loss landscape that features comparable curvatures along the $m$ and $b$ directions.\n",
    "\n",
    "See that the following function will normalize an array of data in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9da39cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x):\n",
    "    \"\"\"\n",
    "    Returns `x_normed` such that `x_normed.mean()` is 0 \n",
    "    and `x_normed.std()` is 1.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array_like, shape-(N,)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    normed_x : array_like, shape-(N,)\n",
    "        The normalized data\"\"\"\n",
    "    return (x - x.mean()) / x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3984cf1f",
   "metadata": {},
   "source": [
    "Lets normalize our height and wingspan data and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0c1f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_height = norm(height)\n",
    "normed_wingspan = norm(wingspan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529812f6",
   "metadata": {},
   "source": [
    "Compute the mean and standard deviation of both `normed_height` and `normed_wingspan` and explicitly confirm that they have the expected values (you should check four values in total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cbcd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#<COGINST>\n",
    "assert np.isclose(normed_height.mean(), 0)\n",
    "assert np.isclose(normed_height.std(), 1)\n",
    "\n",
    "assert np.isclose(normed_wingspan.mean(), 0)\n",
    "assert np.isclose(normed_wingspan.std(), 1)\n",
    "#</COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ddd175",
   "metadata": {},
   "source": [
    "Let's plot this normalized dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aa6c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(normed_height, normed_wingspan)\n",
    "ax.grid()\n",
    "ax.set_xlabel(\"Height [unitless]\")\n",
    "ax.set_ylabel(\"Wingspan [unitless]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea1606d",
   "metadata": {},
   "source": [
    "Note that the scales of the numbers on the x and y axes have changed: now our data is centered on $(0, 0)$ and most of the values fall within $[-1, 1]$. \n",
    "That being said, the actual distribution of the data points relative to one another is entirely unchanged!\n",
    "That is, we have not in any way manipulated the patterns or relationships between height and wingspan that was encoded in the raw data. \n",
    "\n",
    "Let's try \"training\" our model again, but this time we will use our normalized data.\n",
    "Note how `dist_from_target` evolved here versus before - it should be very close to $0$ by then end of training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c441db27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "\n",
    "plotter, fig, ax = create_plot([\"loss\", \"m\", \"b\", \"dist_from_target\"], ncols=2, last_n_batches=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e6881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "\n",
    "trajectory = []\n",
    "\n",
    "model = LinearModel()\n",
    "\n",
    "# note that we are training for many more epochs\n",
    "# and with a much larger learning rate\n",
    "num_epochs = 100\n",
    "learning_rate = 1e-1\n",
    "\n",
    "true_params_normed = np.array(ordinary_least_squares(normed_height, normed_wingspan))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287d7462",
   "metadata": {},
   "source": [
    "In the following cell you will need to copy the training-loop code from above and paste it here. Then, you will replace:\n",
    "- `height` -> `normed_height`\n",
    "- `wingspan` -> `normed_wingspan`\n",
    "- `true_params` -> `true_params_normed`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c625581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste your earlier training-loop code here, but replace:\n",
    "# height -> normed_height\n",
    "# wingspan -> normed_wingspan\n",
    "# true_params -> true_params_normed\n",
    "\n",
    "# You can hit <Escape>+F to bring up a find-and-replace window in Jupyter\n",
    "# to update height -> normed_height, etc.\n",
    "# (but make sure to only update *this* cell, not the whole notebook)\n",
    "\n",
    "# for n in range(num_epochs):\n",
    "#     ...\n",
    "#\n",
    "# <COGINST>\n",
    "for n in range(num_epochs):\n",
    "    y_pred = model(normed_height)\n",
    "    loss = mean_squared_error_mygrad(y_pred, normed_wingspan)\n",
    "    loss.backward()\n",
    "\n",
    "    plotter.set_train_batch(\n",
    "        dict(\n",
    "            loss=loss,\n",
    "            m=model.m.item(),\n",
    "            b=model.b.item(),\n",
    "            dist_from_target=dist_from_true(model.parameters, true_params_normed),\n",
    "        ),\n",
    "        batch_size=len(height),\n",
    "    )\n",
    "    trajectory.append((model.m.item(), model.b.item()))\n",
    "    gradient_step(model.parameters, learning_rate=learning_rate)\n",
    "# </COGINST>\n",
    "\n",
    "plotter.plot()\n",
    "\n",
    "# Run this cell and exampine the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b928835",
   "metadata": {},
   "source": [
    "How close is our approximate solution to the exact one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c5dc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints the distance between our approximate solution and the exact one\n",
    "train_metrics = plotter.to_xarray(\"train\").batch\n",
    "print(\"distance from target:\", round(train_metrics.dist_from_target[-1].item(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466da0ec",
   "metadata": {},
   "source": [
    "Let's visualize the landscape for $\\mathscr{L}(m,b; (\\hat{x}_n, \\hat{y}_n)_{n=0}^{N-1})$ where $(\\hat{x}_n, \\hat{y}_n)_{n=0}^{N-1}$ represents our normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e7c3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this\n",
    "\n",
    "graph_linear_regression_mse(normed_height, normed_wingspan, trajectory=trajectory)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(normed_height, normed_wingspan)\n",
    "m, b =  ordinary_least_squares(normed_height, normed_wingspan)\n",
    "\n",
    "x = np.linspace(normed_height.min(), normed_height.max(), 1000)\n",
    "\n",
    "ax.plot(x, model.m * x + model.b, c=\"orange\", label=\"Learned Fit\", lw=\"4\")\n",
    "ax.plot(x, m * x + b, c=\"red\", ls= \"--\", label=\"Ideal Fit\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "ax.set_xlabel(\"Normed Height\")\n",
    "ax.set_ylabel(\"Normed Wingspan\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32cb8e0",
   "metadata": {},
   "source": [
    "See that the landscape no longer looks so flat – adjusting $b$ and $m$ have comparable impacts on the quality of our model's predictions.\n",
    "Thus gradient descent will be much more effective at guiding our model's parameters towards $(m^*, b^*)$.\n",
    "Accordingly, our learned model now arrives at parameter values  that are very close to $(m^*, b^*)$\n",
    "This is all thanks to our having normalized our data before training on it.\n",
    "\n",
    "To see how much more reliable this training regimen is, let's train an ensemble of models, each with different initial parameters, and see that they all arrive very close to the same terminus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8702a554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training an ensemble of models on normalized data\n",
    "\n",
    "num_models = 10\n",
    "trajectories = [[] for i in range(num_models)]\n",
    "models = [LinearModel() for i in range(num_models)]\n",
    "\n",
    "num_epochs = 100\n",
    "step_size = 1E-1\n",
    "\n",
    "for n in range(num_epochs):\n",
    "    for model_id, model in enumerate(models):\n",
    "        y_pred = model(normed_height)\n",
    "        loss = mean_squared_error_mygrad(y_pred, normed_wingspan)\n",
    "        loss.backward()\n",
    "\n",
    "        trajectories[model_id].append((model.m.item(), model.b.item()))\n",
    "        gradient_step(model.parameters, learning_rate=learning_rate)\n",
    "\n",
    "trajectories = np.array(trajectories)\n",
    "\n",
    "fig, ax = graph_linear_regression_mse(normed_height, normed_wingspan, trajectory=trajectories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b605ef51",
   "metadata": {},
   "source": [
    "We should see that – no matter what the initial guess of $m$ and $b$ – gradient-based optimization always leads us very close to the optimal solution, at the bottom of this bowl."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f82378",
   "metadata": {},
   "source": [
    "### Re-Scaling Our Model's Predictions\n",
    "\n",
    "Although we see that our model learns well on the normalized data, note that we no longer can simply feed a height (measured in inches) to our model and get a wingspan predicted in inches – our model \"expects\" normalized data, and it's learned parameters will produce predicted wingspans on this \"normalized\" scale.\n",
    "I.e. it was trained to fit:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y} = m \\hat{x} + b\n",
    "\\end{equation}\n",
    "\n",
    "where \n",
    "\\begin{align}\n",
    "\\hat{x} &= \\frac{x - \\bar{x}}{\\mathrm{Std}[x]}\\\\\n",
    "\\hat{y} &= \\frac{y - \\bar{y}}{\\mathrm{Std}[y]}\n",
    "\\end{align}\n",
    "\n",
    "$\\bar{x}$ is the mean height of our observed data; $\\mathrm{Std}[x]$ is the corresponding standard deviation.\n",
    "$\\bar{y}$ is the mean height of our observed data; $\\mathrm{Std}[y]$ is the corresponding standard deviation.\n",
    "\n",
    "Given this, complete the following function that will permit us to pass \"raw\" heights to our model and for us to get \"raw\" wingspan predictions back.\n",
    "\n",
    "Hint: Take the above equation that transforms $y$ into $\\hat{y}$ and rewrite it so that $\\hat{y}$ is transformed to $y$.\n",
    "The output of our model, trained on normalized data, represents $\\hat{y}^{\\mathrm{(pred)}}$ and we need to transform it into $y^{\\mathrm{(pred)}}$ so that the prediction represents a wingspan in inches - as expressed in the original (raw) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114971b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processed_predictions(\n",
    "    model,\n",
    "    x,\n",
    "    height_mean=height.mean(),\n",
    "    height_std=height.std(),\n",
    "    wingspan_mean=wingspan.mean(),\n",
    "    wingspan_std=wingspan.std(),\n",
    "):\n",
    "    \"\"\" Given one or more new input heights, x (measured in) inches, uses the \n",
    "    provided linear model that was trainined on normalized data, to return \n",
    "    the predicted wingspan in inches.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Callable[[array_like], Tensor]\n",
    "        The linear model trained on normalized data\n",
    "    \n",
    "    x : array_like, shape-(N,)\n",
    "        N new observed height values, measured in inches.\n",
    "        We want to make predictions about these values\n",
    "\n",
    "    height_mean : float\n",
    "        The mean of the height training data [inches]\n",
    "    \n",
    "    height_std : float\n",
    "        The std-dev of the height training data  [inches]\n",
    "        \n",
    "    wingspan_mean : float\n",
    "        The mean of the wingspan training data  [inches]\n",
    "    \n",
    "    wingspan_std : float\n",
    "        The std-dev of the wingspan training data  [inches]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray, shape-(N,)\n",
    "        The N predicted wingspans, in inches, produced by the model\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    Call `.data` on your model's output so that it produces a numpy array\n",
    "    and not a mygrad tensor.\n",
    "    \"\"\"\n",
    "    # This function will:\n",
    "    # 1. Normalize the new heights (x) using the original dataset's \n",
    "    #    statistics. \n",
    "    #\n",
    "    # 2. Use the trained model to predict (normalized) wingspans, given\n",
    "    #    the normalized heights.\n",
    "    #\n",
    "    # 3. Un-normalize the predicted values, using the original dataset's \n",
    "    #    statistics, so that the returned values are the predicted wingspans \n",
    "    #    in units of inches.\n",
    "\n",
    "    # Normalize the input heights (`x`) using the provided statistics: `height_mean` and `height_std`\n",
    "    normed_x = (x - height_mean) / height_std  # <COGSTUB>\n",
    "    \n",
    "    # Have the model predict normalized wingspans using these normalized heights.\n",
    "    normed_y = model(normed_x)  # <COGSTUB> \n",
    "    \n",
    "    # `normed_y` represents unitless wingspans that our model predicted.\n",
    "    # We need to \"re-scale\" these, so that they carry interpretable units (inches)\n",
    "    #\n",
    "    # Un-normalize `normed_y` using the provided statistics: `wingspan_mean` and `wingspan_std`\n",
    "    y = normed_y * wingspan_std + wingspan_mean  # <COGSTUB>\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06a5f39",
   "metadata": {},
   "source": [
    "Finally, let's plot predictions from our learned model, but by using `processed_predictions` to normalize the input data and \"rescale\" the resulting predictions to produce predicted wingspans on the desired scale (i.e. in inches).\n",
    "We should see that our learned model matches the ideal linear fit very closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3772d658",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(height, wingspan)\n",
    "\n",
    "x = np.linspace(height.min(), height.max(), 1000)\n",
    "\n",
    "# Produce the processed predictions of your model, given the input `x`,\n",
    "# and assign the output to the variable `y`\n",
    "\n",
    "y = processed_predictions(model, x)  # <COGSTUB>\n",
    "\n",
    "\n",
    "ax.plot(x, y, color=\"orange\", lw=4, label=\"Learned Model\")\n",
    "\n",
    "\n",
    "m, b = ordinary_least_squares(height, wingspan,)\n",
    "ax.plot(x, m * x + b, c=\"red\", label=\"Ideal Fit\")\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Height [inches]\")\n",
    "ax.set_ylabel(\"Wingspan [inches]\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad628a9",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This exercise notebook provides the glue that connects the essential concepts that we have learned about thus far in our journey towards understanding machine learning using neural networks (i.e. deep learning).\n",
    "Namely, we:\n",
    "\n",
    "- Defined a mathematical model designed to transform observed data into useful predictions.\n",
    "  - In this case the model was a simple linear model, but we could easily generalize it to more complicated mathematical forms.\n",
    "  - It was natural for us to represent our model in terms of a [Python class](https://www.pythonlikeyoumeanit.com/module_4.html), since this allowed us to keep track of our model's parameters, our initialization scheme for the parameter values, and the code for performing a \"forward pass\" of our model on input data, all in one place.\n",
    "- Utilized automatic differentiation by using MyGrad's tensors and mathematical operations to store our model's parameters and to perform all of the mathematics associated with evaluating the model and the loss function for our problem. \n",
    "   - This gave us easy access to the gradient of the loss function with respect to our model's parameters. \n",
    "- Searched for optimal model parameter values - ones that minimize our loss function - by using gradient descent.\n",
    "   - We were introduced to the term \"epoch\" as an indicator that we had processed our dataset in its entirety.\n",
    "   - The selection of the learning rate was not informed by gradient descent or any obvious mathematics; we basically just made a guess at an appropriate value here (more on this later).\n",
    "   - This style of updating a mathematical model by using data containing the desired (or \"true\") predictions is known as \"supervised learning\".\n",
    "- Saw that the shape of our loss landscape had an impact on the efficacy of the gradient descent process, and, furthermore, that normalizing our data (to have a mean of $0$ and standard deviation of $1$) could help reshape this loss landscape to improve the model optimization process.\n",
    "   - This dynamic was rooted in the fact that the scales of the numbers associated with our raw data were such that making a small adjustment to $m$ made a much bigger impact on the quality of our model's predictions than did making a comparable adjustment to $b$.\n",
    "   - Normalizing our data helped to place $m$ and $b$ on more of an equal footing in terms of their influence on the model's predictions, and this led to healthier optimization performance, since we are using a single learning rate across all of the model's parameters.\n",
    "   - This parameter-scale balancing act will prove to be important for other, more sophisticated mathematical models as well, and data normalization will regularly be leveraged to help with this.\n",
    "\n",
    "It is recommended that you revisit and revise this notebook regularly to keep the lessons learned here in hand.\n",
    "\n",
    "In practice, we will never have the luxury of glimpsing the full loss landscape associated with our model and dataset as we did here.\n",
    "This is because our models will almost inevitable contain too many parameters to permit a plot of a 3D surface.\n",
    "So we wont have the benefit of qualitatively inspecting the trajectory of our gradient-based descent down the loss's surface, nor will we be able to easily glean the features of the surface's shape that prove difficult to traverse.\n",
    "For this reason, it is important to thoroughly internalize the lessons learned from this simple problem and prepare ourselves to anticipate their manifestations in more complicated scenarios - where we will need to be much more savvy and clever to deal with them."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python",
   "notebook_metadata_filter": "nbsphinx,-kernelspec"
  },
  "nbsphinx": {
   "execute": "never"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
