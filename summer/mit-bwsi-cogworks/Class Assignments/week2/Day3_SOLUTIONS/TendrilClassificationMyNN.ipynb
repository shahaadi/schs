{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78553578",
   "metadata": {},
   "source": [
    "# MyNN Implementation for Tendril Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61606938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mygrad as mg\n",
    "\n",
    "from cog_datasets import ToyData\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0536b48",
   "metadata": {},
   "source": [
    "We've used a one-layer neural network to solve a classification problem on a toy data set: a spiral formation of 2D data points. In this notebook, we will rewrite our solution to take advantage of MyNN, our main neural network library. This will serve as a gentle introduction to the library, which will prove to be very useful in moving forward with neural networks. In fact, MyNN is similar in spirit to PyTorch, so the insight you gain using MyNN will be helpful in moving on to full-scale deep learning libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6601f0f8",
   "metadata": {},
   "source": [
    "As before, let's construct and visualize our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94f879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the spiral dataset and its labels.\n",
    "num_tendrils = 3\n",
    "spiral_data = ToyData(num_classes=num_tendrils)\n",
    "\n",
    "xtrain, ytrain, _, _ = spiral_data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8591a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = spiral_data.plot_spiraldata()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4d22db",
   "metadata": {},
   "source": [
    "We'll reuse our accuracy function from the previous notebook that checked how accurate our model's predictions were."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3626e930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, truth):\n",
    "    \"\"\"\n",
    "    Returns the mean classification accuracy for a batch of predictions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    predictions : Union[numpy.ndarray, mg.Tensor], shape=(M, D)\n",
    "        The scores for D classes, for a batch of M data points\n",
    "    \n",
    "    truth : numpy.ndarray, shape=(M,)\n",
    "        The true labels for each datum in the batch: each label is an\n",
    "        integer in [0, D)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    accuracy : float\n",
    "    \"\"\"\n",
    "    # Use your solution from the previous notebook\n",
    "    # <COGINST>\n",
    "    return np.mean(np.argmax(predictions, axis=1) == truth)\n",
    "    # </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4477bfee",
   "metadata": {},
   "source": [
    "## Introducing MyNN\n",
    "\n",
    "Before, we had to construct the parameters of our neural network manually; recall that we created a `w`, `b`, and `v` ourselves with randomly initialized numpy arrays. Additionally, we had to manually perform gradient descent in order to update our parameters. One of the main advantages of using a neural network library such as MyNN is that this sort of low-level implementation is already taken care of. Essentially, neural network libraries will package up the general form of various functionality we may want to use such as gradient descent so that we can focus on algorithmic developments rather than reimplementing gradient descent every time we want to train a network.\n",
    "\n",
    "\n",
    "### MyNN Layers\n",
    "We were using fully-connected (dense) layers to solve our classification problem. These are packaged up conveniently inside MyNN in the `layers` module. Let's import the dense layer now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c737a250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mynn.layers.dense import dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd769fa",
   "metadata": {},
   "source": [
    "MyNN allows us to conveniently create \"layers\" for our neural network - this is an object that initializes and stores the weights associated \n",
    "\n",
    "When we create a dense layer, we simply specify the desired shape of that layer. We can then call that layer like a function to pass data through it. As an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d166fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This initializes a shape-(2, 3) Tensor, `weight`, and a shape-(3,) Tensor, `bias`\n",
    "# and stores these tensors in this \"dense layer\". The weights are drawn from \n",
    "# default statistical distributions - we can also specify the distribution \n",
    "# that we want\n",
    "dense_layer = dense(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54923b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9005b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layer.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02f2616",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.rand(4, 2)\n",
    "\n",
    "# Calling `dense_layer(data)` multiplies the shape-(4, 2) matrix w/ our shape-(2, 3) \n",
    "# layer produces a shape-(4, 3) result\n",
    "# This performs: `data @ w + b` \n",
    "dense_layer(data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89b94bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can easily access all of a layer's parameters, stored in a tuple,\n",
    "# via <layer>.parameters\n",
    "dense_layer.parameters\n",
    "\n",
    "# this will make it easy for us to access all of our model's parameters\n",
    "# for gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665bdeea",
   "metadata": {},
   "source": [
    "There are other types of layers (such as `conv`) packaged inside MyNN as well. \n",
    "\n",
    "You may have already thought to reuse some of the code that you wrote in the universal function approximator or in the Tendril classifier you wrote using plain MyGrad already; this is essentially all that MyNN is doing: packaging up useful chunks of code so that we can more easily, more quickly, and with fewer mistakes implement neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b439cf4",
   "metadata": {},
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d653c6c5",
   "metadata": {},
   "source": [
    "We'll also need to use our ReLU activation function. It and other activations are stored in `mynn.activations`. Let's import that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0240beeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mygrad.nnet.activations import relu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b2e026",
   "metadata": {},
   "source": [
    "The final piece we need to create our model is a weight initializer. \n",
    "\n",
    "### Parameter-Initializers\n",
    "In MyGrad, we used a he-normal to initialize our weights, and initialized our bias to 0. By default, MyNN will initialize a bias to zero, but we will need to pass in an initializer for the weight matrix (by default, MyNN will use a uniform distribution). \n",
    "\n",
    "The He-normal distribution and all other initializers are in `mygrad.nnet.initializers`. There are several other initialization schemes defined in that module. Feel free to poke around and explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccbcab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mygrad.nnet.initializers import he_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7722faa",
   "metadata": {},
   "source": [
    "## Creating a MyNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1454ed9f",
   "metadata": {},
   "source": [
    "Let's recreate the model that we developed before using MyNN this time!  \n",
    "\n",
    "This `Model` class will maintain all of our layers and define how to propagate input through the network. By creating a model class, we can both organize the layers for our neural network as well as create a simple way for running a forward pass on our data through every layer. Creating a model object and passing in our data will give us the output of our model - it's that simple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30e9edd",
   "metadata": {},
   "source": [
    "In `__init__(self)` we simply define our layers. In this case we have two dense layers as denoted by `dense1` and `dense2`.  To understand what happens in the dense layer try reading the documentation for `dense()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3cd3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, num_neurons, num_classes):\n",
    "        \"\"\"This initializes all of the layers in our model, and sets them\n",
    "        as attributes of the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        num_neurons : int\n",
    "            The size of our hidden layer\n",
    "            \n",
    "        num_classes : int\n",
    "            The size of the outpur layer (i.e. the number\n",
    "            of tendrils).\"\"\"\n",
    "        self.dense1 = dense(2, num_neurons, weight_initializer=he_normal)\n",
    "        self.dense2 = dense(num_neurons, num_classes, weight_initializer=he_normal, bias=False)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        '''Passes data as input to our model, performing a \"forward-pass\".\n",
    "        \n",
    "        This allows us to conveniently initialize a model `m` and then send data through it\n",
    "        to be classified by calling `m(x)`.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : Union[numpy.ndarray, mygrad.Tensor], shape=(M, 2)\n",
    "            A batch of data consisting of M pieces of data,\n",
    "            each with a dimentionality of 2.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        mygrad.Tensor, shape=(M, num_out)\n",
    "            The model's prediction for each of the M pieces of data.\n",
    "        '''\n",
    "        \n",
    "        # We pass our data through a dense layer, use the activation \n",
    "        # function relu and then pass it through our second dense layer\n",
    "        # We don't have a second activation function because it happens\n",
    "        # to be included in our loss function: softmax-crossentropy\n",
    "        return self.dense2(relu(self.dense1(x)))\n",
    "        \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        \"\"\" A convenience function for getting all the parameters of our model.\n",
    "        \n",
    "        This can be accessed as an attribute, via `model.parameters` \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[Tensor, ...]\n",
    "            A tuple containing all of the learnable parameters for our model\"\"\"\n",
    "        return self.dense1.parameters + self.dense2.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ef4236",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(num_neurons=15, num_classes=num_tendrils)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00ea9a8",
   "metadata": {},
   "source": [
    "As before, we'll use the softmax cross-entropy loss provided by MyGrad. That being said, MyNN has some other loss functions in `mynn.losses`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14c06b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mygrad.nnet.losses import softmax_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cfabc2",
   "metadata": {},
   "source": [
    "Lastly, we had been writing our own gradient descent function.\n",
    "However this is also taken care of in MyNN.\n",
    "As you know, gradient descent is an optimization method; thus, it is located inside `mynn.optimizers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af01c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mynn.optimizers.sgd import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a73949",
   "metadata": {},
   "source": [
    "When we construct an optimizer, we must pass it the parameters of our model and any additional hyperparameters (such as learning rate). After we have backpropagated our loss through our network by calling `loss.backward()`, we can call `step()` on the optimizer to perform a single step of the optimization procedure. In our case, the `step()` function will loop over all the parameters of our model and update them according to the gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e8afaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = SGD(model.parameters, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba036b36",
   "metadata": {},
   "source": [
    "As before, we'll create a plot to see our loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b0562d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from noggin import create_plot\n",
    "plotter, fig, ax = create_plot(metrics=[\"loss\", \"accuracy\"])\n",
    "\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c359a0a3",
   "metadata": {},
   "source": [
    "We can use the same exact training loop structure as before. However, MyNN will take care of most of this for us. We'll just need to:\n",
    "\n",
    "- randomize our indices\n",
    "- get a batch of training data\n",
    "- call `model(batch)` on the data to get outputs\n",
    "- get the truth\n",
    "- compute the loss by calling `softmax_crossentropy(predictions, truth)`\n",
    "- backpropagate the loss\n",
    "- call `optim.step()` to perform SGD\n",
    "- plot our training statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d744c3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this model to train our single-layer neural network\n",
    "\n",
    "for epoch_cnt in range(5000):\n",
    "    idxs = np.arange(len(xtrain))  # -> array([0, 1, ..., 9999])\n",
    "    np.random.shuffle(idxs)  \n",
    "    \n",
    "    for batch_cnt in range(0, len(xtrain)//batch_size):\n",
    "        batch_indices = idxs[batch_cnt*batch_size : (batch_cnt + 1)*batch_size]\n",
    "        \n",
    "        batch = xtrain[batch_indices]  # random batch of our training data\n",
    "        truth = ytrain[batch_indices]\n",
    "\n",
    "        # `model.__call__ is responsible for performing the \"forward-pass\"\n",
    "        prediction = model(batch) \n",
    "        \n",
    "        loss = softmax_crossentropy(prediction, truth)\n",
    "        \n",
    "        # you still must compute all the derivatives!\n",
    "        loss.backward()\n",
    "        \n",
    "        # The optimizer is responsible for updating all of the parameters.\n",
    "        # This essentially replaces our `gradient_step` function from before.\n",
    "        optim.step()\n",
    "        \n",
    "        # we'll also compute the accuracy of our model as usual\n",
    "        acc = accuracy(prediction, truth)\n",
    "\n",
    "        plotter.set_train_batch({\"loss\" : loss.item(),\n",
    "                                 \"accuracy\" : acc},\n",
    "                                 batch_size=batch_size)\n",
    "plotter.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2eda7a",
   "metadata": {},
   "source": [
    "As before, we can visualize our decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4ad76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_function(x):\n",
    "    from mygrad.nnet.activations import softmax\n",
    "    with mg.no_autodiff:\n",
    "        return softmax(model(x)).data\n",
    "\n",
    "fig, ax = spiral_data.visualize_model(dummy_function, entropy=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3caaa0",
   "metadata": {},
   "source": [
    "## Fitting a Linear Model\n",
    "\n",
    "Let's try to build up some more intuition for how our model classifies the tendrils. To do this, we will make our model even simpler - we will make a model **without any nonlinearities**. This model will not do a great job on classification, but it's operations will be easy to understand.\n",
    "\n",
    "Create a single-layer neural network:\n",
    "\n",
    "\\begin{equation}\n",
    "F(W, b; x) = \\text{softmax}(Wx + b)\n",
    "\\end{equation}\n",
    "\n",
    "Because we will be using the softmax-crossentropy loss (so the softmax will actually by taken care of by the loss), our model is *actually* just a single linear layer:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "F(W, b; x) = Wx + b\n",
    "\\end{equation}\n",
    "\n",
    "We can use MyNN's `dense` class to create this trivial model in one line! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79262e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use `dense` to create a single shape-(2, num_tendrils) layer; specify `he_normal` as the initializer\n",
    "model = dense(2, num_tendrils, weight_initializer=he_normal)  # <COGSTUB>\n",
    "\n",
    "# Initialize an `SGD` optimizer using this model's parameters. Provide the learning rate: `lr=0.1` \n",
    "optim = SGD(model.parameters, learning_rate=0.1)  # <COGSTUB>\n",
    "\n",
    "batch_size = 50  # <COGSTUB> set our batch-size to 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6146bad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mygrad.nnet.losses import softmax_crossentropy\n",
    "from mynn.optimizers.sgd import SGD\n",
    "from noggin import create_plot\n",
    "\n",
    "# this will create a noggin plot\n",
    "plotter, fig, ax = create_plot(metrics=[\"loss\", \"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6534ca",
   "metadata": {},
   "source": [
    "Let's train this 1-layer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8ebac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to train our linear model\n",
    "\n",
    "for epoch_cnt in range(5000):\n",
    "    idxs = np.arange(len(xtrain))  # -> array([0, 1, ..., 9999])\n",
    "    np.random.shuffle(idxs)  \n",
    "    \n",
    "    for batch_cnt in range(0, len(xtrain)//batch_size):\n",
    "        batch_indices = idxs[batch_cnt*batch_size : (batch_cnt + 1)*batch_size]\n",
    "        batch = xtrain[batch_indices]  # random batch of our training data\n",
    "\n",
    "        # `model.__call__ is responsible for performing the \"forward-pass\"\n",
    "        prediction = model(batch) \n",
    "        truth = ytrain[batch_indices]\n",
    "        \n",
    "        loss = softmax_crossentropy(prediction, truth)\n",
    "        \n",
    "        # you still must compute all the gradients!\n",
    "        loss.backward()\n",
    "        \n",
    "        # the optimizer is responsible for updating all of the parameters\n",
    "        optim.step()\n",
    "        \n",
    "        acc = accuracy(prediction, truth)\n",
    "\n",
    "        plotter.set_train_batch({\"loss\" : loss.item(),\n",
    "                                 \"accuracy\" : acc},\n",
    "                                 batch_size=batch_size)\n",
    "plotter.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cf3418",
   "metadata": {},
   "source": [
    "### Understanding Your Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16d08aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to visualize your results\n",
    "\n",
    "def dummy_function(x):\n",
    "    from mygrad.nnet.activations import softmax\n",
    "    return softmax(model(x)).data\n",
    "\n",
    "fig, ax = spiral_data.visualize_model(dummy_function, entropy=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c77477",
   "metadata": {},
   "source": [
    "Now print out the `weight` tensor of your single dense layer. What is its shape? Look back to the mathematical form of this neural network - what dot products are being performed by the matrix multiplication (are the rows of `weight` being used in the dot-product or the columns)?\n",
    "\n",
    "On paper, sketch the classification visualization that you see above. **Draw the vectors stored in `weight` on top of this sketch**. \n",
    "\n",
    "Reflect on our discussion of the dot-product being a means of measuring how much two vectors *overlap*. What did this simple model learn and how is it doing its classification? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6705f2",
   "metadata": {},
   "source": [
    "<COGINST>\n",
    "The *columns* of our dense-layer's `weight` are being dotted with each data point, $(x, y)$. \n",
    "    \n",
    "Column-0 of `weight` thus is a vector of length-2, and if the datapoint vector, $(x, y)$ has a large overlap with it, then our model will produce a large score for entry-0 of its predictions, which means that the model would predict that the point belongs to tendril 0.\n",
    "\n",
    "The same is true for column-1 of `weight` and column-2 of `weight`, respectively. Each column in `weight` points down the middle of its respective wedge in the classification diagram, and all of the points that are closest to that vector are classified as belonging to the class associated with that weight-vector.\n",
    "This explains the roughly equal-sized classification wedges that emerge in our visualization\n",
    "</COGINST>"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python",
   "notebook_metadata_filter": "nbsphinx,-kernelspec"
  },
  "nbsphinx": {
   "execute": "never"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
