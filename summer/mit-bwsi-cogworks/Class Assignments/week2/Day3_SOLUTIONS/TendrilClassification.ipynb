{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93350912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mygrad as mg\n",
    "\n",
    "from cog_datasets import ToyData\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33b2c1d",
   "metadata": {},
   "source": [
    "In this notebook, we will be training a two-layer neural network to solve a *classification* problem on a toy data set. We will generate a spiral-formation of 2D data points. This spiral will have grouped \"tendrils\", and we will want our neural network to classify *to which tendril a given point belongs*. \n",
    "\n",
    "Read the documentation for `ToyData`. Run the following cells to view the spiral data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b00dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the spiral dataset and its labels.\n",
    "num_tendril = 3\n",
    "data = ToyData(num_classes=num_tendril)\n",
    "\n",
    "# Loads training data and labels as numpy arrays\n",
    "xtrain, ytrain, _, _ = data.load_data()\n",
    "\n",
    "xtrain.shape, ytrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c8a1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = data.plot_spiraldata()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cac6845",
   "metadata": {},
   "source": [
    "- red: label-0\n",
    "- yellow: label-1\n",
    "- blue: label-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce0adc0",
   "metadata": {},
   "source": [
    "Print out the contents of `xtrain` and `ytrain`. What do these arrays correspond to? See that `xtrain` stores the 2D data points in the spiral, and `ytrain` stores the Tendril-ID associated with that point. How many points are in our training data? How are the labels specified for this dataset? Discuss with your neighbor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ed9f49",
   "metadata": {},
   "source": [
    "## Our Model\n",
    "\n",
    "We will extend the universal function approximation function to make a *classification* prediction. That is, given a shape-(2,) point $\\vec{x} = [x_o, y_o]$, our model will produce a shape-(3,) vector $\\vec{y}_{pred} = [y_0, y_1, y_2]$, where the $i$-th element, $y_i$, indicates how \"confident\" our model is that the point located at $\\vec{x}$ belongs to tendril-$i$. \n",
    "\n",
    "\\begin{equation}\n",
    "F(\\{\\vec{v}_i\\}, \\{\\vec{w}_i\\}, \\{b_i\\}; \\vec{x}) = \\sum_{i=1}^{N} \\vec{v}_{i}\\varphi(\\vec{x} \\cdot \\vec{w}_{i} + b_{i}) = \\vec{y}_{pred}\n",
    "\\end{equation}\n",
    "\n",
    "Notice here that $\\vec{v}_i$ is now a *vector*, whereas in the original universal function approximation theorem it was a scalar. This is in accordance with the fact that we now want to *predict* a vector, $\\vec{y}_{pred}$, instead of a single number $y_{pred}$.\n",
    "\n",
    "Given that $\\varphi(\\vec{x} \\cdot \\vec{w}_{i} + b_{i})$ is a scalar, each $\\vec{v}_i$ should have shape-(3,) such that $\\vec{y}_{pred}$ contains a total of classification score predictions – one per tendril."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5279eef8",
   "metadata": {},
   "source": [
    "In this notebook, we will create a single-layer dense neural network that closely resembles the approximator that we created to fit $\\cos{(x)}$.\n",
    "This time, however, $\\vec{x}$ will be a 2D point instead of a single number. Thus a batch of our training data will have a shape $(M, 2)$ instead of $(M, 1)$, where $M$ is the size of our batch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8b4353",
   "metadata": {},
   "source": [
    "Because we are classifying which of three tendrils a 2D point belongs to, we now **want our model to predict *three* numbers** ($\\vec{y}_{pred} = [y_0, y_1, y_2]$), rather than one, as its prediction.\n",
    "These three numbers will be the three \"scores\" that our model predicts for a point: one for each tendril. If score-0 is the largest score, then our model is predicting that the 2D point belongs to tendril 0. And so on.\n",
    "Thus, given batch of shape-$(M, 2)$, our model will produce a shape-$(M, 3)$ tensor as its prediction; for each of the $M$ 2D input points, the model will produce $3$ classification scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac60c93",
   "metadata": {},
   "source": [
    "## The \"Activation\" Function\n",
    "\n",
    "We will be using the so-called \"activation function\" known as a \"rectified linear unit\", or ReLU for short:\n",
    "\n",
    "\\begin{equation}\n",
    "\\varphi_{\\text{relu}}(x) = \n",
    "\\begin{cases} \n",
    "      0, \\quad x < 0 \\\\\n",
    "      x, \\quad 0 \\leq x\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "This is a very popular activation function in the world of deep learning. We will not have time to go into why here, but it is worthwhile to read about. `mygrad` has this function: `mygrad.nnet.activations.relu`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b66b68d",
   "metadata": {},
   "source": [
    "Here we will import  the relu function from `mygrad`, and plot it on $x \\in [-3, 3]$. We will plot its derivative as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1fdaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mygrad.nnet.activations import relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2573a804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mygrad as mg\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = mg.linspace(-3, 3, 1000)\n",
    "\n",
    "ax.grid(True)\n",
    "\n",
    "f = relu(x)\n",
    "ax.plot(x, f, label=\"relu\")\n",
    "\n",
    "f.backward()\n",
    "\n",
    "ax.plot(x, x.grad, label=\"derivative\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9d31b8",
   "metadata": {},
   "source": [
    "\n",
    "### Initializing Our Model Parameters\n",
    "\n",
    "### Model Parameter Shapes\n",
    "\n",
    "Refer back to the linear regression model that we created before, and specifically note the shapes of the different model parameter tensors that we initialized.\n",
    "In the linear regression problem we effectively treated each datum and output as a shape-(1,) vector.\n",
    "Now we are working with shape-(2,) vectors as inputs and shape-(3,) vectors as outputs.\n",
    "Given this context, what must the respective lengths of each $\\vec{w}_i$ and $\\vec{v}_i$ be?\n",
    "\n",
    "<COGINST>\n",
    "Given that each datum $\\vec{x}$ is shape-(2,) then each $\\vec{w}_i$ must also be shape-(2,).\n",
    "The shape of $\\vec{v}_i$ determines and matches the shape of our model's output, thus it must be shape-(3,).\n",
    "</COGINST>\n",
    "\n",
    "As was eluded to in the previous exercise notebook, we don't need to change any of the math of the \"forward pass\" of our model; it is once again:\n",
    "\n",
    "\n",
    "```python\n",
    "relu_out = relu(np.matmul(x, model.w) + model.b)\n",
    "out = np.matmul(relu_out, model.v)\n",
    "```\n",
    "\n",
    "Based on this, what are the shapes of:\n",
    "- `x` (a batch of data)\n",
    "- `model.w`\n",
    "- `model.b`\n",
    "- `model.v`\n",
    "- `out`\n",
    "\n",
    "<COGINST>\n",
    "\n",
    "```\n",
    "x w + b --> matmul[(M,2) w/ (2, N)] + (N,) --> (M, N)\n",
    "v relu(x w + b) --> matmul[(M, N) w/ (N, num_tendril)] --> (M, num_tendril)\n",
    "```\n",
    "\n",
    "- `x`: `(M, 2)`  (where M is batch-size)\n",
    "- `model.w`: `(2, N)`  (where N is the number of neurons in this layer of our model)\n",
    "- `model.b`: `(N,)`\n",
    "- `model.v`: `(N, num_tendrils)` (where `num_tendrils` is the number of possible classes that we are predicting among)\n",
    "- `out`: `(M, num_tendrils)\n",
    "\n",
    "</COGINST>\n",
    "\n",
    "Check your answers with TAs / neighbors before proceeding.\n",
    "\n",
    "### Drawing Initial Values from Random Distributions\n",
    "\n",
    "We will be using a initialization technique known as \"He-normal\" initialization (pronounced \"hey\"). Essentially we will draw all of our dense-layer parameters from a scaled normal distribution, where the distribution will scaled by an additional $\\frac{1}{\\sqrt{2N}}$, where $N$ is dictates that number of parameters among $\\{\\vec{v}_i\\}_{i=1}^{N}$, $\\{\\vec{w}_i\\}_{i=1}^{N}$, and $\\{b_i\\}_{i=1}^{N}$, respectively. This will aid us when we begin training neural networks with large numbers of neurons. MyGrad's `he_normal` will handle all of this for us.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943c41be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mygrad.nnet.initializers.he_normal import he_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768aaa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:      \n",
    "    def __init__(self, num_neurons, num_classes):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_neurons : int\n",
    "            The number of 'neurons', N, to be included in the model.\n",
    "        \n",
    "        num_classes : int\n",
    "            The number of distinct classes that you want your model to predict.\n",
    "        \"\"\"\n",
    "        # set self.N equal to `num_neurons\n",
    "        self.N = num_neurons  # <COGSTUB>\n",
    "        \n",
    "        # set self.num_classes equal to the number of distinct\n",
    "        # classes that you want your model to be able to predict\n",
    "        self.num_classes = num_classes  # <COGSTUB>\n",
    "        \n",
    "        # Use `self.initialize_params()` to draw random values for\n",
    "        # `self.w`, `self.b`, and `self.v`. Note that this method does not return anything\n",
    "        \n",
    "        # <COGINST>\n",
    "        self.initialize_params()\n",
    "        # </COGINST>\n",
    "\n",
    "    def initialize_params(self):\n",
    "        \"\"\"\n",
    "        Randomly initializes and sets values for  `self.w`,\n",
    "        `self.b`, and `self.v`.\n",
    "\n",
    "        Uses `mygrad.nnet.initializers.normal to draw tensor\n",
    "        values w, v from the he-normal distribution, using a gain of 1.\n",
    "\n",
    "        The b-values are all initialized to zero.\n",
    "\n",
    "        self.w : shape-(???)  ... using he-normal\n",
    "        self.b : shape-(???)  ... as a tensor of zeros\n",
    "        self.v : shape-(???)  ... using he-normal\n",
    "\n",
    "        ??? -> you fill in the values\n",
    "        \"\"\"\n",
    "        # Use the attributes `self.N` and `self.num_classes` as-needed\n",
    "        # to construct the shapes of the tensors\n",
    "\n",
    "        self.w = he_normal(2, self.N) # <COGSTUB> draw a shape-(???, N) tensor using `he_normal`\n",
    "        self.b = mg.zeros((self.N,))  # <COGSTUB> draw a shape-(???) tensor using `mg.zeros(<shape-tuple>)`\n",
    "        self.v = he_normal(self.N, self.num_classes)  # <COGSTUB> draw a shape-(???) tensor using `he_normal`\n",
    "\n",
    "   \n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Performs a so-called 'forward pass' through the model\n",
    "        on the specified data. I.e. uses the linear model to\n",
    "        make a prediction based on `x`.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : array_like, shape-(M, 2)\n",
    "            An array of M observations, each a 2D point.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        prediction : mygrad.Tensor, shape-(M, num_classes)\n",
    "            A corresponding tensor of M predictions based on\n",
    "            the form of the universal approximation theorem.\n",
    "        \"\"\"\n",
    "        # # This is nearly identical to the `Model.__call__` method that you coded for the universal\n",
    "        # function approximator, when fitting cosine.\n",
    "        #\n",
    "        # The one difference is that we are using relu instead of sigmoid\n",
    "        \n",
    "        # <COGINST>\n",
    "        relu_out = relu(x @ self.w + self.b)  # matmul[(M,2) w/ (2, N)] + (N,) --> (M, N)\n",
    "        out = relu_out @ self.v  # matmul[(M, N) w/ (N, num_tendril)] --> (M, num_tendril)\n",
    "        return out\n",
    "        # </COGINST>\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        \"\"\" A convenience function for getting all the parameters of our model.\n",
    "        \n",
    "        This can be accessed as an attribute, via `model.parameters` \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[Tensor, ...]\n",
    "            A tuple containing all of the learnable parameters for our model\"\"\"\n",
    "        # return a tuple containing the model's learnable parameters: w, b, and v\n",
    "        return (self.w, self.b, self.v)  # <COGLINE>\n",
    "    \n",
    "    def load_parameters(self, w, b, v):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.v = v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b17e849",
   "metadata": {},
   "source": [
    "## Computing Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e72dc6d",
   "metadata": {},
   "source": [
    "Because we are solving a classification problem rather than a regression problem, we can measure the accuracy of our predictions. We will write an `accuracy` function which accepts our models predictive scores for a batch of data, shape-(M, 3), and the \"truth\" labels for that batch, shape-(M,). \n",
    "\n",
    "Thus, if score-0 for some point is the maximum score, and the label for that point is 0, then the prediction for that point is correct. \n",
    "\n",
    "The function should return the mean classification accuracy for that batch of predictions (a single number between 0 and 1). Write a simple test for your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1058d145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, truth):\n",
    "    \"\"\"\n",
    "    Returns the mean classification accuracy for a batch of predictions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    predictions : Union[numpy.ndarray, mg.Tensor], shape=(M, D)\n",
    "        The scores for D classes, for a batch of M data points\n",
    "        \n",
    "    truth : numpy.ndarray, shape=(M,)\n",
    "        The true labels for each datum in the batch: each label is an\n",
    "        integer in [0, D)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The accuracy: the fraction of predictions that your model got correct,\n",
    "        which should be in [0, 1]\"\"\"\n",
    "    # Use `np.argmax` to convert the (M, D) prediction scores (floats)\n",
    "    # to an array of (M,) predicted integer labels that reside in [0, D)\n",
    "    predicted_labels = np.argmax(predictions, axis=1)  # <COGSTUB>\n",
    "    \n",
    "    # Now we compare the array of predicted labels to the true labels using\n",
    "    # to produce a shape-(M,) boolean-valued array of `True`/`False` values \n",
    "    # where the predictions do (True) or don't (False) match the truth.\n",
    "    prediction_vs_truth = predicted_labels == truth  # <COGSTUB>\n",
    "    \n",
    "    # Lastly, recall that we can perform arithmetic on boolean-valued arrays/tensors. \n",
    "    # `True` will behave like 1 and `False` will behave like 0.\n",
    "    #\n",
    "    # Compute the fraction correct predictions, which is our prediction accuracy.\n",
    "    fraction_correct = np.sum(prediction_vs_truth) / len(prediction_vs_truth)  # <COGSTUB>\n",
    "    \n",
    "    return fraction_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a96797",
   "metadata": {},
   "source": [
    "Let's test our accuracy function.\n",
    "We'll create a shape-`(M=3, num_classes=2)` of predictions array, meaning that we have three predictions and each prediction provides a score between two classes.\n",
    "\n",
    "Look at the particular prediction-scores provided below.\n",
    "\n",
    "On a piece of paper, write down the prediction scores; for each of the 3 sets of predictions, write down the corresponding predicted label (each one should be either a `0` or a `1`, depending on whether the first or second score is largest).\n",
    "Compare these predicted labels to the true labels. What fraction of these predictions are correct?\n",
    "\n",
    "Use your `accuracy` function and check that it returns the expected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb54836",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array(\n",
    "    [\n",
    "        [0.1, 10.2],\n",
    "        [90.0, -2.0],\n",
    "        [18.0, 0.0],\n",
    "    ]\n",
    ")\n",
    "truth = np.array([1, 1, 0])\n",
    "\n",
    "# <COGINST>\n",
    "# The predicted labels are: [1, 0, 0] and the truth is [1, 1, 0]\n",
    "# Thus we got 2/3 of the predictions correct.\n",
    "\n",
    "accuracy(predictions, truth)  \n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1878cc",
   "metadata": {},
   "source": [
    "## Our Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f30c1f1",
   "metadata": {},
   "source": [
    "### The softmax activation function\n",
    "\n",
    "We will be using the \"cross-entropy\" function for our loss. This loss function is derived from the field of information theory, and is designed to compare probability distributions. This means that we will want to convert the numbers of $\\vec{y}_{pred} = F(\\{\\vec{v}_i\\}, \\{\\vec{w}_i\\}, \\{b_i\\}; \\vec{x})$, into numbers that behave like probabilities. To do this, we will use the \"softmax\" function:\n",
    "\n",
    "Given an $m$-dimensional vector $\\vec{y}$, the softmax function returns a a vector, $\\vec{p}$ of the same dimensionality:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{softmax}(\\vec{y}) = \\vec{p}\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "p_i = \\frac{e^{y_{i}}}{\\sum_{j=0}^{m-1}{e^{y_{j}}}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414a869d",
   "metadata": {},
   "source": [
    "Convince yourself that the elements of $\\vec{p}$ do indeed satisfy the basic requirements of being a probability distribution. I.e. :\n",
    "\n",
    "- $0 \\leq p_i \\leq 1$, for each $p_i$\n",
    "- $\\sum_{i=0}^{m-1}{p_i} = 1$\n",
    "\n",
    "where $m$ is the number of classes in our classification problem.\n",
    "\n",
    "In the following cell, apply MyGrad's `softmax` function along the rows of the tensor\n",
    "\n",
    "```python\n",
    "x = mg.tensor([[10.0, 10.0],\n",
    "               [-10.0, 10.0]])\n",
    "```\n",
    "\n",
    "and study the result. Do the output's respective rows satisfy the aforementioned properties?\n",
    "Looking at the input `x`, does the output make intuitive sense?\n",
    "Feel free to play around with various inputs to `softmax` to strengthen your intuition for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3fff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mygrad.nnet import softmax\n",
    "\n",
    "# <COGINST>\n",
    "\n",
    "x = mg.tensor([[10.0, 10.0],\n",
    "               [-10.0, 10.]])\n",
    "softmax(x)\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c2d3af",
   "metadata": {},
   "source": [
    "### The cross-entropy loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfd75d0",
   "metadata": {},
   "source": [
    "So, armed with the softmax function, we can convert our classification scores, $\\vec{y}$, to classification probabilities, $\\vec{p}$ (or at the very least, numbers that *act* like probabilities. This opens the door for us to utilize the [cross-entropy loss](https://en.wikipedia.org/wiki/Cross_entropy).\n",
    "\n",
    "Given our prediction probabilities for the $m$ classes in our problem, $\\vec{p}$, we have the associated \"true\" probabilities $\\vec{t}$ (since we are solving a supervised learning problem). E.g., \n",
    "\n",
    "- if our point $\\vec{x}$ resides in tendril-0, then $\\vec{t} = [1., 0., 0.]$\n",
    "- if our point $\\vec{x}$ resides in tendril-1, then $\\vec{t} = [0., 1., 0.]$\n",
    "- if our point $\\vec{x}$ resides in tendril-2, then $\\vec{t} = [0., 0., 1.]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94a90b7",
   "metadata": {},
   "source": [
    "In terms of our predictions, $\\vec{p}$, and our truth-values, $\\vec{t}$, the cross-entropy loss is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathscr{L}(\\vec{p}, \\vec{t}) = -\\sum_{i=0}^{m}{t_{i}\\log{p_{i}}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5285f0",
   "metadata": {},
   "source": [
    "This loss function measures *how different two probability distributions, $\\vec{p}$ and $\\vec{t}$ are*. The loss gets larger as the two probability distributions become more disparate, and the loss is minimized (0, specifically) when the two distributions are identical.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f07dd6",
   "metadata": {},
   "source": [
    "## The softmax-crossentropy function\n",
    "\n",
    "Because it is very common to perform the softmax on the outputs of your model, to \"convert them to probabilities\", and then pass those probabilities to a cross-entropy function, it is more efficient to have a function that does both of these steps. This is what `mygrad`'s [softmax_crossentropy](https://mygrad.readthedocs.io/en/latest/generated/mygrad.nnet.losses.softmax_crossentropy.html) function does. Take the time to read its documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393edd52",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from mygrad.nnet.losses import softmax_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5903fb85",
   "metadata": {},
   "source": [
    "Study and run the following cells.\n",
    "What do the shapes of `p` and `t` correspond to?\n",
    "Looking at the contents of a given pair of `p` and `t` does the output of `softmax_crossentropy` make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c453c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.array([[100., 0., 0.]])\n",
    "t = np.array([0])\n",
    "\n",
    "softmax_crossentropy(p, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bfb1ef",
   "metadata": {},
   "source": [
    "<COGINST>\n",
    "The above describes a single prediction among three classes, where the true class is label-0 and\n",
    "the highest-confidence prediction is label-0, thus the loss is very small (essentially at the minimum).\n",
    "</COGINST>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc36b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.array([[100., 0., 0.]])\n",
    "t = np.array([1])\n",
    "\n",
    "softmax_crossentropy(p, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a49b7da",
   "metadata": {},
   "source": [
    "<COGINST>\n",
    "The above describes a single prediction among three classes, where the true class is label-1 and\n",
    "the highest-confidence prediction is label-0, thus the loss is very large because the model is confidently incorrect.\n",
    "</COGINST>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245c6411",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.array([[100., 100., 100.]])\n",
    "t = np.array([0])\n",
    "\n",
    "softmax_crossentropy(p, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb53931",
   "metadata": {},
   "source": [
    "<COGINST>\n",
    "The above describes a single prediction among three classes, where the true class is label-0 and\n",
    "the model is ambivalent as to which class is correct – each class has the same predicted score. Thus\n",
    "the loss is not as high as before, but still reflects that the model is wrong.\n",
    "</COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3647b4b",
   "metadata": {},
   "source": [
    "Run this final cell.\n",
    "What is the relationship between its output and the outputs of the three cells above?\n",
    "Verify this relationship explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f725271",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.array([[100.,   0.,   0.],\n",
    "              [100.,   0.,   0.],\n",
    "              [100., 100., 100.]])\n",
    "\n",
    "t = np.array([0, 1, 0])\n",
    "\n",
    "softmax_crossentropy(p, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99171f93",
   "metadata": {},
   "source": [
    "<COGINST>\n",
    "The above describes a three independent predictions, each among three classes, where the true class labels are label-0, label-1, and label-0 respectively.\n",
    "This captures the first three scenarios in a single set of inputs; `softmax_crossentropy` is designed to compute the average loss across a batch of predictions, thus the output here is: $\\frac{0 + 100 + 1.099}{3}$\n",
    "</COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57c4f8c",
   "metadata": {},
   "source": [
    "## Defining your gradient descent and forward pass functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c6513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_step(tensors, learning_rate):\n",
    "    \"\"\"\n",
    "    Performs gradient-step in-place on each of the provides tensors \n",
    "    according to the standard formulation of gradient descent.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tensors : Union[Tensor, Iterable[Tensors]]\n",
    "        A single tensor, or an iterable of an arbitrary number of tensors.\n",
    "\n",
    "        If a `tensor.grad` is `None`for a specific tensor, the update on\n",
    "        that tensor is skipped.\n",
    "\n",
    "    learning_rate : float\n",
    "        The \"learning rate\" factor for each descent step. A positive number.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The gradient-steps performed by this function occur in-place on each tensor,\n",
    "    thus this function does not return anything\n",
    "    \"\"\"\n",
    "    # paste your solution from a previous notebook here\n",
    "    # <COGINST>\n",
    "    if isinstance(tensors, mg.Tensor):\n",
    "        # Only one tensor was provided. Pack\n",
    "        # it into a list so it can be accessed via\n",
    "        # iteration\n",
    "        tensors = [tensors]\n",
    "\n",
    "    for t in tensors:\n",
    "        if t.grad is not None:\n",
    "            t.data -= learning_rate * t.grad\n",
    "    # </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075fa6eb",
   "metadata": {},
   "source": [
    "Initialize your noggin plotter so that it will track two metrics: loss and accuracy\n",
    "\n",
    "```python\n",
    "plotter, fig, ax = create_plot(metrics=[\"loss\", \"accuracy\"])\n",
    "```\n",
    "\n",
    "Also, initialize your model parameters and batch-size. \n",
    "- Start off with a small number of neurons in your layer - try $N=3$. Increase number of parameters in your model to improve the quality of your result. You can use the visualization that we provide at the end of this notebook to get a qualitative feel for your notebook\n",
    "- A batch-size of 50 is fine, but feel free to experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb226236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running this code will recreate your model, re-initializing all of its parameters\n",
    "# Thus you must re-run this cell if you want to train your model from scratch again.\n",
    "\n",
    "# - Create the noggin figure using the code snippet above\n",
    "# - Set `batch_size = 50`: the number of predictions that we will make in each training step\n",
    "# - Create your model\n",
    "\n",
    "\n",
    "from noggin import create_plot\n",
    "plotter, fig, ax = create_plot(metrics=[\"loss\", \"accuracy\"])\n",
    "\n",
    "batch_size = 50  # <COGSTUB> set your batch-size to 50\n",
    "\n",
    "lr = 0.1 # <COGSTUB> specify your learning rate for gradient descent; try using 0.1\n",
    "\n",
    "# Initialize your Model class using 15 neurons, and the appropriate number of classes (how many tendrils are there?)\n",
    "# Feel free to change the number of neurons later\n",
    "model = Model(num_neurons=15, num_classes=num_tendril)  # <COGSTUB> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217234ee",
   "metadata": {},
   "source": [
    "Referring to the code that you used to train your universal function approximator, write code to train your model on the spiral dataset for a specified number of epochs. Remember to shuffle your training data before you form batches out of it. Also, remember to use the the `softmax_crossentropy` loss.\n",
    "\n",
    "Try training your model for 1000 epochs. A learning rate $\\approx 0.1$ is a sensible starting point. Watch the loss and accuracy curves evolve as your model trains.\n",
    "\n",
    "Below, you will be able to visualize the \"decision boundaries\" that your neural network learned. Try adjusting the number of neurons in your model, the number of epochs trained, the batch size, and the learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb5ac5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the loss function from mygrad, as directed above\n",
    "from mygrad.nnet.losses import softmax_crossentropy # <COGLINE>\n",
    "\n",
    "\n",
    "for epoch_cnt in range(1000):\n",
    "\n",
    "    # `idxs` will store shuffled indices, used to randomize the batches for\n",
    "    # each epoch\n",
    "    idxs = np.arange(len(xtrain))  # -> array([0, 1, ..., 9999])\n",
    "    np.random.shuffle(idxs)\n",
    "\n",
    "    for batch_cnt in range(0, len(xtrain) // batch_size):\n",
    "        \n",
    "        # get the batch-indices from `idxs` (refer to the universal function approx notebook)\n",
    "        batch_indices = idxs[batch_cnt * batch_size : (batch_cnt + 1) * batch_size]  # <COGSTUB>\n",
    "\n",
    "        # Use `batch_indices` to index into `xtrain` and `ytrain`.\n",
    "        batch = xtrain[batch_indices]  # <COGSTUB> your random batch of data, shape-(M, 2)\n",
    "        truth = ytrain[batch_indices]  # <COGSTUB> the true labels for this batch, shape-(M,)\n",
    "        \n",
    "        predictions = model(batch)  # <COGSTUB> make predictions with your model, shape-(M, 3) tensor of scores\n",
    "\n",
    "        # Compute the loss associated with our predictions\n",
    "        # The loss should be a Tensor so that you can do back-prop\n",
    "        loss = softmax_crossentropy(predictions, truth) # <COGSTUB>\n",
    "\n",
    "        # Use mygrad compute the derivatives for your model's parameters, so\n",
    "        # that we can perform gradient descent.\n",
    "        loss.backward()  # <COGINST>\n",
    "\n",
    "        # Update your model's parameters using `gradient_step`.\n",
    "        # Keep in mind that this function updates the parameters in-place; it doesn't return anything.\n",
    "        gradient_step(model.parameters, lr) # <COGLINE>\n",
    "        \n",
    "        # Compute the accuracy of the model's predictions\n",
    "        acc = accuracy(predictions, truth) # <COGSTUB> use your `accuracy` function\n",
    "\n",
    "        plotter.set_train_batch(\n",
    "            {\"loss\": loss.item(), \"accuracy\": acc}, batch_size=batch_size\n",
    "        )\n",
    "plotter.plot()\n",
    "\n",
    "# Running this cell will train your model; look back up to the noggin plot to watch\n",
    "# the loss and accuracy evolve over time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f694b8b9",
   "metadata": {},
   "source": [
    "This cell will allow you to visualize the decision boundaries that your model learned. We must define a function whose only input is the data, and whose out output is the softmax (**not** softmax crossentropy) of your classification scores.\n",
    "\n",
    "The red regions are all of the points where your model will say \"this is belongs to the red tendril\", and so on.\n",
    "Do you notice any anomalies in these decision boundaries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e3ae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "\n",
    "def dummy_function(x):\n",
    "    from mygrad.nnet.activations import softmax\n",
    "    with mg.no_autodiff:  # <- this tells mygrad that derivatives will not be needed and thus makes things more efficient\n",
    "        return softmax(model(x))\n",
    "\n",
    "fig, ax = data.visualize_model(dummy_function, entropy=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60301d3b",
   "metadata": {},
   "source": [
    "The following plot will display the \"uncertainty\" of your model. Purple means that the model is highly confident.\n",
    "Yellow means that the model is uncertain. \n",
    "\n",
    "Do the regions of uncertainty make sense here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b7a55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = data.visualize_model(dummy_function, entropy=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccd6d11",
   "metadata": {},
   "source": [
    "Try training your model for more iterations. You should also try adjusting the number of neurons in your model. How do the decision boundaries change when you decrease or increase the number of neurons in the model?"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python",
   "notebook_metadata_filter": "nbsphinx,-kernelspec"
  },
  "nbsphinx": {
   "execute": "never"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
