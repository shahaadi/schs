{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e88848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import mygrad as mg\n",
    "from mygrad.nnet.initializers import uniform\n",
    "\n",
    "\n",
    "mg.turn_memory_guarding_off()  # will help parts of this notebook run faster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85df4a84",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "\n",
    "In our work with the universal function approximation theorem and, subsequently, neural networks, we will frequently be making use of a family of non-linear functions, which are known as activation functions: $\\varphi(x)$\n",
    "The non-linearity of these functions are specifically meant to have an \"on/off\" shape to them; that is, the function should return $0$ (\"off\") across a wide domain of inputs, and a non-zero (\"on\") for all other inputs.\n",
    "\n",
    "Lastly, these non-linear functions are typically designed to be monotonic, meaning that they never \"reverse direction\" as we increase $x$.\n",
    "That is, as we increase $x$, once the function \"turns on\", it will never turn off. Or, once the function turns off, it will never turn back on.\n",
    "\n",
    "## The Sigmoid Function\n",
    "\n",
    "A textbook example of an activation function that has this \"on/off\" shape is the sigmoid function:\n",
    "\n",
    "\\begin{equation}\n",
    "\\varphi(x) = \\frac{1}{1 + e^{-x}}\n",
    "\\end{equation}\n",
    "\n",
    "Let's plot this function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e03ed4d",
   "metadata": {},
   "source": [
    "### Plotting our \"activation function\"\n",
    "\n",
    "Here we will import the `sigmoid` from `mygrad.nnet.activations`. Plot this function on the domain $[-10, 10]$.\n",
    "\n",
    "Note that `sigmoid` is designed to be a vectorized function just like NumPy's functions. I.e. you can feed it an array of values, and it will return an array of corresponding outputs:\n",
    "\n",
    "```python\n",
    ">>> x = np.array([-1, 0, 1])\n",
    ">>> sigmoid(x)  # sigmoid is vectorized\n",
    "Tensor([0.26894142, 0.5       , 0.73105858])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30592dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mygrad.nnet.activations import sigmoid\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "x = np.linspace(-10, 10, 1000)  # <COGSTUB> use np.linspace to create 1,000 evenly-spaced points between [-10, 10]\n",
    "\n",
    "y = sigmoid(x)  # <COGSTUB> evaluate the sigmoid function for all `x` values. \n",
    "\n",
    "ax.plot(x, y)\n",
    "\n",
    "ax.grid()\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"sigmoid(x)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe1f054",
   "metadata": {},
   "source": [
    "You should see that the sigmoid has the aforementioned monotonic \"on/off\" pattern to it:\n",
    "for increasingly-negative $x$, $\\varphi(x) \\rightarrow 0$ (off), and for increasingly-positive $x$, $\\varphi(x) \\rightarrow 1$ (on).\n",
    "Furthermore, as $x$ moves from negative to positive values, the activation increases without ever decreasing, thus it is monotonic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8867462d",
   "metadata": {},
   "source": [
    "## The ReLU function\n",
    "\n",
    "Another popular – and much more frequently used – activation function is the \"rectified linear unit\", or ReLU function.\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{relu}(x) = \\begin{cases} \n",
    "      0 & x\\leq 0 \\\\\n",
    "      x & 0 < x\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Import the `relu` function from mygrad via: `from mygrad.nnet.activations import relu`, and plot this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c0090d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mygrad.nnet.activations import relu\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "x = np.linspace(-10, 10, 1000)  # <COGSTUB> use np.linspace to create 1,000 evenly-spaced points between [-10, 10]\n",
    "\n",
    "y = relu(x)  # <COGSTUB> evaluate the relu function for all `x` values. \n",
    "\n",
    "ax.plot(x, y)\n",
    "\n",
    "ax.grid()\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"relu(x)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e908e67",
   "metadata": {},
   "source": [
    "Check your understanding of the qualities that we are looking for in an activation function: is this a non-linear, monotonic function with the desired \"on/off\" pattern? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93db3a87",
   "metadata": {},
   "source": [
    "## \"Steering\" an activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eafb8c",
   "metadata": {},
   "source": [
    "Thus far, we have learned about the basic shapes/qualities that we are looking for in an activation function, but we haven't seen why these are actually useful. What is the point of these \"on/off\" patterns, and how do these activation functions help us create flexible mathematical models?\n",
    "\n",
    "To help answer these questions, it is essential to recognize that we will actually be using *parameterized* activation functions\n",
    "\n",
    "\\begin{equation}\n",
    "\\varphi(w, b; x)\n",
    "\\end{equation}\n",
    "\n",
    "whose parameters $w$ and $b$ can be tuned to modify the shape and orientation of the activation function.\n",
    "These parameters are essential to the utility of these activation functions; $w$ and $b$ **enables an activation function to behave like a neuron**, where it can learn to selective send signals (i.e. \"turn on\") for some values of $x$ and to remain \"off\" for other values of $x$.\n",
    "\n",
    "It is very often the case – and will always be the case in this course – that $w$ and $b$ serve as a linear weight and bias, respectively, on $x$.\n",
    "That is, our activation function will  have the particular form:\n",
    "\n",
    "\\begin{equation}\n",
    "\\varphi(w, b; x) = \\varphi(x \\cdot w + b)\n",
    "\\end{equation}\n",
    "\n",
    "It is important to recognize that $x$ is an independent input to $\\varphi(w, b; x)$, whereas $w$ and $b$ are trainable/learnable/tunable parameters for the activation function.\n",
    "When we eventually create a mathematical model out of many of these activation functions (a **network** of these neurons!), the model's trainable parameters will consist of the $w$ and $b$ parameters for each of its activations functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f356e441",
   "metadata": {},
   "source": [
    "Running the following cell will create an interactive plot of $\\varphi(x \\cdot w + b)$, where $\\varphi$ is the sigmoid function.\n",
    "Use the sliders to adjust the values of $w$ and $b$, to develop an intuition how these parameters can be used to modify the activation function.\n",
    "Answer the following questions:\n",
    "- Which parameter can be used to change *where* the activation function turns from \"off\" to \"on\"?\n",
    "- Which parameter affects the actual shape of the sigmoid curve; e.g. changes how steep the sigmoid is?\n",
    "- Can either of these parameters be used to flip the activation function's on/off pattern? That is, can you create an activation function that goes from \"on\" to \"off\", instead of from \"off\" to \"on\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27fb8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell and experiment with the plot sliders\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "\n",
    "w = 1.0\n",
    "b = 0.0\n",
    "\n",
    "\n",
    "def param_sigmoid(x, w=1.0, b=0.0):\n",
    "    return sigmoid(w * x + b)\n",
    "\n",
    "\n",
    "ax.plot(x, param_sigmoid(x), ls=\"--\", label=\"original\")\n",
    "(line,) = ax.plot(x, param_sigmoid(x, w=w, b=b))\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "ax.set_title(f\"sigmoid(w * x + b)\")\n",
    "ax.set_xlabel(\"x\")\n",
    "\n",
    "\n",
    "@interact(\n",
    "    b=widgets.FloatSlider(min=-5, max=5, step=0.3, value=0),\n",
    "    w=widgets.FloatSlider(min=-5, max=5, step=0.2, value=1),\n",
    ")\n",
    "def update(w=1.0, b=0.0):\n",
    "    line.set_ydata(param_sigmoid(x, w=w, b=b))\n",
    "    fig.canvas.draw_idle()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fdef41",
   "metadata": {},
   "source": [
    "Let's repeat this analysis for $\\mathrm{relu}(x \\cdot w + b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b86c503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell and experiment with the plot sliders\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "\n",
    "w = 1.0\n",
    "b = 0.0\n",
    "\n",
    "\n",
    "def param_relu(x, w=1.0, b=0.0):\n",
    "    return relu(x * w + b)\n",
    "\n",
    "\n",
    "ax.plot(x, param_relu(x), ls=\"--\", label=\"original\")\n",
    "(line,) = ax.plot(x, param_relu(x, w=w, b=b))\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "ax.set_title(\"relu(x * w + b)\")\n",
    "ax.set_xlabel(\"x\")\n",
    "\n",
    "@interact(\n",
    "    b=widgets.FloatSlider(min=-5, max=5, step=0.3, value=0),\n",
    "    w=widgets.FloatSlider(min=-5, max=5, step=0.2, value=1),\n",
    ")\n",
    "def update(w=1.0, b=0.0):\n",
    "    line.set_ydata(param_relu(x, w=w, b=b))\n",
    "    fig.canvas.draw_idle()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8116d7",
   "metadata": {},
   "source": [
    "## Randomly generated \"function approximators\"\n",
    "\n",
    "We will soon be introduced to the universal function approximation theorem, which has us construct an approximating function $F$ that is the sum of $N$ parameterized activation functions (a.k.a **neurons**), each one scaled by an additional learnable parameter $v_i$:\n",
    "\n",
    "\\begin{equation}\n",
    "F(\\{v_i\\}_{i=1}^{N}, \\{w_i\\}_{i=1}^{N}, \\{b_i\\}_{i=1}^{N}; x) = \\sum_{i=1}^{N} v_{i}\\varphi(x \\cdot w_{i} + b_{i})\n",
    "\\end{equation}\n",
    "\n",
    "The idea here is that, given a large enough number of **neurons**, $N$, with the appropriately-tuned parameters $\\{v_i\\}_{i=1}^{N}, \\{\\vec{w}_i\\}_{i=1}^{N}, \\{b_i\\}_{i=1}^{N}$, we can make $F(\\{v_i\\}_{i=1}^{N}, \\{\\vec{w}_i\\}_{i=1}^{N}, \\{b_i\\}_{i=1}^{N}; x)$ adopt whatever shape we'd like (as long as that shape is continuous, smooth, and bounded).\n",
    "\n",
    "To develop an intuition for this, let's plot forms of $F$ where we have *drawn random values for* $\\{v_i\\}_{i=1}^{N}, \\{\\vec{w}_i\\}_{i=1}^{N}, \\{b_i\\}_{i=1}^{N}$.\n",
    "We will see that the resulting plot of $F(x)$ should be able to display some strikingly-diverse shapes.\n",
    "\n",
    "In the following cell, pick a value for `num_neurons` (which represents $N$) and execute the cell to plot the randomly-arranged function $F$.\n",
    "Try running the cell multiple times to see the various forms of $F$ that can manifest for that value of `num_neurons`.\n",
    "Also try increasing the value of `num_neurons`; you should see that $F$ can take on increasingly-complicated shapes as you increase that value.\n",
    "\n",
    "Each grey dashed curve corresponds to one of the scaled neurons, $v_{i}\\varphi(x \\cdot w_{i} + b_{i})$.\n",
    "The solid blue curve is their sum, $F(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295471c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this a bunch of times to see different combinations of neurons\n",
    "\n",
    "num_neurons = 4  # try adjusting this number\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "xlim = min(100, num_neurons * 10)\n",
    "\n",
    "x = np.linspace(-xlim, xlim, 1000)\n",
    "\n",
    "\n",
    "w = uniform(num_neurons, lower_bound=-1, upper_bound=1) * 2\n",
    "b = uniform(num_neurons, lower_bound=-1, upper_bound=1) * 10\n",
    "v = uniform(num_neurons, lower_bound=-1, upper_bound=1) * 2\n",
    "\n",
    "out = 0.0\n",
    "\n",
    "with mg.no_autodiff:\n",
    "    for i in range(num_neurons):\n",
    "        y = v[i] * param_sigmoid(x, w=w[i], b=b[i])\n",
    "        ax.plot(x, y, ls=\"--\", c=\"gray\", alpha=0.5)\n",
    "        out += y\n",
    "\n",
    "\n",
    "ax.plot(x, out)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"F(x)\")\n",
    "ax.set_title(f\"A randomly-initialized F(x), using N={num_neurons} neurons.\")\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef53de0",
   "metadata": {},
   "source": [
    "## Summary and looking ahead\n",
    "\n",
    "We were introduced to the **activation functions** sigmoid and ReLU (rectified linear unit).\n",
    "What these, and other activation functions ($\\varphi$), have in common is that they are **monotonic non-linear functions, with clear \"on\"/\"off patterns**.\n",
    "A parameterized activation function $\\varphi(w, b; x)$ is what we refer to as a **neuron** in the parlance of neural networks and deep learning. The learnable parameters $w$ and $b$ determine which values of $x$ will cause the neuron $\\varphi(w, b; x)$ to \"activate\" (or, \"turn on\").\n",
    "\n",
    "It is most often the case that $w$ and $b$ represent a linear scale and bias on $x$: $\\varphi(w, b; x) = \\varphi(x \\cdot w + b)$. \n",
    "The interactive plots in this notebook gave us a feel for how $w$ and $b$ can \"steer\" the shape of a neuron's activation pattern. At last, we saw that summing $N$ neurons with randomly-drawn parameters (in addition to activation-scaling parameters $v_i$), can produce highly-diverse shapes. \n",
    "Indeed, this is the foundation for creating a so-called universal function approximator, $F(\\{v_i\\}_{i=1}^{N}, \\{w_i\\}_{i=1}^{N}, \\{b_i\\}_{i=1}^{N}; x)$, which can take on any shape that we'd like, as long as we have a large enough value for $N$ *and* as long as we can find the appropriate values for $\\{v_i\\}_{i=1}^{N}, \\{w_i\\}_{i=1}^{N}, \\{b_i\\}_{i=1}^{N}$...\n",
    "\n",
    "Suppose we want $F(x)$ to resemble the cosine function on $[-2 \\pi, 2\\pi]$, how *do* we find values $\\{v_i\\}_{i=1}^{N}, \\{w_i\\}_{i=1}^{N}, \\{b_i\\}_{i=1}^{N}$ to achieve this?\n",
    "We'll use gradient descent, which is the technique that we previously used to iteratively our linear model!\n",
    "In fact, we'll solve this exact problem in the next notebook, which is all about fitting a universal function approximator."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python",
   "notebook_metadata_filter": "nbsphinx,-kernelspec"
  },
  "nbsphinx": {
   "execute": "never"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
