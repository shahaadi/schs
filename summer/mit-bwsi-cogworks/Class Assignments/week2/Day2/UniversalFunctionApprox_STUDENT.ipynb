{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddc89cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mygrad as mg\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca87a83",
   "metadata": {},
   "source": [
    "# Training a Universal Function Approximator\n",
    "\n",
    "The purpose of a function approximator is to be able to approximate some function $f(x)$ on some finite domain $x \\in [x_{\\mathrm{min}}, x_{\\mathrm{max}}]$, whose form may be extremely complicated (or unknown!), with a simpler **model function**, $F_{\\mathrm{model}}(x)$.\n",
    "The universal function approximation (UFA) theorem provides us with the general form for this model function:\n",
    "\n",
    "\\begin{equation}\n",
    "F(\\{v_i\\}_{i=1}^{N}, \\{w_i\\}_{i=1}^{N}, \\{b_i\\}_{i=1}^{N}; x) = \\sum_{i=1}^{N} v_{i}\\varphi(x \\cdot w_{i} + b_{i})\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "where $(\\{v_i\\}_{i=1}^{N}, \\{w_i\\}_{i=1}^{N}, \\{b_i\\}_{i=1}^{N})$ are our **model's parameters**.\n",
    "The **size** of our model, $N$, determines the number of parameters in our model. We are responsible for choosing a value for $N$.\n",
    "\n",
    "$\\varphi(\\cdot)$ is a non-linear, continuous, and bounded function.\n",
    "That is, it has the \"on/off\" pattern of the so-called activation functions that we analyzed in the previous notebook.\n",
    "A common example of $\\varphi(\\cdot)$ is the \"sigmoid\" function:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma(wx + b) = \\frac{1}{1 + e^{-(wx + b)}}\n",
    "\\end{equation}\n",
    "\n",
    "The **universal function approximation theorem** says that, given a large enough value for $N$, we can *always* find values for $(\\{v_i\\}_{i=1}^{N}, \\{w_i\\}_{i=1}^{N}, \\{b_i\\}_{i=1}^{N})$ in order to make $F(x)$ an arbitrarily good approximation of $f(x)$.\n",
    "\n",
    "That is, for any positive value $\\varepsilon$ we can ensure that $F_{\\mathrm{model}}(x)$ and $f_{\\mathrm{true}}(x)$ never have a discrepancy larger than $\\varepsilon$ within some finite domain for $x$.\n",
    "\n",
    "\\begin{equation}\n",
    "| F_{\\mathrm{model}}( x ) - f_{\\mathrm{true}} ( x ) | < \\varepsilon \\;,\\; x \\in [x_{\\mathrm{min}}, x_{\\mathrm{max}}]\n",
    "\\end{equation}\n",
    "\n",
    "Put another way: You can ask me to pick some (positive) value for $\\varepsilon$. The universal function approximation theorem says that, no matter what value of $\\varepsilon$ I pick, you can *always* find a model size ($N$) and find values for $((v_i)_{i=1}^{N}, (w_i)_{i=1}^{N}, (b_i)_{i=1}^{N})$ such that $| F_{\\mathrm{model}}( x ) - f_{\\mathrm{true}} ( x ) | < \\varepsilon $, as long as we stick to the agreed upon finite domain for $x$.\n",
    "In general, the smaller the value of $\\varepsilon$ that you request, the larger $N$ will need to be to make $F_{\\mathrm{model}}$ a closer approximation of $f_{\\mathrm{true}}$.\n",
    "\n",
    "In later work we will see that the UFA theorem is even more general than what we covered here; it extends to vector-valued functions that operate on vector spaces as well:\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{F}(\\{\\vec{v}_i\\}_{i=1}^{N}, \\{\\vec{w}_i\\}_{i=1}^{N}, \\{b_i\\}_{i=1}^{N}; \\vec{x}) = \\sum_{i=1}^{N} \\vec{v}_{i}\\varphi(\\vec{x} \\cdot \\vec{w}_{i} + b_{i})\n",
    "\\end{equation}\n",
    "\n",
    "this will be useful when we are dealing with non-scalar input data (e.g. $\\vec{x}$ stores the pixels of an image), and when our function needs to return multiple values (e.g. $\\vec{F(\\vec{x}}$ returns confidence scores that an image depicts any one of ten classes of objects). \n",
    "\n",
    "(here is a more rigorous statement of the universal function approximation theorem: https://en.wikipedia.org/wiki/Universal_approximation_theorem)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a70f01c",
   "metadata": {},
   "source": [
    "## Our problem\n",
    "Here, we will find values for the parameters $N,v_{i},b_{i},w_{i}$  (where $i=1,\\cdots ,N$) such that $F(x)$ approximates \n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = \\cos(x)\\\\\n",
    "x \\in [-2\\pi, 2\\pi]\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "We will use the aforementioned sigmoid function, $\\sigma(x)$, as our activation function $\\varphi(\\cdot)$.\n",
    "\n",
    "Our model size, $N$, is a **hyper parameter**, which we must find through trial and error, or some other means. I.e. $N$ is not something we can determine directly via gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a674ca",
   "metadata": {},
   "source": [
    "### Plotting our \"activation function\"\n",
    "\n",
    "Here we will import the `sigmoid` from `mygrad.nnet.activations`. Plot this function on the domain $[-10, 10]$.\n",
    "\n",
    "Note that `sigmoid` is designed to be a vectorized function just like NumPy's functions. I.e. you can feed it an array of values, and it will return an array of corresponding outputs:\n",
    "\n",
    "```python\n",
    ">>> x = np.array([-1, 0, 1])\n",
    ">>> sigmoid(x)  # sigmoid is vectorized\n",
    "Tensor([0.26894142, 0.5       , 0.73105858])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0240d36",
   "metadata": {},
   "source": [
    "Let's plot the `sigmoid` activation function on the domain `[-10, 10]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54af6b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to import the sigmoid activation function\n",
    "\n",
    "from mygrad.nnet.activations import sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf6c0ea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting sigmoid(x)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "x = np.linspace(-10, 10, 1000) # use np.linspace to create 1,000 evenly-spaced points between [-10, 10]$\\varepsilon$\n",
    "\n",
    "y = sigmoid(x) # evaluate the sigmoid function for all `x` values. \n",
    "\n",
    "ax.plot(x, y)\n",
    "\n",
    "ax.grid()\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"sigmoid(x)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cad1fa",
   "metadata": {},
   "source": [
    "### Write a gradient-descent function\n",
    "\n",
    "**Simply copy and paste the `gradient_step` function that you wrote/used in the Gradient Descent notebook and the Linear Regression notebook**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79cfd2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_step(tensors, learning_rate):\n",
    "    \"\"\"\n",
    "    Performs gradient-step in-place on each of the provides tensors \n",
    "    according to the standard formulation of gradient descent.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tensors : Union[Tensor, Iterable[Tensor]]\n",
    "        A single tensor, or an iterable (e.g a list) of an arbitrary number of tensors.\n",
    "\n",
    "        If a `tensor.grad` is `None`for a specific tensor, the update on\n",
    "        that tensor is skipped.\n",
    "\n",
    "    learning_rate : float\n",
    "        The \"learning rate\" factor for each descent step. A positive number.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The gradient-steps performed by this function occur in-place on each tensor,\n",
    "    thus this function does not return anything\n",
    "    \"\"\"\n",
    "    # If `tensors` is a single Tensor, make it into a list containing that tensor\n",
    "    if isinstance(tensors, mg.Tensor):\n",
    "        # Only one tensor was provided. Pack\n",
    "        # it into a list so it can be accessed via\n",
    "        # iteration\n",
    "        tensors = [tensors]\n",
    "\n",
    "    # Iterate over `tensors`, which is a list of Tensor-instances, using a for-loop.\n",
    "    #\n",
    "    # Update each tensor's underlying data (`.data`) using the gradient-step.\n",
    "    # If the tensor's gradient is not set (i.e. it is `None`), skip the tensor\n",
    "    #\n",
    "    #  For each tensor in tensors...\n",
    "    #      If the .grad of tensor is not None...\n",
    "    #          Update the .data attribute of the tensor using the gradient-descent step\n",
    "    for t in tensors:\n",
    "        if t.grad is not None:\n",
    "            t.data -= learning_rate * t.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a912f165",
   "metadata": {},
   "source": [
    "## Defining Our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41d41b1",
   "metadata": {},
   "source": [
    "### Working with \"Batches\" of Data\n",
    "\n",
    "There are important advantages to updating our model using gradient descent based on *batches* of data, instead of based on individual predictions. It is both computationally inefficient to train our model by passing it one datum, $x$, at a time, plus the gradient of our loss function will be a crude estimate of the true gradient of the loss function if it is based only on one sample of data at a time. \n",
    "\n",
    "Thus we will want to pass in a **batch** of $M$ pieces of input data, $\\{x_{j}\\}_{j=0}^{M-1}$, and evaluate our model for each of these values independently.  That is, we will pass on a batch of $M$ pieces of data and produce $M$ corresponding predictions from our model.\n",
    "\n",
    "We will pass in a \"batch\" of $M$ input values, $X_{\\mathrm{batch}}$, as a shape-$(M, 1)$ vector to get $M$ corresponding predictions:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "F\\big(X_{\\mathrm{batch}}=\\begin{bmatrix}\n",
    "       x_{1} \\\\\n",
    "       x_{2} \\\\\n",
    "       \\vdots \\\\\n",
    "       x_{m}\n",
    "     \\end{bmatrix}\\big) \\rightarrow \n",
    "\\begin{bmatrix}\n",
    "       y^{\\mathrm{pred}}_{1} \\\\\n",
    "       y^{\\mathrm{pred}}_{2} \\\\\n",
    "       \\vdots \\\\\n",
    "       y^{\\mathrm{pred}}_{m}\n",
    "     \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "Each prediction is made only on its corresponding piece of input data. Thus, prediction $y^{\\mathrm(pred)}_j = F(\\{v_i\\}, \\{w_i\\}, \\{b_i\\}; x_j )$ depends only on:\n",
    "\n",
    "- our model's parameters: $(v_i)_{i=1}^{N}, (w_i)_{i=1}^{N}, (b_i)_{i=1}^{N}$ \n",
    "- datum $x_j$\n",
    "\n",
    "it is **not** impacted by any of the other pieces of data in the batch. This is very important to keep in mind!\n",
    "\n",
    "We will make our $M$ predictions for the batch using vectorization and not for-loops. $X_{\\mathrm{batch}}$ will be a shape-$(M, 1)$ numpy-array. It will soon become clear why we use the shape $(M, 1)$ instead of the more intuitive shape $(M,)$. \n",
    "\n",
    "Similarly, we can store our model's parameters in tensors of shrewdly-selected shapes to enable efficient vectorized computations of $F(X_{\\mathrm{batch}})$:\n",
    "- $(w_i)_{i=1}^{N}$ will be stored in a shape-(1, N) tensor\n",
    "- $(b_i)_{i=1}^{N}$ will be stored in a shape-(N,) tensor\n",
    "- $(v_i)_{i=1}^{N}$ will be stored in a shape-(N, 1) tensor\n",
    "\n",
    "These shapes will also make it trivial for us to extend our code to working with vector-valued inputs/outputs instead of scalars in the future.\n",
    "\n",
    "Let's see why these shapes pay off for us. Conveniently, we can calculate\n",
    "\n",
    "\\begin{equation}\n",
    "x_j \\cdot w_{i}\n",
    "\\end{equation}\n",
    "\n",
    "for all combinations of $j=1,\\cdots ,M$ and $i=1,\\cdots , N$ via simple matrix multiplication between the shape-$(M, 1)$ `x` and the shape-$(1, N)$ `w`, producing a shape-$(M, N)$ output matrix storing those $M \\times N$ combinations of products. \n",
    "\n",
    "And thus the following expression:\n",
    "\n",
    "\\begin{equation}\n",
    "\\varphi(x_{j} \\cdot w_{i} + b_{i})\n",
    "\\end{equation}\n",
    "\n",
    "can be performed for all combinations of $j$ and $i$ via the following line of code:\n",
    "\n",
    "```python\n",
    "sigmoid(mg.matmul(x, w) + b) # matmul[(M,1) w/ (1, N)] + (N,) --> (M, N)\n",
    "```\n",
    "\n",
    "where each $b_{i}$ is added to the ith column of the shape-(M, N) tensor via [broadcasted addition](https://numpy.org/doc/stable/user/basics.broadcasting.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91483591",
   "metadata": {},
   "source": [
    "Multiplying each such term by $v_{i}$ and summing over $i$ is just another matrix multiplication:\n",
    "\n",
    "```python\n",
    "model_out = mg.matmul(sig_out, v)  # matmul[(M, N) w/ (N, 1)] --> (M, 1)\n",
    "```\n",
    "\n",
    "Thus, in full, our model's prediction\n",
    "\n",
    "\\begin{equation}\n",
    "F(\\{v_i\\}, \\{w_i\\}, \\{b_i\\}; x_j ) = \\sum_{i=1}^{N} v_{i}\\varphi(x_{j} \\cdot w_{i} + b_{i})\n",
    "\\end{equation}\n",
    "\n",
    "can be performed over the entire batch via:\n",
    "\n",
    "```python\n",
    "sig_out = sigmoid(mg.matmul(x, w) + b)  # matmul[(M,1) w/ (1, N)] + (N,) --> (M, N)\n",
    "model_out = mg.matmul(sig_out, v)       # matmul[(M, N) w/ (N, 1)] --> (M, 1)\n",
    "```\n",
    "\n",
    "Thus `model_out` is a shape-$(M, 1)$ tensor that holds the prediction of our model, corresponding with each datum in our shape-(M, 1) batch.\n",
    "\n",
    "The code that was just laid out here will help you fill out the upcoming `Model.__call__` method, such that it accepts a batch of shape-$(M, 1)$, and produces a shape-$(M, 1)$ tensor of corresponding predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b37e679c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will draw our model's initial parameter values from a \n",
    "# normal distribution (a.k.a Gaussian distribution or \"bell curve\")\n",
    "# centered at 0 and with a standard deviation of 1\n",
    "from mygrad.nnet.initializers import normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c51671d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \n",
    "    def initialize_params(self, num_neurons: int):\n",
    "        \"\"\"\n",
    "        Randomly initializes and sets values for  `self.w`,\n",
    "        `self.b`, and `self.v`.\n",
    "        \n",
    "        Uses `mygrad.nnet.initializers.normal to randomly draw tensor\n",
    "        values for w, b, and v from a normal distribution with\n",
    "        0-mean and std-dev of 1.\n",
    "        \n",
    "        self.w : shape-(1, N)\n",
    "        self.b : shape-(N,)\n",
    "        self.v : shape-(N, 1)\n",
    "        \n",
    "        where `N` is the number of \"neurons\" in the model.\n",
    "        \"\"\"\n",
    "        # Use the `normal` distribution to draw all of your v, w, and b values.\n",
    "        #\n",
    "        # E.g. `normal(num_neuron)` will return a shape-(N,) tensor of \n",
    "        # randomly-drawn values (where N is the number of neurons)\n",
    "        #\n",
    "        # `normal(1, num_neurons)` will return a shape-(1, N) tensor of\n",
    "        # randomly-drawn values.\n",
    "        #\n",
    "        # assign `self.w`, `self.b`, and `self.v` each a tensor value drawn from\n",
    "        # the appropriate distribution\n",
    "        self.w = normal(1, num_neurons)# draw the appropriately-shaped tensor from `normal` for w\n",
    "        self.b = normal(num_neurons)# draw the appropriately-shaped tensor from `normal` for b\n",
    "        self.v = normal(num_neurons)# draw the appropriately-shaped tensor from `normal` for v\n",
    "        \n",
    "        # note that this method does not return anything -- it updates `self.w`, `self.b`, `self.v` in-place.\n",
    "\n",
    "    def __init__(self, num_neurons: int):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_neurons : int\n",
    "            The number of 'neurons', N, to be included in the model.\n",
    "        \"\"\"\n",
    "        self.N = num_neurons # set self.N equal to `num_neurons`\n",
    "        \n",
    "        # Use `self.initialize_params()` to draw random values for\n",
    "        # `self.w`, `self.b`, and `self.v` \n",
    "        # STUDENT CODE HERE\n",
    "        self.initialize_params(num_neurons)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Performs a so-called 'forward pass' through the model\n",
    "        on the specified data. I.e. uses the model to\n",
    "        make a prediction based on `x`.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : array_like, shape-(M, 1)\n",
    "            An array of M observations.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        prediction : mygrad.Tensor, shape-(M, 1)\n",
    "            A corresponding tensor of M predictions based on\n",
    "            the form of the universal approximation theorem.\n",
    "        \"\"\"\n",
    "        # Refer to the code that was described in the section above for guidance\n",
    "        \n",
    "        # STUDENT CODE HERE\n",
    "        sig_out = sigmoid(x @ self.w + self.b) \n",
    "        out = sig_out @ self.v \n",
    "        return out\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        \"\"\" A convenience function for getting all the parameters of our model.\n",
    "        \n",
    "        This can be accessed as an attribute, via `model.parameters` \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[Tensor, ...]\n",
    "            A tuple containing all of the learnable parameters for our model\"\"\"\n",
    "        # Return a tuple containing all of the model's trainable parameters\n",
    "        # STUDENT CODE HERE\n",
    "    \n",
    "    def load_parameters(self, w, b, v):\n",
    "        # Don't change this\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.v = v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602e33da",
   "metadata": {},
   "source": [
    "### Writing a loss function\n",
    "\n",
    "We will use the following loss function\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathscr{L}_{1} = \\frac{1}{M}\\sum_{j=1}^{M} | y_{j}^{\\mathrm{pred}} - y_{j}^{\\mathrm{true}} |\n",
    "\\end{equation}\n",
    "\n",
    "This is often referred to as \"L-1\" (\"ell one\") loss.\n",
    "\n",
    "Write the function `l1_loss`, which accepts the shape-$(M,1)$ batch of **predictions** from our model along with the shape-$(M, 1)$ **true** values (which we are hoping to approximate) and returns the average loss, $L$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d896a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_loss(pred, true):\n",
    "    \"\"\"\n",
    "    Computes Mean[|pred - true|]\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pred : mygrad.Tensor, shape=(M,)\n",
    "        The values predicted by our model.\n",
    "        \n",
    "    true : mygrad.Tensor, shape=(M,)\n",
    "        The values that our model *should* have predicted.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mygrad.Tensor, shape=()\n",
    "        The l1-loss averaged over the size-M batch of predictions. This should\n",
    "        be a 0D tensor.\n",
    "    \"\"\"\n",
    "    # this is just like the mean squared-error loss, but instead of\n",
    "    # squaring each difference you take the absolute value using \n",
    "    # the function `np.abs`\n",
    "    \n",
    "    diff = pred - true# shape-(M, )\n",
    "    abs_diff = np.abs(diff) # shape-(M, )\n",
    "    mean_abs_diff = np.sum(abs_diff) / len(abs_diff) # shape-()\n",
    "    return mean_abs_diff "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2dae6b",
   "metadata": {},
   "source": [
    "Test your implementation of `l1_loss`. \n",
    "\n",
    "Create three pairs of `pred` / `true` tensors.\n",
    "For each pair, compute by hand what $\\frac{1}{M}\\sum_{j=1}^{M} | y_{j}^{\\mathrm{pred}} - y_{j}^{\\mathrm{true}} |$ should be.\n",
    "Then check that `l1_loss` returns the expected results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92eea940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss should be (|1.0 - 0.0| + |-1.0 - 0.0|) / 2 = 1: Tensor(1.)\n",
      "loss should be (|0.0 - 0.0| + |-1.0 - 0.0|) / 2 = 1 / 2: Tensor(0.5)\n",
      "loss should be (|0.0 - 10.0| + |-10.0 - 0.0|) / 2 = 10: Tensor(10.)\n"
     ]
    }
   ],
   "source": [
    "# STUDENT CODE HERE\n",
    "# <COGINST>\n",
    "pred = mg.tensor([1.0, -1.0])\n",
    "true = mg.tensor([0.0, 0.0])\n",
    "\n",
    "print(\"loss should be (|1.0 - 0.0| + |-1.0 - 0.0|) / 2 = 1:\", l1_loss(pred, true))\n",
    "\n",
    "\n",
    "pred = mg.tensor([0.0, -1.0])\n",
    "true = mg.tensor([0.0, 0.0])\n",
    "\n",
    "print(\"loss should be (|0.0 - 0.0| + |-1.0 - 0.0|) / 2 = 1 / 2:\", l1_loss(pred, true))\n",
    "\n",
    "pred = mg.tensor([0.0, -10.0])\n",
    "true = mg.tensor([10.0, 0.0])\n",
    "\n",
    "print(\"loss should be (|0.0 - 10.0| + |-10.0 - 0.0|) / 2 = 10:\", l1_loss(pred, true))\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4babd969",
   "metadata": {},
   "source": [
    "## Preparing the training data\n",
    "\n",
    "You will create a numpy-array or *constant-Tensor* that samples the domain $[-2\\pi, 2\\pi]$ using 1,000 evenly-spaced points. Call this `train_data`. This should have the shape of (1000, 1). (You will do this lower down in the notebook).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668a8e1e",
   "metadata": {},
   "source": [
    "### Training Our Approximating Function! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fada580c",
   "metadata": {},
   "source": [
    "We will need to make take randomized \"batches\" of our training data, and use them to train our model. Each time we process all of the batches in our training data, we have completed an \"epoch\" of training.\n",
    "\n",
    "Below, we will use batches of size-25. Thus we will need to process $1000/25 = 40$ batches to complete an epoch of training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddaf37f",
   "metadata": {},
   "source": [
    "Here, we will set up our training data, initialize our model's parameters, set our batch size, and define the function that we are attempting to approximate. We will also create a plot that updates during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fc115e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2496708921.py, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 18\u001b[0;36m\u001b[0m\n\u001b[0;31m    model = # Initialize your model; start off with N=10 \"neurons\"\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Running this code will recreate your model, re-initializing all of its parameters\n",
    "# Thus you must re-run this cell if you want to train your model from scratch again.\n",
    "\n",
    "# Create the noggin figure (don't change this)\n",
    "from noggin import create_plot\n",
    "plotter, fig, ax = create_plot(metrics=[\"loss\"])\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "\n",
    "# Creating the shape-(1000, 1) training data...\n",
    "#\n",
    "# Here, we use np.linspace to create 1,000 evenly-spaced values between [-2pi, 2pi]\n",
    "#\n",
    "# Then, we use reshape this array from shape-(1000,) to shape-(1000, 1)\n",
    "train_data = np.linspace(-2*np.pi, 2*np.pi, 1000).reshape(1000, 1)\n",
    "\n",
    "\n",
    "model = # Initialize your model; start off with N=10 \"neurons\"\n",
    "\n",
    "# Set the batch-size (referred to as M in our equations)\n",
    "# This is the number of predictions that we will make in each training step\n",
    "batch_size = #  # start with the value 25\n",
    "\n",
    "# Define the function `true_f`, which should just accept `x` and return `np.cos(x)`\n",
    "# (or any other function that you want to approximate later on)\n",
    "def true_f(x): \n",
    "    # STUDENT CODE HERE\n",
    "\n",
    "\n",
    "# We will store our model's weights in this list every 10 epochs \n",
    "# so that we can assess what our model's predictions look like mid-training.\n",
    "# Don't change this\n",
    "params = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060a036a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After you complete the code in this cell, running it will train your model. \n",
    "# You can run it consecutively with different learning rates to continue training your model\n",
    "# at different learning rates.\n",
    "\n",
    "# When you run this cell, the plot above it will update in real-time. Check it out!\n",
    "\n",
    "# `lr` is our learning rate for gradient descent\n",
    "# Try a using value of 0.01 to start.\n",
    "#\n",
    "# If your loss function plateaus, you can reduce this by 10x and\n",
    "# resume training (i.e. just re-run this cell) to further reduce the loss.\n",
    "lr = #\n",
    "\n",
    "# An \"epoch\" means that the training data has been processed in full.\n",
    "# Thus two epochs of training means that the model has processed / been updated\n",
    "# by each datum in the dataset twice. \n",
    "#\n",
    "# We will train for 1000 epochs; you can change this if you'd like\n",
    "for epoch_cnt in range(1000):  \n",
    "    ###### Drawing random indices to shuffle our data ########### \n",
    "    # For each new epoch we need to create randomly-drawn batches of data...\n",
    "\n",
    "    # Create a numpy array of indices [0, 1, ..., T-1] where\n",
    "    # T is the size  the number of pieces of data  of the training data\n",
    "    idxs = #\n",
    "\n",
    "    # Use np.random.shuffle to shuffle these indices.\n",
    "    # Note that this functions will sort  the indices *in-place* - it does\n",
    "    # not return anything\n",
    "    \n",
    "    # STUDENT CODE HERE\n",
    "\n",
    "    # Let's keep track of our model's progress. Every 10 epochs we'll\n",
    "    # record our model's weights so that we can visualize what its\n",
    "    # predictions look like as it was training.\n",
    "    # (Do not change this code)\n",
    "    if epoch_cnt % 10 == 0:\n",
    "        params.append([w.data.copy() for w in [model.w, model.b, model.v]])\n",
    "\n",
    "    for batch_cnt in range(0, len(train_data) // batch_size):\n",
    "        \n",
    "        ########### Preparing our batch ##################\n",
    "        # Take a size-`batch_size` (i.e. M) slice from the randomized indices \n",
    "        # that are stored in `idxs`. \n",
    "        # Each iteration of this for-loop should create a subsequent, non-overlapping slice\n",
    "        # from `idxs` so that each datum will be included in a batch for a given epoch.\n",
    "        batch_indices = #\n",
    "        \n",
    "        # `batch_indices` is now a shape-(M,) array of randomly shuffled indices.\n",
    "        # We will use these to pick M pieces of data from our dataset to form a random \n",
    "        # \"batch\" of data.\n",
    "        batch = # this is a shape-(M, 1) tensor\n",
    "\n",
    "        ########### Assessing our predictions ##############\n",
    "        # Pass the shape-(M, 1) batch data to your model to produce a corresponding\n",
    "        # shape-(M, 1) tensor of predictions \n",
    "        #\n",
    "        predictions = #  A shape-(M, 1) tensor of model predictions\n",
    "\n",
    "        # Use `true_f` to compute the true (a.k.a desired) values for this batch of data\n",
    "        truth = #\n",
    "\n",
    "        # Compute the loss associated with our predictions, compared to the true values\n",
    "        loss = # \n",
    "\n",
    "        # Use mygrad's automatic differentiation capabilities to calculate the derivatives needed for \n",
    "        # updating our model's parameters: dL/dw, dL/db, and dL/db\n",
    "        # STUDENT CODE HERE\n",
    "\n",
    "        # Use your gradient-step function, passing all of your model's\n",
    "        # parameters (w, b, v), and the learning rate. This will update your\n",
    "        # model's parameters to minimize the loss\n",
    "        #\n",
    "        # Remember that your gradient-step function updates the parameter-values in-place.\n",
    "        # It doesn't return anything\n",
    "        # STUDENT CODE HERE\n",
    "\n",
    "        # This will record the current loss, and will plot it\n",
    "        plotter.set_train_batch({\"loss\": loss.item()}, batch_size=batch_size)  # (Don't change this)\n",
    "    plotter.set_train_epoch()  # (Don't change this)\n",
    "\n",
    "# this will ensure you plotted the most recent data\n",
    "plotter.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853857dd",
   "metadata": {},
   "source": [
    "We should now expect to see that $F_{\\mathrm{model}}(x) \\approx \\cos ( x )$ for $x \\in [-2\\pi, 2\\pi]$.\n",
    "\n",
    "To evaluate the quality of your model (i.e. your approximating function $F_{\\mathrm{model}}(x)$), plot $f_{\\mathrm{true}}(x)$ (the desired function) and $F_{\\mathrm{model}}(x)$ on the sample plot. Use `train_data` as your `x` values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49a6bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a8e97e",
   "metadata": {},
   "source": [
    "Let's see what our model looked like *as it was training/learning*. Run the following cell to see the true function (plotted in blue) and our approximating function (plotted in orange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb64a741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "# Don't change anything in this cell\n",
    "\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "x = np.linspace(-4 * np.pi, 4 * np.pi, 1000)\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, np.cos(x))\n",
    "ax.set_ylim(-2, 2)\n",
    "_model = Model(model.N)\n",
    "_model.load_parameters(*params[0])\n",
    "(im,) = ax.plot(x.squeeze(), _model(x[:, np.newaxis]).squeeze())\n",
    "\n",
    "\n",
    "def update(frame):\n",
    "    # ax.figure.canvas.draw()\n",
    "    _model.load_parameters(*params[frame])\n",
    "    im.set_data(x.squeeze(), _model(x[:, np.newaxis]).squeeze())\n",
    "    return (im,)\n",
    "\n",
    "\n",
    "ani = FuncAnimation(\n",
    "    fig,\n",
    "    update,\n",
    "    frames=range(0, len(params)),\n",
    "    interval=20,\n",
    "    blit=True,\n",
    "    repeat=True,\n",
    "    repeat_delay=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897bb049",
   "metadata": {},
   "source": [
    "Let's visualize the form learned by each of our \"neurons\", scaled by . That is, we will plot\n",
    "\n",
    "\\begin{equation}\n",
    "\\varphi(x w_{i} + b_{i})\n",
    "\\end{equation}\n",
    "\n",
    "for each $i=1, ..., N$ on $x \\in [-2\\pi, 2\\pi]$. Note that $v_i$ is *not* included here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e860cb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, nrows=model.N // 2)\n",
    "\n",
    "x = # create a shape-(50,) array evenly spaces on [-2pi, 2pi]\n",
    "\n",
    "# `axes`, `model.w.data`, and `model.b.data` are all 2D numpy arrays, and each\n",
    "# store (N,) elements\n",
    "#\n",
    "# Use the .ravel() method on each of these to transform them into flat arrays/tensors\n",
    "# of shape-(N,)\n",
    "\n",
    "# For each i in [0, 1, ..., N-1] plot sigmoid(x * w_i + b_i) in the ith `axes` object\n",
    "\n",
    "\n",
    "flat_axes = # use .ravel() to make shape-(N,)\n",
    "flat_w = # use .ravel() to make shape-(N,)\n",
    "flat_b = # use .ravel() to make shape-(N,)\n",
    "\n",
    "for i in range(model.N):\n",
    "    ax = # get axis-i\n",
    "    w = # get w-i\n",
    "    b = # get b-i\n",
    "    \n",
    "    # x is a shape-(50,) array of values evenly-spaced between [-2pi, 2pi]\n",
    "    # w is a single number\n",
    "    # b is a single number\n",
    "    \n",
    "    sig_out = # compute the output of a single neuron\n",
    "    ax.plot(x, sig_out)\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "for ax in axes.ravel():\n",
    "    ax.grid(\"True\")\n",
    "    ax.set_ylim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e01c1f4",
   "metadata": {},
   "source": [
    "Finally, let's include the scaling factors, $\\{v_i\\}_{i=1}^{N}$, each of which multiplies a respective neuron. That is, plot\n",
    "\n",
    "\\begin{equation}\n",
    "v_{i}\\varphi(x w_{i} + b_{i})\n",
    "\\end{equation}\n",
    "\n",
    "for each $i$ on $x \\in [-2\\pi, 2\\pi]$. \n",
    "\n",
    "**What will the result look like if you plot the sum of all of these curves? (Hint: look back to the form of the universal function approximation theorem**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4848b0c1",
   "metadata": {},
   "source": [
    "Your response here:\n",
    "\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45609b74",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x = np.linspace(-2 * np.pi, 2 * np.pi)\n",
    "\n",
    "# plots the full model output as a thick dashed black curve\n",
    "ax.plot(\n",
    "    x,\n",
    "    model(x[:, np.newaxis]),\n",
    "    color=\"black\",\n",
    "    ls=\"--\",\n",
    "    lw=4,\n",
    "    label=\"full model output\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Add to the plot the scaled activation for each neuron: v (x * w + b)\n",
    "# Plot each of these using the same instance of `Axes`: `ax`.\n",
    "\n",
    "# Use the same code as in the previous cell, but include v_i in your calculation\n",
    "# of the neuron's activation\n",
    "\n",
    "# STUDENT CODE HERE\n",
    "\n",
    "ax.grid(\"True\")\n",
    "ax.legend()\n",
    "ax.set_title(\"Visualizing the 'activity' of each of the model's scaled neurons\")\n",
    "ax.set_xlabel(r\"$x$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524fbb54",
   "metadata": {},
   "source": [
    "### Things to Try (be sure to preserve any good code you have before trying these things)\n",
    "- Once you have a good model, try plotting $f(x)$ and $F(x)$ *beyond* the domain that you trained it on. For example, try plotting them on $[-4\\pi, 4\\pi]$. What do you see? Is this reasonable? Discuss with a neighbor.\n",
    "  - Dig in a little deeper, plot each of the model's scaled neurons $v_{i}\\varphi(\\vec{x} \\cdot \\vec{w}_{i} + b_{i})$ on $[-4\\pi, 4\\pi]$. You should be able to visually see how the sigmoidal curves will sum outside of $[-2\\pi, 2\\pi]$\n",
    "- Once your loss curve (often called the \"learning curve\") plateaus, try reducing the learning rate by a factor of 5 or 10 and resume training. This will likely lower your loss further.\n",
    "- Experiment with batch-size. What happens if you use a batch-size of 200 to train your model instead of 25?\n",
    "- Try decreasing the the parameter-number in your model from $N=10$ down to $N=1$. Thus `w` will have the shape (1, 1) instead of (1, 10), etc. Train this model as best you can, and plot $F(x)$. What shape does this take? Can you explain why?\n",
    "- Using $N=10$, repeat your training but train on the domain $[2\\pi, 6\\pi]$. Are you able to get your model to train well? Why should shifting the domain have any affect if $f(x)$ is perfectly periodic. Consider what special properties our original domain, $[-2\\pi, 2\\pi]$ has. Consider, also, how we initialize our model's parameters. Discuss with your neighbor what you suspect might be the issue here. You can use `noggin` to plot the mean values of `w`, `v`, and `b` as you train. You can also plot the mean values of the gradients that are back-propagating through your model, with some minor modifications to your code. This is very interesting to visualize.\n",
    "- Fit a different $f(x)$ other than cosine. Do you need more parameters to approximate more complicated functions?\n",
    "- Try increasing $N$ to $N=100$. You may need to try adjusting your learning rate during training, lowering it as you go. Does increasing $N$ make things better or worse in this instance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64f1e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting F(x) outside of its training domain\n",
    "\n",
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adcebb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot each of the scaled neurons, v*sigmoid(w*x + b), on [-4 pi, 4 pi]\n",
    "\n",
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806eed53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model using N=1\n",
    "\n",
    "# INTERPRETATION: For N=1, F(x) = v * sigmoid(x*w + b), thus F(x) must\n",
    "# have the form of a sigmoid function (albeit a shallow one)\n",
    "\n",
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93414128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fafee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing w_grad.mean()\n",
    "\n",
    "# STUDENT CODE HERE"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python",
   "notebook_metadata_filter": "nbsphinx,-kernelspec"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "nbsphinx": {
   "execute": "never"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
