{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ba09bc6",
   "metadata": {},
   "source": [
    "# Training a Two-Layer Neural Network on Cifar-10\n",
    "\n",
    "The tendril classification problem had us construct a single layer neural network to tackle a simple classification problem on shape-$(2,)$ input data. In this notebook, we will work with an dataset of images. We will be using the famed [cifar-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html), so that our  model can classify pictures of cars, planes, cats, dogs, frogs, and other items. There are 10 classes in total represented in this dataset.\n",
    "Each image is an a shape-``(3, 32, 32)`` array, corresponding to 32 x 32 RGB values. $3 \\times 32 \\times 32 = 3072$ thus each image is a vector of length $3072$.\n",
    "\n",
    "We will be training a two-layer neural network. Our loss function is the cross-entropy loss. The first two layers will use the ReLU activation function and the last layer will use softmax activation. \n",
    "\n",
    "\n",
    "#### The Model in Full\n",
    "\n",
    "\\begin{equation}\n",
    "D_1(x) = \\operatorname{ReLU}(xW_{1} + b_{1})\\\\\n",
    "D_2(x) = \\operatorname{ReLU}(D_1(x) W_{2} + b_{2})\\\\\n",
    "F(\\{W\\}, \\{b\\}; x) = \\operatorname{softmax}(D_2(x) W_3)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "We will again be using the popular cross-entropy classification loss. Keep in mind that `mygrad`, and other auto-differentiation libraries, provide a convenient softmax_crossentropy function, which efficiently computes the softmax *and then* the cross-entropy.\n",
    "So take care to not apply the softmax function in your model's forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69de072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mygrad as mg\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee9300b",
   "metadata": {},
   "source": [
    "Running the following cell will download and load the CIFAR10 dataset.\n",
    "As you will see, it consists of a training dataset of 50,000 images and a test set of 10,000 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b829e38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cog_datasets\n",
    "\n",
    "cog_datasets.download_cifar10()\n",
    "\n",
    "x_train, y_train, x_test, y_test = cog_datasets.load_cifar10()\n",
    "\n",
    "print('Training data shape: ', x_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', x_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print(x_train.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdb6929",
   "metadata": {},
   "source": [
    "Let's investigate what our data looks like at a glance. The following cell will plot some examples from each of the 10 classes in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87af2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(y_train == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(x_train[idx].transpose(1,2,0).astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d901034f",
   "metadata": {},
   "source": [
    "We will need to do three things to the images in the train and test sets:\n",
    "- Change the datatypes of the arrays to be 32-bit floating point numbers instead of 8-bit unsigned integers. Recall that `arr = arr.astype(\"float32\")` will update the array's data type.\n",
    "- Reshape each set of images from a shape-(N, 3, 32, 32) to a  shape-(N, 3072) array. Recall that `arr = arr.reshape(len(arr), -1)` will \"flatten\" each image in the stack of `N` images to a vector of the appropriate length (`-1` tells numpy to compute this value for you).\n",
    "- Normalize each image so that each image has a mean pixel value of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb16d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the train and test data to be shape (N, 3072)  (N=50,000 for train, N=10,000 for test)\n",
    "\n",
    "# For both sets of images – x_train and x_test – update the data type of the array\n",
    "# to by float-32. Also reshape the array from (N, 3, 32, 32) to (N, 3072)\n",
    "x_train = #\n",
    "x_test = #\n",
    "\n",
    "# Using only the training data, compute the mean-image (a shape-(3072,) array)\n",
    "# and the std-dev image (also shape-(3072,) array)\n",
    "#\n",
    "# That is, we are computing the mean 1st pixel over all images, the mean 2nd pixel,\n",
    "# and so on, creating a shape-(3072,) array of these mean pixel values.\n",
    "#\n",
    "# Given that x_train is a shape-(N, 3072) array – corresponding to N pictures each with 3,072 pixels – \n",
    "# how can we comput the pixel-wise mean and std? \n",
    "# Hint: x_train.mean(axis=???) and x_train.std(axis=???)\n",
    "mean_image = #\n",
    "std_image = #\n",
    "\n",
    "\n",
    "# Perform a pixel-wise normalization of the training data: x_train = (x_train - mean_image) / std_image\n",
    "# STUDENT CODE HERE\n",
    "\n",
    "# Using the *same* mean and std statistics, do a pixel-wise normalization of the testing\n",
    "# data: x_test = (x_test - mean_image) / std_image\n",
    "#\n",
    "# It is important that we always use the identical normalization/augmentation process on\n",
    "# our test data as we used on our training data\n",
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826ccc72",
   "metadata": {},
   "source": [
    "Now, let's construct our model using `MyNN` and define our [accuracy function](https://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/Problems/ComputeAccuracy.html).\n",
    "\n",
    "We can experiment with the sizes of our layers, but try:\n",
    " \n",
    "- layer-1: size-100\n",
    "- layer-2: size-50\n",
    "- layer-3: size-? (hint: we don't get to pick this)\n",
    "  - This final layer should not have any bias: `dense(..., bias=False)`\n",
    "  - We will be using `softmax_crossentropy` for our loss, we don't need an activation function for this last layer.\n",
    "\n",
    "Use the `he_normal` initialization for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360ca989",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mygrad.nnet.initializers.he_normal import he_normal\n",
    "from mygrad.nnet.activations.relu import relu\n",
    "from mygrad.nnet.losses import softmax_crossentropy\n",
    "\n",
    "from mynn.optimizers.sgd import SGD\n",
    "from mynn.layers.dense import dense\n",
    "\n",
    "\n",
    "# Define your MyNN-`Model` class here. It should have:\n",
    "# - an `__init__` method that initializes all of your layers\n",
    "# - a `__call__` method that defines the model's \"forward pass\"\n",
    "# - a `parameters` property that returns a tuple of all of your\n",
    "#   model's learnable parameters (refer to the Tendrils-MyNN)\n",
    "#   notebook for the syntax of defining a class-property)\n",
    "class Model:\n",
    "    def __init__(self, n1, n2, num_pixels, num_classes):\n",
    "        \"\"\"\n",
    "        Initializes a model with two hidden layers of size `n1` and `n2`\n",
    "        respectively.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n1 : int\n",
    "            The number of neurons in the first hidden layer\n",
    "\n",
    "        n2 : int\n",
    "            The number of neurons in the second hidden layer\n",
    "        \n",
    "        num_pixels : int\n",
    "            The number of pixels in a *single* image\n",
    "\n",
    "        num_classes : int\n",
    "            The number of classes predicted by the model\"\"\"\n",
    "        \n",
    "        # Use the `dense` class from mynn to create three dense layers.\n",
    "        #\n",
    "        # Each layer should use the he-normal initialization scheme (same as the last notebook)\n",
    "        \n",
    "        # The shape of dense1, which is associated with W_1, should be shape-(??, ??)\n",
    "        # \n",
    "        # Keep in mind that we will be doing the matrix multiplication: X W_1\n",
    "        # If:\n",
    "        #   - X has shape-(M, D) where M is batch-size and D is num_pixels per image,\n",
    "        #   - Our first dense layer should produce a total of n1 outputs\n",
    "        # then what should the shape of this dense layer be?\n",
    "        self.dense1 = #\n",
    "        \n",
    "        # Create a shape-(n1, n2) dense layer: it takes in n1 inputs from the preceding layer and\n",
    "        # produces n2 outputs -- one for each of its neurons\n",
    "        self.dense2 = #\n",
    "        \n",
    "        # For our output layer, create a shape-(n2, ???) dense layer\n",
    "        # Given this particular classification problem, what must the output size of this\n",
    "        # layer be?\n",
    "        self.dense3 = # make sure to set `bias=False` for this one\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\" Performs a \"forward-pass\" of data through the network.\n",
    "        \n",
    "        This allows us to conveniently initialize a model `m` and then send data through it\n",
    "        to be classified by calling `m(x)`.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : Union[numpy.ndarray, mygrad.Tensor], shape=(M, 3072)\n",
    "            A batch of data consisting of M pieces of data,\n",
    "            each with a dimentionality of 3072 (the number of\n",
    "            values among all the pixels in a given image).\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        mygrad.Tensor, shape-(M, num_class)\n",
    "            The model's prediction for each of the M images in the batch,\n",
    "        \"\"\"\n",
    "        # Use the model's three dense layers and the relu activation function\n",
    "        # to process the input:\n",
    "        # x -> dense1 -> relu -> dense2 -> relu -> dense3 -> out\n",
    "        # \n",
    "        # Note that the output of dense3 does not pass through a relu!\n",
    "        # STUDENT CODE HERE\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        \"\"\" A convenience function for getting all the parameters of our model.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        List[mygrad.Tensor]\n",
    "            A list of all of the model's trainable parameters \n",
    "        \"\"\"\n",
    "        # return a list of parameters from each of your model's three layers\n",
    "        # STUDENT CODE HERE\n",
    "\n",
    "\n",
    "# Define your classification-accuracy function\n",
    "def accuracy(predictions, truth):\n",
    "    \"\"\"\n",
    "    Returns the mean classification accuracy for a batch of predictions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    predictions : Union[numpy.ndarray, mg.Tensor], shape=(M, D)\n",
    "        The scores for D classes, for a batch of M data points\n",
    "\n",
    "    truth : numpy.ndarray, shape=(M,)\n",
    "        The true labels for each datum in the batch: each label is an\n",
    "        integer in [0, D)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "    \"\"\"\n",
    "    # use the solution from your previous notebooks\n",
    "    # STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fe0e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a noggin plot, that keeps track of the metrics: \"loss\" and \"accuracy\"\n",
    "from noggin import create_plot\n",
    "\n",
    "plotter, fig, ax = create_plot(metrics=[\"loss\", \"accuracy\"], last_n_batches=int(5e3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9499ab64",
   "metadata": {},
   "source": [
    "Initialize your model and optimizer, using SGD from MyNN. Specify the parameters, learning rate and weight_decay for your \n",
    "optimizer.\n",
    "\n",
    "A learning rate of $0.1$ and a weight decay of $5\\times10^{-4}$ is sensible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132f862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = # create a model with 100 neurons in layer 1 and 50 neurons in layer 2. What is the number of classes?\n",
    "\n",
    "# Be sure to pass your model's parameters to the optimizer \n",
    "optim = # use SGD with a lr of 0.1 and weight_decay of 5e-4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5174895",
   "metadata": {},
   "source": [
    "Now write code to train your model! Experiment with your learning rate and weight_decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f2b9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of predictions that we will make in each training step\n",
    "batch_size = # Set to 100\n",
    "\n",
    "\n",
    "# We will train for 10 epochs; you can change this if you'd like.\n",
    "# You will likely want to train for much longer than this\n",
    "for epoch_cnt in range(10):\n",
    "    \n",
    "    # Create the indices to index into each image of your training data\n",
    "    # e.g. `array([0, 1, ..., 9999])`, and then shuffle those indices.\n",
    "    # We will use this to draw random batches of data\n",
    "    idxs = np.arange(len(x_train))  # -> array([0, 1, ..., 9999])\n",
    "    np.random.shuffle(idxs)  \n",
    "    \n",
    "    for batch_cnt in range(0, len(x_train) // batch_size):\n",
    "        # Index into `x_train` to get your batch of M images.\n",
    "        batch_indices = #\n",
    "        batch = # use `batch_indices` to get the random batch of our training data\n",
    "        \n",
    "        # compute the predictions for this batch using your model\n",
    "        prediction = #\n",
    "        \n",
    "\n",
    "        # use `batch_indices` to get the true values (i.e. labels) for this training batch \n",
    "        truth = #\n",
    "        \n",
    "\n",
    "        # compute the loss associated with our predictions vs the truth\n",
    "        # (use softmax_cross_entropy)\n",
    "        loss = #\n",
    "\n",
    "\n",
    "        # Use mygrad compute the derivatives for your model's parameters, so\n",
    "        # that we can perform gradient descent.\n",
    "        # STUDENT CODE HERE\n",
    "        \n",
    "\n",
    "        # perform a step of gradient descent using the optimizer\n",
    "        # STUDENT CODE HERE\n",
    "        \n",
    "        \n",
    "        # compute the accuracy between the prediction and the truth \n",
    "        acc = #\n",
    "        \n",
    "\n",
    "        plotter.set_train_batch({\"loss\" : loss.item(),\n",
    "                                 \"accuracy\" : acc},\n",
    "                                 batch_size=batch_size)\n",
    "    \n",
    "    # After each epoch we will evaluate how well our model is performing\n",
    "    # on data from cifar10 *that it has never \"seen\" before*. This is our\n",
    "    # \"test\" data. The measured accuracy of our model here is our best \n",
    "    # estimate for how our model will perform in the real world \n",
    "    # (on 32x32 RGB images of things in this class)\n",
    "    test_idxs = np.arange(len(x_test))  # no need to shuffle these!\n",
    "    \n",
    "    # Iterates over all `batch_size`-sized batches of our test data\n",
    "    for batch_cnt in range(0, len(x_test)//batch_size):\n",
    "        batch_indices = # get the next batch of `test_idxs`\n",
    "        \n",
    "        batch = # use `batch_indices` to get the batch of our **test data**\n",
    "        truth = # use `batch_indices` to get the batch of our **test labels**\n",
    "        \n",
    "        # We do not want to compute gradients here, so we use the\n",
    "        # no_autodiff context manager to disable the ability to\n",
    "        with mg.no_autodiff:\n",
    "            # Get your model's predictions for this test-batch\n",
    "            # and measure the test-accuracy for this test-batch\n",
    "            prediction = #\n",
    "            test_accuracy = #\n",
    "        \n",
    "        # pass your test-accuracy here; we used the name `test_accuracy`\n",
    "        plotter.set_test_batch({\"accuracy\" : test_accuracy}, batch_size=batch_size)\n",
    "    plotter.set_test_epoch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89876361",
   "metadata": {},
   "source": [
    "## Evaluating Your Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49239d41",
   "metadata": {},
   "source": [
    "How well is your model performing?\n",
    "According to the noggin plot, what is the training accuracy of your model? What is the testing accuracy? Is there a gap?\n",
    "\n",
    "Below, we provide code to randomly pick an image from the test set, plot it, and print your model's predicted label vs the true label. `cog_datasets.load_cifar10.labels` returns a tuple of the label-names in correspondence with each truth-index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f7f5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading `img_test` for plotting purposes\n",
    "_, _, img_test, label_test = cog_datasets.load_cifar10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba2b93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell multiple times to see your model's predictions\n",
    "# for various test images\n",
    "\n",
    "labels = cog_datasets.load_cifar10.labels  # tuple of cifar-10 labels\n",
    "\n",
    "index = np.random.randint(0, len(img_test))  # pick a random test-image index\n",
    "\n",
    "true_label_index = label_test[index]\n",
    "true_label = labels[true_label_index]\n",
    "\n",
    "with mg.no_autodiff:\n",
    "    prediction = model(x_test[index:index + 1])  # passing in a shape-(1, 3072) array \n",
    "    predicted_label_index = np.argmax(prediction.data, axis=1).item()  # largest score indicates the prediction\n",
    "    predicted_label = labels[predicted_label_index]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# matplotlib wants shape-(H, W, C) images, with unsigned 8bit pixel values\n",
    "img = img_test[index].transpose(1, 2, 0).astype('uint8')\n",
    "\n",
    "ax.imshow(img)\n",
    "ax.set_title(f\"Predicted: {predicted_label}\\nTruth: {true_label}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d786d9e",
   "metadata": {},
   "source": [
    "Can you understand some of the mistakes that your model is making? Perhaps it sees a white airplane flying over water, and confuses it for a boat. Can *you* figure out what some of these images depict? Some are pretty hard to identify, given the low resolution. \n",
    "\n",
    "To get a more comprehensive understanding of where our model is getting things right and what mistakes it is making, let's plot a **confusion matrix**.\n",
    "We will have our model make predictions for all of the images in our test set, and we will compare these predictions to the images' true labels.\n",
    "The confusion matrix will display true labels on its vertical axis and the predicted labels (sorted in the same order) on the horizontal axis. Thus, if you want to see how often our model mistakes a car for a truck, you would find the \"car\" label on the vertical axis, and the \"truck\" label on the horizontal axis.\n",
    "The square where those labels meet reports the number of car-pictures that our model thought was a truck.\n",
    "Therefore a *perfect* model, given perfectly labeled data, would produce a confusion matrix where all of the off-diagonal elements are $0$ and only the diagonal squares have any \"weight\" to them.\n",
    "\n",
    "\n",
    "Run the following cell to compute the confusion matrix.\n",
    "Our test set consists of 10,000 images, so it is relatively easy to figure out the proportion represented by each square.\n",
    "Answer the following questions:\n",
    "- Which classes is the model most reliable at identifying? What is the best and worst accuracies for a single class?\n",
    "- Are the model's mistakes seemingly random, or does it tend to confuse classes that resemble each other.\n",
    "- One thing to consider: the model doesn't explicitly know the difference between foreground and background of an image; does the model easily mistake classes of objects/animals that can appear against similar backgrounds? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95b9e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to plot the confusion matrix\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "idxs = np.arange(len(x_test))  # -> array([0, 1, ..., 9999])\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for batch_cnt in range(0, len(x_test) // batch_size):\n",
    "\n",
    "    batch_indices = idxs[batch_cnt*batch_size : (batch_cnt + 1)*batch_size]\n",
    "    batch = x_test[batch_indices]  # random batch of our training data\n",
    "\n",
    "\n",
    "    with mg.no_autodiff:\n",
    "        predictions.append(model(batch))  # you must pass in a shape-(1, 3072) array\n",
    "\n",
    "predictions = np.argmax(np.concatenate(predictions), -1)  # shape-(N,) array of predicted labels\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(predictions, y_test, display_labels=classes);"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python",
   "notebook_metadata_filter": "nbsphinx,-kernelspec"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "nbsphinx": {
   "execute": "never"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
