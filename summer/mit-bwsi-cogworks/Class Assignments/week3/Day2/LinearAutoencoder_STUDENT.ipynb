{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bedbe3b5",
   "metadata": {},
   "source": [
    "# Introduction to Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9483f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mygrad as mg\n",
    "import mynn\n",
    "import numpy as np\n",
    "\n",
    "from mygrad.nnet.initializers import he_normal\n",
    "from mynn.layers.dense import dense\n",
    "from mynn.losses.mean_squared_loss import mean_squared_loss\n",
    "from mynn.optimizers.sgd import SGD\n",
    "from mynn.optimizers.adam import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775732bf",
   "metadata": {},
   "source": [
    "## 1 Defining a Linear Autoencoder Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a4a7bd",
   "metadata": {},
   "source": [
    "### 1.1 What Exactly Is An Autoencoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47724bd",
   "metadata": {},
   "source": [
    "Autoencoders are a family of neural networks that can be leveraged to learn dense, abstract representations of data. These representations can often be used to make salient important, high-level features about our data. As an example of an encoder network, Facenets's facial-recognition model could take a picture of a face and **encode it** into a 512-dimensional vector that allowed us to tell the difference between individuals based on their personal appearances. \n",
    "\n",
    "We can think of an autoencoder as two separate neural nets: an encoder and a decoder. We pass in data to our encoder, which generates a useful, abstract representation for the data. This abstract representation of the data is then passed into the decoder, which will **try to recover the original data**. We train the encoder and decoder simultaneously, with the autoencoder learning to how to compress and uncompress the data in the most lossless way possible.\n",
    "\n",
    "Autoencoders are a sort of hybrid supervised-unsupervised network. There *is* a truth value, but it is simply the original data (i.e., `ytrain = xtrain`). When the network is trained with `mean_squared_error` loss between the network's outputs and inputs, gradient descent will try to find parameters that result in good encodings (representations with reduced dimensionality) that allow the decoder to recover or reconstruct the original input.\n",
    "\n",
    "Our entire autoencoder can be used for de-noising data. The learning mapping to a reduced dimension can be used for visualization, clustering, compression, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967c1593",
   "metadata": {},
   "source": [
    "We'll be training something that is *structured* like neural network with two dense layers, but its \"layers\" will not have any activation functions nor biases.\n",
    "Thus this is not a neural network at all: recall that the \"neuron\" terminology comes from the use of non-linear activation functions! Our auto-encoder simply consists of two matrix multiplications:\n",
    "\n",
    "\\begin{equation}\n",
    "D_{\\mathrm{enc}}(X) = X W_{\\mathrm{enc}} \\\\\n",
    "D_{\\mathrm{dec}}(X) = X W_{\\mathrm{dec}} \\\\\n",
    "F(W_{\\mathrm{enc}}, W_{\\mathrm{dec}}; X) = D_{\\mathrm{dec}}(D_{\\mathrm{enc}}(X)) = X W_{\\mathrm{enc}} W_{\\mathrm{dec}}\n",
    "\\end{equation}\n",
    "\n",
    "Assume $X$ represents a batch of $N$ pieces of  input-data, and has a shape `(N, D_full)`. $W_{\\mathrm{enc}}$ is responsible has a shape `(D_full, D_hidden)`, $W_{\\mathrm{dec}}$ has shape `(D_hidden, D_full)`, and `D_hidden` < `D_full`. The first layer can be thought to \"encode\" each datum in $X$ into a smaller dimension of size `D_hidden`. The second layer then \"decodes\" the encoding by mapping it back to the original dimension of size `D_full`:\n",
    "\n",
    "```\n",
    "(N, D_full) x (D_full, D_hidden) x (D_hidden, D_full) -> (N, D_full)\n",
    "```\n",
    "\n",
    "Thus **our goal** is to find values for values for our encoder-weights and decoder weights such that:\n",
    "\n",
    "\\begin{equation}\n",
    "F(W_{\\mathrm{enc}}, W_{\\mathrm{dec}}; X) \\approx X\n",
    "\\end{equation}\n",
    "\n",
    "The reason we restrict ourselves to linear activations is that once when we apply an autoencoder to word embeddings, we will want to maintain linear relationships for visualizing word embedding relationships.\n",
    "\n",
    "Once we have completely trained our autoencoder, we can simply chop off the decoder and use the encoder to generate our desired dense representations.\n",
    "\n",
    "Complete the `LinearAutoencoder` MyNN class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eea0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearAutoencoder:\n",
    "    def __init__(self, D_full, D_hidden):\n",
    "        \"\"\" This initializes all of the layers in our model, and sets them\n",
    "        as attributes of the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        D_full : int\n",
    "            The size of the inputs.\n",
    "            \n",
    "        D_hidden : int\n",
    "            The size of the 'bottleneck' layer (i.e., the reduced dimensionality).\n",
    "        \"\"\"\n",
    "        # Initialize the encoder and decorder dense layers using the `he_normal` initialization\n",
    "        # schemes.\n",
    "        # What should the input and output dimensions of each layer be? \n",
    "        # STUDENT CODE HERE\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        '''Passes data as input to our model, performing a \"forward-pass\".\n",
    "        \n",
    "        This allows us to conveniently initialize a model `m` and then send data through it\n",
    "        to be classified by calling `m(x)`.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : Union[numpy.ndarray, mygrad.Tensor], shape=(M, D_full)\n",
    "            A batch of data consisting of M pieces of data,\n",
    "            each with a dimentionality of D_full.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        mygrad.Tensor, shape=(M, D_full)\n",
    "            The model's prediction for each of the M pieces of data.\n",
    "        '''\n",
    "        # keep in mind that this is a linear model - there is no \"activation function\"\n",
    "        # involved here\n",
    "        # STUDENT CODE HERE\n",
    "        \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        \"\"\" A convenience function for getting all the parameters of our model.\n",
    "        \n",
    "        This can be accessed as an attribute, via `model.parameters` \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[Tensor, ...]\n",
    "            A tuple containing all of the learnable parameters for our model \"\"\"\n",
    "        # STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf96a04",
   "metadata": {},
   "source": [
    "## 2 Our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d6375f",
   "metadata": {},
   "source": [
    "We will use the Iris dataset, in which each datum is a 4-dimensional vector, and train an Autoencoder that will learn to compress the data into a 2-dimensional space. By reducing the dimensionality of the data, we will be able to visually identify clusters within the dataset.\n",
    "\n",
    "The Iris dataset consists of 150 4-dimensional feature vectors describing three classes of Iris flowers. Each sample is a row vector \n",
    "\\begin{equation}\n",
    "\\vec{x}=\\begin{bmatrix}\\text{sepal length} & \\text{sepal width} & \\text{petal length} & \\text{petal width}\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "The Iris dataset contains data on three species of Iris. Each species has distinct features, and so should form clusters among the species.\n",
    "\n",
    "Similar to our work in Week 2, zero center the data and scale by dividing by the standard deviation. Also make sure to check the shape of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a5feeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cogworks_data.language import get_data_path\n",
    "\n",
    "iris = np.load(get_data_path(\"iris_data.npy\"))\n",
    "\n",
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710ff832",
   "metadata": {},
   "source": [
    "Let's up a noggin plotter to display the loss metric. Use `max_fraction_spent_plotting=.75`. Note there's no additional accuracy metric since this is a regression problem instead of a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777b535a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from noggin import create_plot\n",
    "plotter, fig, ax = create_plot(metrics=[\"loss\"], max_fraction_spent_plotting=.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3d19c5",
   "metadata": {},
   "source": [
    "Create a LinearAutoencoder with `D_hidden=2` and train it using MyNN's `mean_squared_loss` as the loass function.\n",
    "Consider: what is our \"truth\" here?\n",
    "Recall that we are training our encoder-decoder network to be able *reproduce* the original data after encoding and then decoding each datum \n",
    "\n",
    "You probably will only need 500 or less epochs.\n",
    "Try using a batch-size of 25.\n",
    "Use the `SGD` optimizer with the default parameters (e.g. `learning_rate=0.1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b10d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed1e6af",
   "metadata": {},
   "source": [
    "To obtain a reduced-dimensional embedding of a datum **only apply the encoder** to that datum. \n",
    "That is, do not perform a full forward pass of your model - instead, only apply the first dense layer to the dataset.\n",
    "Save the output to the variable `reduced`, which should be a shape-(150, 2) tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3887d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mg.no_autodiff:\n",
    "    # STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a358dbf0",
   "metadata": {},
   "source": [
    "Now use the below code to plot your reduced-dimensionality dataset. The plot will color the three different species of iris included in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80c2046",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['iris_satosa', 'iris_versicolor', 'iris_virginica']\n",
    "colors = ['red', 'green', 'blue']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(3):\n",
    "    x = reduced[i*50:(i+1)*50, 0]\n",
    "    y = reduced[i*50:(i+1)*50, 1]\n",
    "    ax.scatter(x, y, c=colors[i], label=names[i])\n",
    "ax.grid()\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea20bfe2",
   "metadata": {},
   "source": [
    "Let's take a moment to think about what our autoencoder is doing here. Just as when we constructed a one-layer dense network for the tendril dataset, which found three linear separators for the three tendrils, our autoencoder is finding a linear separator of the data. Because our hidden layer is 2-dimensional, the linear separator is a 2-dimensional plane in the original 4-dimensional space. The autoencoder then projects the original 4-dimensional data down into 2 dimensions. This projection is what is plotted above.\n",
    "\n",
    "What information are we now able to deduce from our now reduced data? Taking a step away from NLP, what uses might this dimensionality reduction have in data analysis? Discuss with a partner."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python",
   "notebook_metadata_filter": "nbsphinx,-kernelspec"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "nbsphinx": {
   "execute": "never"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
