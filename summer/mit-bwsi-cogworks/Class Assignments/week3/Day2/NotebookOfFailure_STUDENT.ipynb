{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d598364",
   "metadata": {},
   "source": [
    "# Notebook of Failure \n",
    "\n",
    "(aka \"Even Odd Problem with Feed Forward Networks\")\n",
    "\n",
    "In this notebook, we'll attempt to solve the following problem: Given an input vector of zeros and ones, predict `1` if the number of ones in the vector is even, and predict `0` if the number of ones in the vector is odd.\n",
    "\n",
    "Sounds easy enough, right? :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57dc102",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c848409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mygrad as mg\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815daa37",
   "metadata": {},
   "source": [
    "## Create even/odd dataset\n",
    "\n",
    "Create a function that returns numpy arrays `x` and `y`, where:\n",
    "* `x` is an NumPy array of shape $(N, T)$ where each element is 0 or 1 with equal probability\n",
    "* `y` is an NumPy array of shape $(N,)$ where $y_i$ is 1 if number of 1s in row $i$ is even and 0 otherwise\n",
    "\n",
    "$N$ is the size of our batch and $T$ is the length of each vector of zeros and ones.\n",
    "\n",
    "For example, `generate_dataset(4, 8)` produces four vectors, each containing a length-8 sequence of zeros and ones. For `x`, it might produce:\n",
    "```python\n",
    "[[1. 0. 1. 1. 0. 1. 1. 1.]\n",
    " [0. 1. 1. 1. 0. 1. 0. 1.]\n",
    " [0. 1. 0. 1. 0. 1. 0. 0.]\n",
    " [0. 1. 1. 0. 0. 1. 1. 1.]]\n",
    "```\n",
    "\n",
    "Then the corresponding truth, `y`, would be:\n",
    "\n",
    "```python\n",
    "[1 0 0 0]\n",
    "```\n",
    "Note that `y` needs to have dtype `np.int` to be used with MyGrad/MyNN cross entropy loss.\n",
    "\n",
    "Also note that it's possible for rows to appear more than once, but this gets less and less probable as the the number of columns increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5511643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(N, T):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    N : int\n",
    "        The number of even/odd sequences to generate\n",
    "        \n",
    "    T : int\n",
    "        The length of each even/odd sequence\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[numpy.ndarray, numpy.ndarray], shapes=(N, T) & (T,)\n",
    "        A tuple containing:\n",
    "            - the batch of even/odd sequences; shape-(N, T)\n",
    "            - the integer label for each sequence: 1 if even, 0 if odd; shape-(N,)\n",
    "        \"\"\"\n",
    "    # STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3113dad",
   "metadata": {},
   "source": [
    "Test your `generate_dataset`. Generate four sequences, each sequence being length-8. Manually tally each sequence and verify that each label is correct for the even/oddness of each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caa65c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5102a94b",
   "metadata": {},
   "source": [
    "Now generate a dataset with 10000 rows and 32 columns, and split the data/labels evenly into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7c3843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cbc7d7",
   "metadata": {},
   "source": [
    "Print out the shapes of your train/test sequence-data/labels to verify that they match with your expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97ac3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6024635",
   "metadata": {},
   "source": [
    "## Define MyNN model\n",
    "\n",
    "Initially try using a two-layer neural network (one hidden layer of ReLU units):\n",
    "\n",
    "\\begin{equation}\n",
    "f(W_{1}, W_{2}, b_{1}, b_{2};\\;X) = \\mathrm{softmax}(\\mathrm{ReLU}(XW_{1} + b_{1})W_{2} + b_{2})\n",
    "\\end{equation}\n",
    "\n",
    "with cross entropy loss.\n",
    "\n",
    "For convenience, use MyGrad's `softmax_crossentropy` loss. This means that the MyNN model doesn't need to apply the softmax activation in its forward pass (because `softmax_crossentropy` will do it for us), i.e.,\n",
    "\n",
    "\\begin{equation}\n",
    "f(W_{1}, W_{2}, b_{1}, b_{2};\\;X) = \\mathrm{ReLU}(XW_{1} + b_{1})W_{2} + b_{2}\n",
    "\\end{equation}\n",
    "\n",
    "Ultimately, we will have our neural network produce **two** classification scores: $p_{odd}$ and $p_{even}$\n",
    "\n",
    "Use `from mygrad.nnet.initializers import normal`, and specify `normal` as the weight initializer for your dense layers. A layer size of 100 for your first layer, $W_{1}$, is a reasonable start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9d30d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mynn.layers.dense import dense\n",
    "from mynn.optimizers.sgd import SGD\n",
    "\n",
    "from mygrad.nnet.activations import relu\n",
    "from mygrad.nnet.initializers import normal\n",
    "from mygrad.nnet.losses import softmax_crossentropy\n",
    "\n",
    "# Define your MyNN-model\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, dim_in, num_hidden, dim_out):\n",
    "        # STUDENT CODE HERE\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\" The model's forward pass functionality.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : Union[numpy.ndarray, mygrad.Tensor], shape=(N, T)\n",
    "            The batch of size-N.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        mygrad.Tensor, shape=(N, 2)\n",
    "            The model's predictions for each of the N pieces of data in the batch.\n",
    "        \"\"\"\n",
    "        # STUDENT CODE HERE\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        \"\"\" A convenience function for getting all the parameters of our model. \"\"\"\n",
    "        # STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e55b33",
   "metadata": {},
   "source": [
    "Now initialize model and optimizer. Try using 100 units in hidden layer. For your optimizer, try the SGD with a `learning_rate` of 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e57461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c870d6ff",
   "metadata": {},
   "source": [
    "Now, create an accuracy function to compare your predictions to your labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3244a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, truth):\n",
    "    \"\"\"\n",
    "    Returns the mean classification accuracy for a batch of predictions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    predictions : Union[numpy.ndarray, mg.Tensor], shape=(N, 2)\n",
    "        The scores for 2 classes, for a batch of N data points\n",
    "        \n",
    "    truth : numpy.ndarray, shape=(N,)\n",
    "        The true labels for each datum in the batch: each label is an\n",
    "        integer in [0, 1]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "    \"\"\"\n",
    "    # STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2ef433",
   "metadata": {},
   "source": [
    "Now set up a noggin plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788e1ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from noggin import create_plot\n",
    "plotter, fig, ax = create_plot(metrics=[\"loss\", \"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd996021",
   "metadata": {},
   "source": [
    "Time to train your model!. You can try setting `batch_size = 100` and training for 700 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37071684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT CODE HERE\n",
    "\n",
    "for epoch_cnt in range(num_epochs):\n",
    "    idxs = np.arange(len(xtrain))\n",
    "    np.random.shuffle(idxs)  \n",
    "    \n",
    "    for batch_cnt in range(0, len(xtrain) // batch_size):\n",
    "        # random batch of our training data\n",
    "        # STUDENT CODE HERE\n",
    "\n",
    "        # perform the forward pass on our batch\n",
    "        # STUDENT CODE HERE\n",
    "        \n",
    "        # calculate the loss\n",
    "        # STUDENT CODE HERE\n",
    "        \n",
    "        # perform backpropagation\n",
    "        # STUDENT CODE HERE\n",
    "        \n",
    "        # update your parameters\n",
    "        # STUDENT CODE HERE\n",
    "        \n",
    "        # calculate the accuracy\n",
    "        # STUDENT CODE HERE\n",
    "        \n",
    "        plotter.set_train_batch({\"loss\" : loss.item(), \"accuracy\" : acc}, batch_size=batch_size)\n",
    "    \n",
    "    for batch_cnt in range(0, len(xtest) // batch_size):\n",
    "        idxs = np.arange(len(xtest))\n",
    "        batch_indices = idxs[batch_cnt * batch_size : (batch_cnt + 1) * batch_size]\n",
    "        batch = xtest[batch_indices]\n",
    "        truth = ytest[batch_indices]\n",
    "        \n",
    "        with mg.no_autodiff:\n",
    "            prediction = model(batch)\n",
    "            acc = accuracy(prediction, truth)\n",
    "        \n",
    "        plotter.set_test_batch({\"accuracy\" : acc}, batch_size=batch_size)\n",
    "    \n",
    "    plotter.set_train_epoch()\n",
    "    plotter.set_test_epoch()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc71aa66",
   "metadata": {},
   "source": [
    "Inspect the final train and test accuracy of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841b7e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a78cfd",
   "metadata": {},
   "source": [
    "Considering these accuracies, was the network successful or an utter failure?\n",
    "\n",
    "When using the suggested initial setup, it looks like this model is essentially memorizing the training set while learning absolutely nothing that's generalizable. Why do you think this is such a hard problem for this typical neural network architecture to learn? Would dropout on the input layer help? Could convolutions help? Discuss with your neighbors!\n",
    "\n",
    "Now try experimenting with training set size, number of columns in data, number of layers, layer sizes, activations functions, regularization (weight_decay), optimizer, etc. to try to improve performance on test set."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python",
   "notebook_metadata_filter": "nbsphinx,-kernelspec"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "nbsphinx": {
   "execute": "never"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
