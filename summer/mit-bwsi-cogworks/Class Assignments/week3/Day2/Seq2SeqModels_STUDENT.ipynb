{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "700f757f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from mynn.layers.dense import dense\n",
    "from mynn.optimizers.adam import Adam\n",
    "\n",
    "from mygrad.nnet.losses import softmax_crossentropy\n",
    "from mygrad.nnet.initializers import glorot_normal\n",
    "from mygrad.nnet.activations import relu\n",
    "\n",
    "import mygrad as mg\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ac28b5",
   "metadata": {},
   "source": [
    "# Seq2Seq Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309f3846",
   "metadata": {},
   "source": [
    "As we saw in the previous notebook, RNNs on their own are not well-suited to translation tasks, where word order may not necessarily align across languages.\n",
    "In this notebook we will be introduced to \"Sequence-to-Sequence\", or Seq2Seq, models.\n",
    "Seq2Seq models will leverage RNNs as a building block.\n",
    "\n",
    "We will want to utilize the `generate_batch` function and `RNN` class from the previous notebook to create and train our Seq2Seq model.\n",
    "Assuming you were able to complete the notebook, no modifications should need to be made to the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e6781b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(seq_len_min=1, seq_len_max=20, batch_size=10):\n",
    "    \"\"\"\n",
    "    Generates a batch of sequences and corresponding one-hot encodings.\n",
    "    \n",
    "    Each digit-sequence has a length of T-1 (not including the <END> token),\n",
    "    where the value for T-1 is randomly generated.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    seq_len_min : int, optional (default=1)\n",
    "       The smallest permissable length of the pattern,\n",
    "       excluding start and end tokens\n",
    "       \n",
    "    seq_len_max : int, optional (default=20)\n",
    "       The longest permissable length of the pattern,\n",
    "       excluding start and end tokens\n",
    "       \n",
    "    batch_size : int, optional (default=10)\n",
    "        The number of sequences to generate\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray]\n",
    "        1. the one-hot encoded sequences, including an end token; shape-(T, N, 12)\n",
    "        2. the target sequences, including an end token; shape-(T, N)\n",
    "        3. the original sequences of digits; shape-(T-1, N)\n",
    "    \"\"\"\n",
    "\n",
    "    # Randomly generate a sequence length in [seq_len_min, seq_len_max]\n",
    "    # This is the value T-1\n",
    "    T_1 = np.random.randint(seq_len_min, seq_len_max + 1)  #\n",
    "\n",
    "    # Randomly generate the shape-(T-1, N) sequence of digits [0-9]. This\n",
    "    # represents the N integer-valued sequences of length T-1 that our model\n",
    "    # will be translating\n",
    "    #\n",
    "    # E.g. If T-1 = 3 and N = 2, this might produce:\n",
    "    #\n",
    "    #  array([[3, 9],\n",
    "    #         [2, 5],\n",
    "    #         [9, 3]])\n",
    "    # seq-0: 3, 2, 9\n",
    "    # seq-1: 9, 5, 3\n",
    "    #\n",
    "    # Assign this to the variable `digits`\n",
    "    digits = np.random.randint(0, 10, (T_1, batch_size))  #\n",
    "\n",
    "    # Create an array of zeros to fill with one-hot encodings of sequences.\n",
    "    # This should have a shape of (T, N, 12) and a dtype of float-32.\n",
    "    #\n",
    "    # The sequence length is T because the source sequence\n",
    "    # needs to include the end token (but not start)\n",
    "    #\n",
    "    # Call this array `one_hot_x`\n",
    "    one_hot_x = np.zeros((T_1 + 1, batch_size, 12), dtype=np.float32)  #\n",
    "\n",
    "    # Use `digits` to populate `one_hot_x` with the appropriate one-hot encodings.\n",
    "    #\n",
    "    # You can achieve this either via advanced indexing:\n",
    "    # one_hot_x[np.arange(T_1).reshape(-1, 1), np.arange(batch_size), digits] = 1\n",
    "    # \n",
    "    # Or by using a for-loop:\n",
    "    # for ind in np.ndindex(digits.shape):\n",
    "    #     one_hot_x[ind + (digits[ind],)] = 1\n",
    "    one_hot_x[np.arange(T_1).reshape(-1, 1), np.arange(batch_size), digits] = 1\n",
    "\n",
    "    # In `one_hot_x`, at the last token position for all batches, set the <END> token's one-hot encoding\n",
    "    #\n",
    "    # Hint: Recall that `one_hot_x` has a shape of (T, N, 12).\n",
    "    # We want to access the T-th sequence entry and the 12th encoding position in this array\n",
    "    # for all N batches using basic indexing. And we want to set all elements in this selected\n",
    "    # subarray to 1.\n",
    "    one_hot_x[-1, :, -1] = 1  #\n",
    "\n",
    "    # Create the \"target\" sequences, which are simply the reversed input sequences.\n",
    "    # This should be a shape-(T, N) array, where the T-th row is filled with `11`, \n",
    "    # which is the <END> token.\n",
    "    ends = np.full(batch_size, 11).reshape(1, -1)\n",
    "    y = np.concatenate([digits[::-1], ends], axis=0)\n",
    "\n",
    "    # Return the appropriate arrays - in accordance with the docstring.\n",
    "    return one_hot_x, y, digits  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4353d440",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \"\"\"Implements a simple-cell RNN that produces both outputs and hidden descriptors.\"\"\"\n",
    "    def __init__(self, dim_input, dim_recurrent, dim_output):\n",
    "        \"\"\" Initializes all layers needed for RNN\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dim_input: int \n",
    "            Dimensionality of data passed to RNN (C)\n",
    "        \n",
    "        dim_recurrent: int\n",
    "            Dimensionality of hidden state in RNN (D)\n",
    "        \n",
    "        dim_output: int\n",
    "            Dimensionality of output of RNN (K)\n",
    "        \"\"\"\n",
    "        # <COGINST>\n",
    "        self.fc_x2h = dense(dim_input, dim_recurrent, weight_initializer=glorot_normal)\n",
    "        self.fc_h2h = dense(dim_recurrent, dim_recurrent, weight_initializer=glorot_normal, bias=False)\n",
    "        self.fc_h2y = dense(dim_recurrent, dim_output, weight_initializer=glorot_normal)\n",
    "        # </COGINST>\n",
    "    \n",
    "    \n",
    "    def __call__(self, x, h=None):\n",
    "        \"\"\" Performs the full forward pass for the RNN.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x: Union[numpy.ndarray, mygrad.Tensor], shape=(T, N, C)\n",
    "            The one-hot encodings for the each sequence in the batch\n",
    "        \n",
    "        h: Optional[Union[numpy.ndarray, mygrad.Tensor]], shape=(1, N, D)\n",
    "            An optional initial hidden dimension state h_0.\n",
    "            If None, initialize an array of zeros.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[y, h]\n",
    "            y: mygrad.Tensor, shape=(T, N, K)\n",
    "                The final classification scores for each RNN step\n",
    "            h: mygrad.Tensor, shape=(T, N, D)\n",
    "                The hidden states computed at each RNN step, excluding the initial state h_0\n",
    "        \"\"\"\n",
    "        # <COGINST>\n",
    "        # the only change needed to accomodate batches of data is adding `x.shape[1]`\n",
    "        # as dim-1 of the initial h_t if none is provided\n",
    "        h_t = np.zeros((1, x.shape[1], self.fc_h2h.weight.shape[0]), dtype=np.float32) if h is None else h\n",
    "        h = []\n",
    "        \n",
    "        for x_t in x:\n",
    "            h_t = relu(self.fc_x2h(x_t[np.newaxis]) + self.fc_h2h(h_t))\n",
    "            h.append(h_t)\n",
    "        \n",
    "        h = mg.concatenate(h, axis=0)\n",
    "        \n",
    "        return self.fc_h2y(h), h\n",
    "        # </COGINST>\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        \"\"\" A convenience function for getting all the parameters of our model.\n",
    "        \n",
    "        This can be accessed as an attribute, via `model.parameters` \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[Tensor, ...]\n",
    "            A tuple containing all of the learnable parameters for our model\n",
    "        \"\"\"\n",
    "        return self.fc_x2h.parameters + self.fc_h2h.parameters + self.fc_h2y.parameters # <COGLINE>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79af8330",
   "metadata": {},
   "source": [
    "Seq2Seq models follow an encoder-decoder structure, where one RNN encodes the original input sequence and a second RNN decodes the encoding to yield the final output.\n",
    "By chaining together two RNNs in this way, we are able to first encode information about the input sequence as a whole, then parse that full-sequence-encoding when making the final predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d14adb",
   "metadata": {},
   "source": [
    "The encoding stage of Seq2Seq is quite simple - just run the original sequence through an RNN.\n",
    "This will give us a set of predictions $(\\vec{z}_t)$ and a set of hidden descriptors $(\\vec{h}{}^e_t)$, as we are used to:\n",
    "\n",
    "\\begin{equation}\n",
    "\\big(\\vec{z}_t\\big)_{t=1}^T,\\; \\big(\\vec{h}{}_t^e\\big)_{t=1}^T- = \\operatorname{RNN}_{\\text{Encoder}}\\big((\\vec{x}_t)_{t=1}^T\\big)\n",
    "\\end{equation}\n",
    "\n",
    "The decoder is where our models starts to get more interesting, as we will use each prediction $\\vec{y}_t$ as the subsequent decoder input $\\vec{s}_{t+1}$.\n",
    "\n",
    "First and foremost, in order to pass information from the encoder to the decoder, we will use the final encoder hidden descriptor as the initial decoder hidden descriptor:\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{h}{}^d_0 := \\vec{h}{}^e_T\n",
    "\\end{equation}\n",
    "\n",
    "Additionally, we will have the initial input to the decoder be a single start token:\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{s}_1 = \\operatorname{One-Hot}\\,(\\langle\\text{Start}\\rangle)\n",
    "\\end{equation}\n",
    "\n",
    "We do this to give the decoder some kind of starting point for every sequence that we want to generate.\n",
    "We will run $\\vec{s}_1$ and $\\vec{h}{}^d_0$ through **a single step of the decoder**, which will then give us both prediction scores $\\vec{y}_1$ and a new hidden descriptor $\\vec{h}{}^d_1$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{y}_1,\\; \\vec{h}{}^d_1 = \\operatorname{RNN}_{\\text{Decoder}}\\big(\\vec{s}_1,\\, \\vec{h}{}^d_0\\big) \\\\\n",
    "\\end{equation}\n",
    "\n",
    "Now, we will determine the class with the largest prediction score in $\\vec{y}_1$, and use a one-hot encoding for that class as the input to the next step of the decoder:\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{s}_2 = \\operatorname{One-Hot}\\,(\\operatorname{arg\\,max}(\\vec{y}_1))\n",
    "\\end{equation}\n",
    "\n",
    "For example, if our predicted scores $\\vec{y}_1$ were `[0.32, 0.48, -0.12, 0.07]`, we would set $\\vec{s}_2$ to be the one-hot encoding `[0, 1, 0, 0]`.\n",
    "This process is known as \"greedy decoding\", as at each step we \"greedily\" take the best-scoring token to be the next input.\n",
    "\n",
    "In general, our decoder will follow the update functions\n",
    "\n",
    "\\begin{align}\n",
    "\\vec{y}_t,\\; \\vec{h}{}^d_t &= \\operatorname{RNN}_{\\text{Decoder}}\\big(\\vec{s}_t,\\, \\vec{h}{}^d_{t-1}\\big), \\\\\n",
    "\\vec{s}_{t+1} &= \\operatorname{One-Hot}\\,\\big(\\!\\operatorname{arg\\,max}\\big(\\vec{y}_t\\big)\\big),\n",
    "\\end{align}\n",
    "\n",
    "for a total of $T$ steps.\n",
    "Once $T$ predictions have been made, our final output from the model will be $\\big(\\vec{y}_t\\big)_{t=1}^T$.\n",
    "Note that we do not return the one-hot encodings that we used as the inputs $(\\vec{s}_t)_{t=1}^T$.\n",
    "We want to return all the classification scores so that the model can learn to raise the 'certainty' of the correct labels as it trains;\n",
    "returning the one-hot encodings of the largest score would be percieved by the loss function as the model already having absolute confidence in all of its predictions!\n",
    "\n",
    "It is also important to mention that, for more general NLP problems, we would stop the decoder from running once it yielded an end token, at which point we would return the prediction scores for only the subset of the sequence that we had processed.\n",
    "However this would require us to process our truth values accordingly to handle the potential difference in sequence length between the model output and the truth values.\n",
    "Thus for simplicity, we are always running the decoder for $T$ iterations, regardless of whether an end token appears before the final token or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b34587a",
   "metadata": {},
   "source": [
    "Let's start by setting up the tools we will need to convert each decoder output into the next input.\n",
    "\n",
    "Complete the function `one_hot_encode_prediction` below that takes in a Tensor of classification scores and returns a `float32` array of one-hot encodings for the classes with the largest prediction scores.\n",
    "NumPy's `argmax` function will be particularly helpful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8defadf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_prediction(y_t):\n",
    "    \"\"\" Converts a batch of classification scores y_t into one-hot\n",
    "    encodings corresponding to the class with the largest score.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_t: mygrad.Tensor, shape=(1, N, K)\n",
    "        The predicted classification scores from one step of\n",
    "        Seq2Seq decoding\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    s_t1: numpy.ndarray, shape=(1, N, K), dtype=np.float32\n",
    "        One-hot encodings of the labels corresponding to\n",
    "        the largest prediction scores in y_t. This should\n",
    "        be a float32 NumPy array.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    N denotes batch size\n",
    "    K denotes the number of classes (i.e. the size of our vocabulary)\n",
    "    \"\"\"\n",
    "    y_t = y_t.data\n",
    "    \n",
    "    # Creates a shape-(1, N, K) boolean-values array where \n",
    "    # the values are true wherever the element stored the max\n",
    "    # value for a given shape-(K,) array of predictions\n",
    "    max_values_mask = (y_t == y_t.max(axis=-1, keepdims=True))\n",
    "    \n",
    "    # Convert False -> 0.0 and True -> 1.0\n",
    "    # Returns a shape-(1, N, K) \"one-hot\" representation of the predictions\n",
    "    return max_values_mask.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365dd9d3",
   "metadata": {},
   "source": [
    "Run the following cell to check your implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f3211d",
   "metadata": {},
   "source": [
    "Now we'll write the `Seq2Seq` class.\n",
    "Thankfully, between the `RNN` class and `one_hot_encode_prediction` function, we already have all the major pieces we need to do this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbfa30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq:\n",
    "    def __init__(self, dim_input, dim_recurrent, dim_output):\n",
    "        \"\"\" Initializes all RNN layers needed for Seq2Seq\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dim_input: int \n",
    "            Dimensionality of data passed to Seq2Seq (C)\n",
    "        \n",
    "        dim_recurrent: int\n",
    "            Dimensionality of hidden state in RNN layers (D)\n",
    "        \n",
    "        dim_output: int\n",
    "            Dimensionality of output of Seq2Seq (K)\n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "        For this particular problem, the input dimension and\n",
    "        output dimension will be the same (C = K). In general,\n",
    "        however, this may not be the case.\n",
    "        \"\"\"\n",
    "        # Instantiate two RNNs - an encoder and a decoder.\n",
    "        # Both should use all of `dim_input`,\n",
    "        # `dim_recurrent`, and `dim_output`.\n",
    "        self.encoder = RNN(dim_input, dim_recurrent, dim_output) #\n",
    "        self.decoder = RNN(dim_input, dim_recurrent, dim_output) #\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\" Performs the full forward pass (encoding and decoding) for Seq2Seq.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x: Union[numpy.ndarray, mygrad.Tensor], shape=(T, N, C)\n",
    "            The one-hot encodings for the each sequence in the batch\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        y: mygrad.Tensor, shape=(T, N, K)\n",
    "            The final classification scores from the output of each decoder step\n",
    "        \"\"\"\n",
    "        # Run the input sequence through the encoder RNN and retain the hidden states that it returns\n",
    "        # - We won't need the predictions (y-values) from our encoder (element 0 of the returned tuple)\n",
    "        # - Save the hidden states (element 1 of the returned tuple) to the variable `enc_h`\n",
    "        #\n",
    "        # The hidden states will have shape (T, N, D)\n",
    "        # STUDENT CODE HERE\n",
    "        enc_h = self.encoder(x)[1]\n",
    "        \n",
    "        # Access the last hidden descriptor from `enc_h`\n",
    "        # to be used as the initial h_{-1} for the decoder.\n",
    "        # Make sure to preserve the dimensions, such that\n",
    "        # the shape of this hidden descriptor is (1, N, D)\n",
    "        # STUDENT CODE HERE\n",
    "        \n",
    "        # Create a list `y` to store the prediction scores y_t\n",
    "        y = #\n",
    "        \n",
    "        # Create the initial input to the decoder:\n",
    "        # a float32 array with shape (1, N, C) representing\n",
    "        # a one-hot encoding for the <START> token\n",
    "        # (which we represented as a 10).\n",
    "        # STUDENT CODE HERE\n",
    "        \n",
    "        # Iteratively produce new y_t, h_t from the decoder.\n",
    "        # Store each y_t in the list `y`.\n",
    "        # Using `one_hot_encode_prediction`, get the next\n",
    "        # decoder input s_{t+1} from the output y_t.\n",
    "        #\n",
    "        # A standard for-loop is appropriate here.\n",
    "        # STUDENT CODE HERE\n",
    "        \n",
    "        # Concatenate the y_t Tensors stored in `y` along the 0-th axis.\n",
    "        # Return the result.\n",
    "        # STUDENT CODE HERE\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        \"\"\" A convenience function for getting all the parameters of our model.\n",
    "        \n",
    "        This can be accessed as an attribute, via `model.parameters` \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[Tensor, ...]\n",
    "            A tuple containing all of the learnable parameters for our model\n",
    "        \"\"\"\n",
    "        # STUDENT CODE HERE\n",
    "    \n",
    "    def save_parameters(self, filename):\n",
    "        np.savez(filename, *(t.data for t in model.parameters))\n",
    "    \n",
    "    def load_param_weights(self, filename):\n",
    "        with np.load(filename) as f:\n",
    "            for param, arr in zip(self.parameters, f.values()):\n",
    "                param.data = arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0432a7fd",
   "metadata": {},
   "source": [
    "Create a noggin plot to track loss and accuracy below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c45120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from noggin import create_plot\n",
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc93d15",
   "metadata": {},
   "source": [
    "Instantiate a `Seq2Seq` model and `Adam` optimizer.\n",
    "Adam's default parameters are a good starting place, as is a `recurrent_dim` of $50$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643c1b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d542cb7",
   "metadata": {},
   "source": [
    "Write your training loop below, using `softmax_crossentropy` to compute loss.\n",
    "As with the basic `RNN`, make sure to reshape the output of the model and the target labels appropriately to match the shapes the function expects.\n",
    "Train your model for $10000$ iterations, with a batch size of $100$ and sequence lengths ranging from $1$ to $20$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0854eb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = #\n",
    "\n",
    "for k in range(10_000):\n",
    "    # Use generate_batch to get the... \n",
    "    # - batch: shape-(T, N, C) \n",
    "    # - the target sequences, including an end token; shape-(T, N)\n",
    "    # - (ignore the last entry -- the original squence)\n",
    "    batch, target, _ = #\n",
    "\n",
    "    predictions = # get the shape-(T, N, K) predictions\n",
    "    \n",
    "    # Reshape the predictions: (T, N, K) -> (T*N, K) \n",
    "    # Reshape the target: (T, N) -> (T*N, ) \n",
    "    loss = # compute the softmax-crossentropy loss using the reshaped predictions & targets\n",
    "    \n",
    "    # Compute dL/dw for the model's weights and use the optimizer to update the weights\n",
    "    # STUDENT CODE HERE\n",
    "    \n",
    "    acc = np.mean(np.argmax(predictions, axis=-1) == target)\n",
    "\n",
    "    plotter.set_train_batch({\"loss\":loss.item(), \"accuracy\":acc}, batch_size=batch_size)\n",
    "    \n",
    "    if k % 500 == 0 and k > 0:\n",
    "        plotter.set_train_epoch()\n",
    "\n",
    "plotter.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ba2ddb",
   "metadata": {},
   "source": [
    "Run the cell below to evaluate your model's accuracy on sequences of varying lengths.\n",
    "How does the Seq2Seq model compare to the basic RNN?\n",
    "Does anything surprise you, and if not, why do you expect these results?\n",
    "Discuss your thoughts with a neighbor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159e52bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_total = defaultdict(int)\n",
    "length_correct = defaultdict(int)\n",
    "\n",
    "with mg.no_autodiff:\n",
    "    for i in range(10_000):\n",
    "        if i % 5000 == 0:\n",
    "            print(f\"i = {i}\")\n",
    "        x, target, sequence = generate_batch(1, 20, 1)\n",
    "\n",
    "        output = model(x)\n",
    "\n",
    "        length_total[sequence.size] += 1\n",
    "        if np.all(np.argmax(output, axis=-1) == target):\n",
    "            length_correct[sequence.size] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e7b9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots()\n",
    "x, y = [], []\n",
    "for i in range(1, 20):\n",
    "    x.append(i)\n",
    "    y.append(length_correct[i] / length_total[i])\n",
    "ax.plot(x, y, marker=\"o\");\n",
    "ax.set_xlabel(\"Sequence Length\")\n",
    "ax.set_ylabel(\"Model Accuracy\")\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867abd2e",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098562bc",
   "metadata": {},
   "source": [
    "Compared to our first attempt at reversing sequences with RNNs, the Seq2Seq model seems to be a great success!\n",
    "Unfortunately, it still has a hard time learning to reverse longer sequences.\n",
    "Why might this be?\n",
    "\n",
    "Well, in RNNs, information about tokens is passed through the hidden state.\n",
    "The encoder shares its final hidden descriptor with the decoder, thus giving the decoder information about the original sequence as it computes the final outputs.\n",
    "However, the hidden state is iteratively updated as new information is recieved by the encoder and decoder.\n",
    "This means that information from the beginning of the sequence will gradually disappear over time.\n",
    "In fact for a length-$20$ sequence, information about the first digit $x_{t=0}$ would need to be saved through $40$ hidden state updates before the decoder needed it!\n",
    "\n",
    "To help the model learn these long-term relations, we can introduce a **attention** mechanism.\n",
    "\n",
    "In short, an attention mechanism allows the Seq2Seq model to look at all the encoder hidden descriptors and weight them according to importance during each decoder step.\n",
    "These weights are learned by the model during training, and allow the model to determine which encoder hidden descriptors are most relevant to the current decoding step, even if that descriptor was very far away!\n",
    "Thus each decoder descriptor will be enhanced with the most relevant (as learned by the model) parts of the encoder's hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a272c22",
   "metadata": {},
   "source": [
    "### Attention Scores and Weights and Context Vectors, Oh My"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dbf312",
   "metadata": {},
   "source": [
    "We will begin by computing **attention scores** between the each of the encoder's hidden states and the current decoder hidden descriptor.\n",
    "\n",
    "At each decoder step $t$, we will have access to the $D$-dimensional decoder hidden descriptor, $\\vec{h}{}^d_t$. We will also have the $(T,D)$ matrix containing all $T$ of the $D$-dimensional encoder hidden descriptors, $H^e$:\n",
    "\n",
    "\\begin{align}\n",
    "&\\:\\begin{matrix}\\xleftarrow{\\hspace{0.75em}} & D & \\xrightarrow{\\hspace{0.75em}}\\end{matrix} \\\\\n",
    "H^e =\\;\\, &\\begin{bmatrix}\\leftarrow & \\vec{h}{}^e_1 & \\rightarrow \\\\ \\leftarrow & \\vec{h}{}^e_2 & \\rightarrow \\\\ \\vdots & \\vdots & \\vdots \\\\ \\leftarrow & \\vec{h}{}^e_T & \\rightarrow\\end{bmatrix}\\;\\;\\begin{matrix}\\bigg\\uparrow \\\\ T \\\\ \\bigg\\downarrow\\end{matrix}\n",
    "\\end{align}\n",
    "\n",
    "Here each row in this matrix is a distinct hidden descriptor from the encoder RNN. We will also define a $(D,D)$ matrix of learnable parameters $W_\\alpha$,\n",
    "\n",
    "\\begin{align}\n",
    "&\\;\\begin{matrix}\\xleftarrow{\\hspace{2.25em}} & D & \\xrightarrow{\\hspace{2.25em}}\\end{matrix} \\\\\n",
    "W_\\alpha =\\;\\, &\\begin{bmatrix}\\uparrow & \\uparrow & \\cdots & \\uparrow \\\\ \\vec{W}_1 & \\vec{W}_2 & \\cdots & \\vec{W}_D \\\\ \\downarrow & \\downarrow & \\cdots & \\downarrow\\end{bmatrix}\\;\\;\\begin{matrix}\\big\\uparrow \\\\ D \\\\ \\big\\downarrow\\end{matrix}\n",
    "\\end{align}\n",
    "\n",
    "where we can think about each column as a distinct $D$-dimensional weight vector. Using these, we will compute attention scores between the current decoder hidden descriptor and each of the encoder hidden states as\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{e}_t = H^e W_\\alpha \\vec{h}{}^d_t\n",
    "\\end{equation}\n",
    "\n",
    "**Here** $\\vec{e}_t$ **is an augmented form of the decoder hidden descriptor at timestep** $t$ $\\big($i.e. $\\vec{h}{}^d_t\\big)$;\n",
    "**each component of this descriptor has been enhanced by the most relevant parts of all of the encoder's hidden descriptors** (where what constitutes 'relevant' is learned by the model during training)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acec29f9",
   "metadata": {},
   "source": [
    "Let's expand this out and take a moment to see how $\\vec{e}_t$ gets us this modified decoder hidden state.\n",
    "We can start with the $H^eW_\\alpha$ matrix multiplication;\n",
    "if we consider each of the $D$ columns of $W_\\alpha$ to be a $D$-dimensional weight vector, then this matrix multiplication is computing dot products between each encoder hidden state and each column vector in the weight matrix.\n",
    "\n",
    "\\begin{align}\n",
    "&\\;\\begin{matrix}\\xleftarrow{\\hspace{4.75em}} & D & \\xrightarrow{\\hspace{4.75em}}\\end{matrix} \\\\\n",
    "H^eW_\\alpha =\\;\\, &\\begin{bmatrix}\\vec{h}{}^e_1\\cdot\\vec{W}_1 & \\vec{h}{}^e_1\\cdot\\vec{W}_2 & \\cdots & \\vec{h}{}^e_1\\cdot\\vec{W}_D \\\\ \\vec{h}{}^e_2\\cdot\\vec{W}_1 & \\vec{h}{}^e_2\\cdot\\vec{W}_2 & \\cdots & \\vec{h}^e_2\\cdot\\vec{W}_D \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\vec{h}{}^e_T\\cdot\\vec{W}_1 & \\vec{h}{}^e_T\\cdot\\vec{W}_2 & \\cdots & \\vec{h}{}^e_T\\cdot\\vec{W}_D\\end{bmatrix}\\;\\;\\begin{matrix}\\bigg\\uparrow \\\\ T \\\\ \\bigg\\downarrow\\end{matrix}\n",
    "\\end{align}\n",
    "\n",
    "So each value in the $H^eW_\\alpha$ matrix represents a similarity between an encoder hidden state and a weight vector.\n",
    "We then matrix multiply with the decoder's hidden state vector, thus weighting the components of the decoder hidden state by the 'similarity scores' computed between each weight vector and each encoder hidden state.\n",
    "\n",
    "\\begin{align}\n",
    "&\\begin{bmatrix}\n",
    "\\hspace{2.3em} (h^d_t)_1 \\hspace{2.3em}\\vphantom{\\vec{h}} \\\\\n",
    "\\hspace{2.3em} (h^d_t)_2 \\hspace{2.3em}\\vphantom{\\vec{h}} \\\\\n",
    "\\vdots \\\\\n",
    "\\hspace{2.3em} (h^d_t)_D \\hspace{2.3em}\\vphantom{\\vec{h}}\n",
    "\\end{bmatrix} \\\\\n",
    "%\n",
    "\\vec{e}_t = H^eW_\\alpha\\vec{h}{}^d_t =\n",
    "\\begin{bmatrix}\n",
    "\\vec{h}{}^e_1\\cdot\\vec{W}_1 \\vphantom{\\sum\\limits_{i=1}^D}\n",
    "    & \\vec{h}{}^e_1\\cdot\\vec{W}_2\n",
    "    & \\cdots \n",
    "    & \\vec{h}{}^e_1\\cdot\\vec{W}_D \\\\\n",
    "\\vec{h}{}^e_2\\cdot\\vec{W}_1 \\vphantom{\\sum\\limits_{i=1}^D}\n",
    "    & \\vec{h}{}^e_2\\cdot\\vec{W}_2\n",
    "    & \\cdots\n",
    "    & \\vec{h}{}^e_2\\cdot\\vec{W}_D \\\\\n",
    "\\vdots\n",
    "    & \\vdots\n",
    "    & \\ddots\n",
    "    & \\vdots \\\\\n",
    "\\vec{h}{}^e_T\\cdot\\vec{W}_1 \\vphantom{\\sum\\limits_{i=1}^D}\n",
    "    & \\vec{h}{}^e_T\\cdot\\vec{W}_2\n",
    "    & \\cdots\n",
    "    & \\vec{h}{}^e_T\\cdot\\vec{W}_D\n",
    "\\end{bmatrix}\n",
    "%\n",
    "&\\begin{bmatrix}\n",
    "\\sum\\limits_{i=1}^D\\big(\\vec{h}{}^e_1\\cdot\\vec{W}_i\\big)(h^d_t)_i \\\\\n",
    "\\sum\\limits_{i=1}^D\\big(\\vec{h}{}^e_2\\cdot\\vec{W}_i\\big)(h^d_t)_i \\\\\n",
    "\\vdots \\\\\n",
    "\\sum\\limits_{i=1}^D\\big(\\vec{h}{}^e_T\\cdot\\vec{W}_i\\big)(h^d_t)_i\n",
    "\\end{bmatrix} =\n",
    "%\n",
    "\\begin{bmatrix}\n",
    "e_{t,1} \\vphantom{\\sum\\limits_{i=1}^D} \\\\ \n",
    "e_{t,2} \\vphantom{\\sum\\limits_{i=1}^D} \\\\ \n",
    "\\vdots \\\\ \n",
    "e_{t,T} \\vphantom{\\sum\\limits_{i=1}^D}\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "As the model trains, the $i^\\text{th}$ weight vector will be tuned to extract meaningful information from all of the encoder hidden descriptors that is relevant specifically to the $i^\\text{th}$ component of the decoder hidden state.\n",
    "If an encoder hidden state consistently has information that is particularly relevant to a component of the decoder hidden state, the weight vectors will learn this pattern and amplify it in the final attention score.\n",
    "\n",
    "The matrix of learnable weights thus allows the model to learn more complex contextual relationships between the hidden descriptors of the encoder and those of the decoder.\n",
    "Notice that if $W_\\alpha$ were the identity matrix, we would actually be calculating the dot products between the decoder hidden descriptor $\\vec{h}{}^d_t$ and each of the encoder hidden states.\n",
    "Thus the attention score defined above can be thought of as a weighted dot product, and hence will retain similar vector-overlap computing abilities along with additional learned weightings;\n",
    "just as dot products measure the overlap between two vectors, _we can think of these attention scores as a measure of how similar, or 'relevant', the encoder hidden descriptor is to the decoder hidden state_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d250c9b2",
   "metadata": {},
   "source": [
    "Conveniently enough, our `RNN` class will return an exactly the tensor we need containing all of the hidden states!\n",
    "This means we only need to define a MyNN `dense` for $W_\\alpha$, and we'll be able to start computing the attention scores.\n",
    "We will want to precompute the $H^eW_\\alpha$ right after we get the output of the encoder, before running any decoder steps:\n",
    "\n",
    "```python\n",
    "precomputed_encoder_score_vectors = W_alpha(encoder_hidden_states)\n",
    "# (T, N, D) @ (D, D) -> (T, N, D)\n",
    "```\n",
    "\n",
    "Doing this means we will only compute these values once, as opposed to recomputing each time we call the decoder.\n",
    "During each decoder step, we can compute the final attention score vector by taking a dot product:\n",
    "\n",
    "```python\n",
    "e_t = (precomputed_encoder_score_vectors * h_t).sum(axis=-1)\n",
    "# (T, N, D) * (1, N, D) -> (T, N, D).sum(axis=-1) -> (T, N)\n",
    "```\n",
    "\n",
    "Here, the $i^\\text{th}$ row corresponds to the attention score $e_{t,i}$ between the current decoder hidden descriptor $\\vec{h}{}^d_t$, and the $i^\\text{th}$ encoder hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213b8580",
   "metadata": {},
   "source": [
    "With this vector of attention scores, we can calculate the **attention weights** for the decoder hidden descriptor.\n",
    "Intuitively, a higher weight will tell the model: \"for the current decoder step, it is important to pay more attention to this particular encoder hidden state than the other hidden states\".\n",
    "A lower weight will tell the model: \"for the current decoder step, it is ok to mostly ignore the information contained within this particular encoder hidden state\".\n",
    "In this way, the model is learning to ascribe an importance of each encoder hidden state to the decoder hidden descriptor.\n",
    "\n",
    "The attention scores for the current decoder hidden descriptor are computed by taking the softmax of the attention scores,\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{\\alpha}_t = \\operatorname{softmax}(\\vec{e}_t) = \\frac{\\exp(\\vec{e}_t)}{\\sum_{i=1}^T\\exp(e_{t,i})} = \\frac{\\exp(\\vec{e}_t)}{\\exp(e_{t,1}) + \\exp(e_{t,2}) + \\cdots + \\exp(e_{t,T})}.\n",
    "\\end{equation}\n",
    "\n",
    "Since MyGrad has a `softmax` function built-in, we can simply apply it to our attention scores, taking care to take the softmax over the dimension corresponding to the $T$ attention scores:\n",
    "\n",
    "```python\n",
    "a_t = mg.nnet.softmax(e_t, axis=0)\n",
    "# softmax((T, N), axis=0) -> (T, N)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aa64d6",
   "metadata": {},
   "source": [
    "Finally, we will use our attention weights to compute a **context vector** that 'summarizes' the information from all the encoder hidden states.\n",
    "We do this by taking a weighted sum of the encoder hidden states, where the coefficients of the hiddens states are the previously computed attention weights!\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{c}_t = \\sum_{j=1}^T \\alpha_{j}\\vec{h}{}^e_j = \\alpha_{1}\\big(\\vec{h}{}^e_1\\big) + \\alpha_{2}\\big(\\vec{h}{}^e_2\\big) + \\cdots + \\alpha_{T}\\big(\\vec{h}{}^e_T\\big)\n",
    "\\end{equation}\n",
    "\n",
    "Since less relevant encoder hidden states are given lower attention weights, the context vector $\\vec{c}_t$ will primarily contain information from the most relevant hidden states.\n",
    "In our implementation, since our batch dimension is the $1^\\text{st}$ axis, we will need to take care to sum along the axis corresponding to the sequence length:\n",
    "\n",
    "```python\n",
    "c_t = (a_t[..., None] * encoder_hidden_states).sum(axis=0, keepdims=True)\n",
    "# (T, N, 1) * (T, N, D) -> (T, N, D).sum(axis=0, keepdims=True) -> (1, N, D)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed116bf0",
   "metadata": {},
   "source": [
    "Well so far this is great and all, but what are we supposed to do with the context vector now?\n",
    "That is, how do we incorporate the context vector into the output from the decoder RNN?\n",
    "\n",
    "There are a number of ways that machine learning researchers have done this;\n",
    "the approach we will take will have us concatenate the context vector $\\vec{c}_t$ to the output $\\vec{y}_t$ of the normal decoder step.\n",
    "\n",
    "\n",
    "That is, we will compute the decoder step as normal, using $\\vec{s}_t$ and $\\vec{h}{}^d_t$, to get $\\vec{y}_t$ and $\\vec{h}{}^d_t$.\n",
    "However, before we store $\\vec{y}_t$ and use it to determine the next decoder input $\\vec{s}_{t+1}$, we will concatenate $\\vec{y}_t$ and the context vector $\\vec{c}_t$.\n",
    "\n",
    "\n",
    "```python\n",
    "y_and_c = mg.concatenate([y_t, c_t], axis=-1)\n",
    "# concatenate([(1, N, K), (1, N, D)], axis=-1) -> (1, N, K + D)\n",
    "```\n",
    "\n",
    "Since we change the dimensionality of the output vector, we will need to apply a dense layer to correct the output dimensionality and recompute the final classification scores.\n",
    "\n",
    "```python\n",
    "y_t = post_concat_dense(y_and_c)\n",
    "# (1, N, K + D) @ (K + D, K) -> (1, N, K)\n",
    "```\n",
    "\n",
    "And that's that!\n",
    "The final dense layer acts as a sort of interpreter, integrating the information carried in the context vector with that in the decoder output.\n",
    "Thus our final output for each decoder step has contained in it information directly taken from the most important encoder hidden states!\n",
    "If there is a long-term dependency in the source and target sequences, the output vector should be able to pick up on it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebacb61f",
   "metadata": {},
   "source": [
    "Let's finally put this all to work.\n",
    "\n",
    "In the class below, augment your previous Seq2Seq models with the attention mechanism described above.\n",
    "On each decoder step, save the computed attention weights.\n",
    "In `__call__`, return both the predicted scores and a $(T,\\, T,\\, N)$ array of attention weights, whose $0^\\text{th}$ dimension corresponds to the attention weights for each decoder step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f030fc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSeq2Seq:\n",
    "    def __init__(self, dim_input, dim_recurrent, dim_output):\n",
    "        \"\"\" Initializes all RNN layers needed for Seq2Seq\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dim_input: int \n",
    "            Dimensionality of data passed to Seq2Seq (C)\n",
    "        \n",
    "        dim_recurrent: int\n",
    "            Dimensionality of hidden state in RNN layers (D)\n",
    "        \n",
    "        dim_output: int\n",
    "            Dimensionality of output of Seq2Seq (K)\n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "        For this particular problem, the input dimension and\n",
    "        output dimension will be the same (C = K). In general,\n",
    "        however, this may not be the case.\n",
    "        \"\"\"\n",
    "        # Instantiate two RNNs, an encoder and a decoder, as done before.\n",
    "        #\n",
    "        # Then, instantiate two MyNN dense layers:\n",
    "        # - one corresponding to the (D, D) matrix W_alpha (with no bias),\n",
    "        # - one to take the concat-ed y_t and c_t vector (dim K+D)\n",
    "        #   to a vector with the appropriate output dimension (dim K).\n",
    "        #\n",
    "        # Use a glorot_normal weight initializer for \n",
    "        # both of these dense layers.\n",
    "        self.encoder = # RNN w /shape: (dim_input, dim_recurrent, dim_output)\n",
    "        self.decoder = # RNN w /shape (dim_input, dim_recurrent, dim_output)\n",
    "        \n",
    "        self.W_alpha = # dense with shape (dim_recurrent, dim_recurrent)\n",
    "        self.post_concat_dense = #  dense with shape (dim_recurrent + dim_output, dim_output)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\" Performs the full forward pass (encoding and decoding) for Seq2Seq.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x: Union[numpy.ndarray, mygrad.Tensor], shape=(T, N, C)\n",
    "            The one-hot encodings for the each sequence in the batch\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        y: mygrad.Tensor, shape=(T, N, K)\n",
    "            The final classification scores from the output of each decoder step\n",
    "        a_ij: numpy.array, shape=(T, T, N)\n",
    "            The computed attention weights for the batch of sequences. The 0-th\n",
    "            dimension corresponds to the attention weights computed at each\n",
    "            decoder step\n",
    "        \"\"\"\n",
    "        # Perform the same encoding and decoder setup as\n",
    "        # in the `Seq2Seq` model. That is:\n",
    "        #\n",
    "        # - get the encoder hidden states `enc_h`,\n",
    "        # - set the initial decoder hidden state to the\n",
    "        #   last encoder hidden state\n",
    "        # - create a list `y` to store the decoder outputs\n",
    "        # - initialize a <START> token one-hot encoding\n",
    "        #   as the intial decoder input\n",
    "        # STUDENT CODE HERE\n",
    "        \n",
    "        # Apply the dense layer corresponding to W_alpha\n",
    "        # to `enc_h` to precompute part of the attention scores\n",
    "        # STUDENT CODE HERE\n",
    "        \n",
    "        # This create a list `a_ij` to store the\n",
    "        # attention weights at each decoder step.\n",
    "        a_ij = []\n",
    "        \n",
    "        for _ in range(T):\n",
    "            # Compute the attention scores `e_t` for\n",
    "            # the current decoder hidden state using the\n",
    "            # precomputed W_alpha * H^e.\n",
    "            e_t = # e_t: shape-(T, N)\n",
    "            \n",
    "            # Take the softmax of the attention scores to\n",
    "            # compute the attention weights. Make sure to take\n",
    "            # the softmax over the `T` dimension.\n",
    "            #\n",
    "            # Store the result in the variable `a_t`.\n",
    "            a_t = # a_t: shape-(T, N)\n",
    "            \n",
    "            # This appends the attention weights for the current\n",
    "            # decoder step to the `a_ij` list. The new axis is to\n",
    "            # allow the weights to be concatenated into a single matrix.\n",
    "            a_ij.append(a_t.data[None])\n",
    "            \n",
    "            # Compute the context vector `c_t` from the encoder hidden states\n",
    "            # `enc_h` and the attention weights `a_t`.\n",
    "            c_t = # c_t: shape-(1, N, D)\n",
    "            \n",
    "            # As in the basic Seq2Seq model, perform one decoder step to\n",
    "            # get y_t and h_t.\n",
    "            \n",
    "            # y_t: shape-(1, N, K)  # y_pred for this timestep\n",
    "            # h_t: shape-(1, N, D)  # hiddent state for this timestep\n",
    "            y_t, h_t = #\n",
    "            \n",
    "            # Concatenate the decoder output y_t and the context\n",
    "            # vector c_t along the last axis.\n",
    "            y_t = # y_t: shape-(1, N, K + D)\n",
    "            \n",
    "            # Apply a dense layer to compress the concatenated vectors\n",
    "            # into a vector with the appropriate output dimensionality (K).\n",
    "            # Append this final `y_t` to the list `y`.\n",
    "            y_t = # y_t: shape-(1, N, K)\n",
    "            y.append(y_t)\n",
    "            \n",
    "            # Use `one_hot_encode_prediction` to find the\n",
    "            # next decoder input s_{t+1} based on the now-combined\n",
    "            # decoder output and context vector `y_t`.\n",
    "            s_t = # s_t: shape-(1, N, K)\n",
    "        \n",
    "        # Concatenate the y_t Tensors stored in `y` along the 0-th axis.\n",
    "        y_concat = # y_concat: shape-(T, N, K)\n",
    "        \n",
    "        # This concatenates the T attention vectors `a_t` that\n",
    "        # were computed for each decoder step (along the 0-th axis).\n",
    "        a_ij = np.concatenate(a_ij, axis=0) # a_ij: shape-(T, T, N)\n",
    "        \n",
    "        # Return the appropriate tensors and arrays - in accordance with the docstring.\n",
    "        # STUDENT CODE HERE\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        \"\"\" A convenience function for getting all the parameters of our model.\n",
    "        \n",
    "        This can be accessed as an attribute, via `model.parameters`\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[Tensor, ...]\n",
    "            A tuple containing all of the learnable parameters for our model\n",
    "        \"\"\"\n",
    "        # STUDENT CODE HERE\n",
    "\n",
    "    def save_parameters(self, filename):\n",
    "        np.savez(filename, *(t.data for t in model.parameters))\n",
    "    \n",
    "    def load_param_weights(self, filename):\n",
    "        with np.load(filename) as f:\n",
    "            for param, arr in zip(self.parameters, f.values()):\n",
    "                param.data = arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29148ae0",
   "metadata": {},
   "source": [
    "Instatiate a noggin plot here to keep track of the loss and accuracy of the model as it trains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb69f844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from noggin import create_plot\n",
    "plotter, fig, ax = create_plot([\"loss\", \"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67fb150",
   "metadata": {},
   "source": [
    "Instantiate an `AttentionSeq2Seq` model and an `Adam` optimizer.\n",
    "For the model, a `dim_recurrent` of $25$ is a good choice.\n",
    "For the optimizer, the default parameters are a good starting place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5420c53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = #\n",
    "optimizer = #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c263311",
   "metadata": {},
   "source": [
    "Write the training loop below.\n",
    "Use a batch size of $100$ with sequence lengths between $1$ and $20$.\n",
    "Train your model for $8,000$ iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576f4fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't change this -- we will be using it to save attention heat maps for a later visualization\n",
    "saved_x, saved_target, saved_sequence = generate_batch(15, 15, 1)\n",
    "saved_attention = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4097cee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = #\n",
    "\n",
    "for k in range(8000):\n",
    "    # Use generate_batch to get the... \n",
    "    # - batch: shape-(T, N, C) \n",
    "    # - the target sequences, including an end token; shape-(T, N)\n",
    "    # - (ignore the last entry -- the original squence)\n",
    "    batch, target, _ = #\n",
    "\n",
    "    predictions, _ = # get the shape-(T, N, K) predictions\n",
    "    \n",
    "    # Reshape the predictions: (T, N, K) -> (T*N, K) \n",
    "    # Reshape the target: (T, N) -> (T*N, ) \n",
    "    loss = # compute the softmax-crossentropy loss using the reshaped predictions & targets\n",
    "    \n",
    "    # Compute dL/dw for the model's weights and use the optimizer to update the weights\n",
    "    # STUDENT CODE HERE\n",
    "    \n",
    "    acc = np.mean(np.argmax(predictions, axis=-1) == target)\n",
    "\n",
    "    plotter.set_train_batch({\"loss\":loss.item(), \"accuracy\":acc}, batch_size=batch_size)\n",
    "    \n",
    "    # This will record our model's attention heat map every 10 steps of training\n",
    "    if k % 10 == 0 and k > 0:\n",
    "        with mg.no_autodiff:\n",
    "            _, a_ij = model(saved_x)\n",
    "            saved_attention.append(a_ij)\n",
    "            \n",
    "    if k % 500 == 0 and k > 0:\n",
    "        plotter.set_train_epoch()\n",
    "\n",
    "plotter.plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdc49b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save_parameters(\"seq2seq_weights.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64a4a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "loaded_model = AttentionSeq2Seq(dim_input=12, dim_recurrent=25, dim_output=12)\n",
    "loaded_model.load_param_weights(\"./seq2seq_weights.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08cdee2",
   "metadata": {},
   "source": [
    "Run the following to evaluate the accuracy of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9a8be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_total = defaultdict(int)\n",
    "length_correct = defaultdict(int)\n",
    "\n",
    "with mg.no_autodiff:\n",
    "    for i in range(5000):\n",
    "        if i % 5000 == 0:\n",
    "            print(f\"i = {i}\")\n",
    "        x, target, sequence = generate_batch(1, 20, 1)\n",
    "\n",
    "        output, _ = loaded_model(x)\n",
    "\n",
    "        length_total[sequence.size] += 1\n",
    "        if np.all(np.argmax(output, axis=-1) == target):\n",
    "            length_correct[sequence.size] += 1\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x, y = [], []\n",
    "for i in range(1, 20):\n",
    "    x.append(i)\n",
    "    y.append(length_correct[i] / length_total[i])\n",
    "\n",
    "ax.plot(x, y, marker=\"o\");\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.set_xlabel(\"Sequence Length\")\n",
    "ax.set_ylabel(\"% perfect sequence generation\")\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3abb00d",
   "metadata": {},
   "source": [
    "Awesome!\n",
    "We can see perfect or near-perfect accuracy for all the sequence lengths we trained on (if trained long enough, the model will be able to completely master this problem)!\n",
    "\n",
    "Let's finish up by taking a look at the attention weights computed for a single sequence.\n",
    "Run the cell below to visualize the attention weights for a sequence length of your choosing.\n",
    "\n",
    "The first cell will show how our model's attention evolved across training.\n",
    "The way that we read this is:\n",
    "  - The row-location (`t`) of the attention heat map which value in the output is being decoded.\n",
    "  - The pattern of hot/cold values in row-`t` indicate where in the *original* sequence our model is attending to in order to produce the decoded output `y_t`.\n",
    "  \n",
    "Consider: in order to produce a reversed list, when you are producing the first element in the output(decoded) list where do you need to look in the original list to determine that value? When producing the second value in the reversed list, where in the original list do you need to look to determine its value? Based on these considerations, see if you can predict what the attention heat map *should* look like for a well-trained sequence-reversal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9ca848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute this cell\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "t = saved_attention[0].squeeze()\n",
    "im = plt.imshow(t, animated=True, cmap='viridis')\n",
    "\n",
    "def updatefig(*args):\n",
    "    im.set_array(saved_attention[args[0]].squeeze())\n",
    "    return im,\n",
    "\n",
    "\n",
    "ax.set_ylabel(\"Sequence-location being decoded\")\n",
    "ax.set_yticks(range(saved_attention[0].shape[0]))\n",
    "ax.set_yticklabels([\"<S>\"] + list(range(1, saved_attention[0].shape[0])))\n",
    "\n",
    "ax.xaxis.tick_top()\n",
    "\n",
    "ax.set_xticks(range(saved_attention[0].shape[0]))\n",
    "ax.set_xticklabels(list(range(0, saved_attention[0].shape[0] - 1)) + [\"<E>\"])\n",
    "ax.set_xlabel(\"Locations where original sequence is being attended to\")\n",
    "ax.xaxis.set_label_position('top') \n",
    "\n",
    "ani = FuncAnimation(fig, updatefig, range(len(saved_attention)), interval=1, blit=True, repeat=False,\n",
    "                    repeat_delay=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72431137",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 15\n",
    "\n",
    "x, target, sequence = saved_x, saved_target, saved_sequence\n",
    "y, a_ij = loaded_model(x)\n",
    "y = np.argmax(y, axis=-1).squeeze() # determine decoder inputs\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_yticks(range(y.size))\n",
    "ax.set_yticklabels([\"<S>\"] + [x for x in y[:-1]])\n",
    "ax.set_ylabel(\"Input to Decoder\")\n",
    "\n",
    "ax.xaxis.tick_top()\n",
    "ax.xaxis.set_label_position('top') \n",
    "\n",
    "ax.set_xticks(range(target.size))\n",
    "ax.set_xticklabels([x for x in sequence.squeeze()] + [\"<E>\"])\n",
    "ax.set_xlabel(\"Original Sequence\")\n",
    "\n",
    "ax.imshow(a_ij.squeeze());"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9df27c9",
   "metadata": {},
   "source": [
    "Did you expect to see these results?\n",
    "Do they make intuitive sense, given the problem we are trying to solve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3749080",
   "metadata": {},
   "source": [
    "Now let's make a function that makes it easy for us to pass our model an input sequence, and have it (try to) produce the mirrored sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d39c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_digits_via_model(digits: list):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    digits: list[int]\n",
    "       A sequence of integers\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    decoded_reversed_list : list[int]\n",
    "        Our model's decoded output; should be a reversed version of `digits` and end with the \"<END>\" character.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> reverse_digits_via_model([1, 2, 3])\n",
    "    [3, 2, 1, '<END>']\n",
    "    \"\"\"\n",
    "    digits = np.asarray(digits)\n",
    "    if digits.ndim == 1:\n",
    "        # make a size-1 batch of digits\n",
    "        digits = digits.reshape(1, -1)  # shape-(N, T)\n",
    "    digits = digits.T\n",
    "    T_1, N = digits.shape\n",
    "\n",
    "    one_hot_x = np.zeros((T_1 + 1, N, 12), dtype=np.float32) \n",
    "    one_hot_x[np.arange(T_1).reshape(-1, 1), np.arange(N), digits] = 1\n",
    "    \n",
    "    scores = model(one_hot_x)[0]  # Ignore T-th output -- <END> token\n",
    "    \n",
    "    out = np.argmax(scores, -1).transpose().tolist()\n",
    "    \n",
    "    # We want to make clear when our model predicts the \"<END>\" token -- this should\n",
    "    # always/only occur at the end of its predeiction!\n",
    "    out = [[x if x != 11 else \"<END>\" for x in sub] for sub in out]\n",
    "    return out[0] if len(out) == 1 else out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b10110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with this!\n",
    "reverse_digits_via_model([1, 2, 4, 8, 9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c9ed76",
   "metadata": {},
   "source": [
    "The introduction of attention was a significant step forward in understandability in deep learning, where oftentimes the high-dimensional, highly-nonlinear functions that get learned are next to impossible to interpret.\n",
    "In fact, attention is such a powerful tool that, in the final notebook, we will see how we can construct a language model _only using attention_."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python",
   "notebook_metadata_filter": "nbsphinx,-kernelspec"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "nbsphinx": {
   "execute": "never"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
