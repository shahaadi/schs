{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce59d4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from mynn.layers.dense import dense\n",
    "from mynn.optimizers.adam import Adam\n",
    "\n",
    "from mygrad.nnet.losses import softmax_crossentropy\n",
    "from mygrad.nnet.initializers import glorot_normal\n",
    "from mygrad.nnet.activations import relu\n",
    "\n",
    "import mygrad as mg\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5101fa07",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96db707b",
   "metadata": {},
   "source": [
    "In 2017, Google Brain release a paper titled _Attention is All you Need_, where they departed from the traditional Seq2Seq architectures used in NLP and introduced a new kind of architecture: the transformer.\n",
    "As the title of their paper suggests, the transformer is based on attention mechanisms, and does not use any kind of recurrent states as in RNNs.\n",
    "This meant that a transformer could be trained significantly faster than a Seq2Seq model using RNNs, as sequences were no longer processed one token at a time, but rather all at once.\n",
    "It also significantly reduced the complexity of the models being worked with.\n",
    "As opposed to having complex Seq2Seq variations that can quickly become intractable, the transformer is composed of relatively straightforward matrix multiplications all the way through.\n",
    "\n",
    "Since their introduction, transformers have been taking over the world of NLP.\n",
    "GPT-3, a state-of-the-art language model capable of outputting text nearly impossible to distinguish from something human-written, is a massive transformer model.\n",
    "Transformers have also seen applications in other fields of machine learning like computer vision, as their expresive power is, well, powerful.\n",
    "\n",
    "In this notebook, we will take a deep dive into the original transformer architecture, writing up our own transformer and applying it to the translation problem of decoding a cipher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a9f8f1",
   "metadata": {},
   "source": [
    "## Making the Data (will be removed in final version, but for now included since the data isn't uploaded anywhere/might be changed slightly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca40545c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cogworks_data.language import get_data_path\n",
    "\n",
    "# path_to_wikipedia = get_data_path(\"wikipedia2text-extracted.txt\")\n",
    "\n",
    "# with open(path_to_wikipedia, \"rb\") as f:\n",
    "#     wikipedia = f.read().decode()\n",
    "#     wikipedia = wikipedia.lower()\n",
    "# print(str(len(wikipedia)) + \" character(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fa3281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# # keep alpha + spaces\n",
    "# wikipedia = re.sub(\"[^ a-z]\", \"\", wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37336226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import zip_longest\n",
    "\n",
    "# str_len = 25\n",
    "# # using all data is like 30 gigs in numpy array form\n",
    "# percent_of_data_to_use = 0.05\n",
    "\n",
    "# wiki_ls = [''.join(filter(None, s)) for s in zip_longest(*([iter(wikipedia)]*(str_len-1)))]\n",
    "# wiki_ls = wiki_ls[:-1] if len(wiki_ls[-1]) != str_len else wiki_ls\n",
    "\n",
    "# wiki_ls = wiki_ls[:int(len(wiki_ls) * percent_of_data_to_use)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fe879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = \"abcdefghijklmnopqrstuvwxyz \"\n",
    "IND_LOOKUP = {x:i for i, x in enumerate(ALPHA)}\n",
    "LOOKUP_TABLE = np.array([x for x in ALPHA])\n",
    "LOOKUP_TABLE = np.concatenate([np.roll(LOOKUP_TABLE, i)[None] for i in range(0, -len(ALPHA), -1)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d844dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cipher(plaintext, key):\n",
    "    len_p = len(plaintext)\n",
    "    len_k = len(key)\n",
    "\n",
    "    key = (key * (len_p // len_k + 1))[:len_p]\n",
    "    \n",
    "    out = \"\"\n",
    "    for p, k in zip(plaintext, key):\n",
    "        ind_p = IND_LOOKUP[p]\n",
    "        ind_k = IND_LOOKUP[k]\n",
    "\n",
    "        out += LOOKUP_TABLE[ind_k, ind_p]\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0981ffb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"clap\"  # the cipher key we will use encrypt all of our training data\n",
    "\n",
    "# wiki_enc_ls = [cipher(x, key) for x in wiki_ls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedd5962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(text):\n",
    "    \"\"\"\n",
    "    adds both a start and end token\n",
    "    \"\"\"\n",
    "    seq_len = len(text)\n",
    "    # +2 to seq_len to add in start/end tokens, +2 to dimension to account for new start/end tokens\n",
    "    # start = [... 0 1 0], end = [... 0 0 1]\n",
    "    out = np.zeros((seq_len+2, len(ALPHA)+2), dtype=np.float32)\n",
    "    \n",
    "    inds = [IND_LOOKUP[x] for x in text]\n",
    "    out[range(1, seq_len+1), inds] = 1\n",
    "    out[0, -2] = 1\n",
    "    out[-1, -1] = 1\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6db13d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # src one-hots dont need start token\n",
    "# wiki_src_dat = np.concatenate([one_hot_encoding(x)[1:, None] for x in wiki_enc_ls], axis=1)\n",
    "\n",
    "# # tgt one-hots dont need end token\n",
    "# wiki_tgt_dat = np.concatenate([one_hot_encoding(x)[:-1, None] for x in wiki_ls], axis=1)\n",
    "\n",
    "# print(wiki_src_dat.shape, wiki_tgt_dat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd638ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tgt inds do need end token (and no start)\n",
    "# wiki_tgt_inds = np.concatenate([np.array([IND_LOOKUP[x] for x in tgt_str] + [len(ALPHA) + 1]).reshape(-1, 1)\n",
    "#                                 for tgt_str in wiki_ls], axis=1)\n",
    "\n",
    "# print(wiki_tgt_inds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bcef04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dat = [\"cogworks has no asterisk\",\n",
    "#             \"blahblahblahblahblahblah\"]\n",
    "\n",
    "# test_dat = [cipher(x, key) for x in test_dat]\n",
    "# test_dat = np.concatenate([one_hot_encoding(x)[1:, None] for x in test_dat], axis=1)\n",
    "\n",
    "# print(test_dat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e09f167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savez(\"./dat/wikipedia2text-encoded.npz\",\n",
    "#          train_src_one_hot=wiki_src_dat,\n",
    "#          train_tgt_one_hot=wiki_tgt_dat,\n",
    "#          train_truth_inds=wiki_tgt_inds,\n",
    "#          test_src_one_hot=test_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcb1ded",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d3eee5",
   "metadata": {},
   "source": [
    "For this notebook, the data we will use is text encoded with a Vigen√®re cipher.\n",
    "We will look at a set of encoded (source) and corresponding decoded (target) sequences to train our model on;\n",
    "our goal will be to train a transformer to decode any new sequences that we get which have been encoded with the original cipher's key.\n",
    "Tokens in these sequences are lowercase alphabetical characters, spaces, or start/end tokens.\n",
    "All source sequences already have a start token prepended, and all target sequences have an end token appended.\n",
    "The order of dimensions of the data is the same as in the Seq2Seq notebook.\n",
    "\n",
    "Load in the training and testing data below, and print the shapes of each of the arrays.\n",
    "What does each dimension in these arrays represent?\n",
    "\n",
    "Note: moving forward, the sequence length $T$ or $t$ will encompass any start or end tokens in the data, respectively.\n",
    "This is in contrast to the Seq2Seq notebook, where we referred to the sequence length as the sequence excluding start and end tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6299f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cogworks_data.language import get_data_path\n",
    "\n",
    "with np.load(get_data_path(\"wikipedia2text-encoded.npz\")) as data:\n",
    "    train_src_one_hot = data[\"train_src_one_hot\"]\n",
    "    train_tgt_one_hot = data[\"train_tgt_one_hot\"]\n",
    "    train_truth_inds = data[\"train_truth_inds\"]\n",
    "    test_src_one_hot = data[\"test_src_one_hot\"]\n",
    "\n",
    "# <COGINST>\n",
    "# (T, N_train, C), (T, N_train, C), (T, N_train), (T, N_test, C)\n",
    "print(train_src_one_hot.shape, train_tgt_one_hot.shape,\n",
    "      train_truth_inds.shape, test_src_one_hot.shape)\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5535518",
   "metadata": {},
   "source": [
    "## Multihead Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09254e7b",
   "metadata": {},
   "source": [
    "In the previous notebook, we constructed an attention mechanism by using a few matrix multiplications to compute a \"relevance score\" between each of the encoder's hidden states and the decoder's current hidden state.\n",
    "This simple mechanism proved very effective for the task of reversing digit sequences, as our model was able to completely master the problem when trained.\n",
    "\n",
    "Here, we will be using a _scaled dot product attention_. As the name suggests, we will compute our attention scores by computing dot products.\n",
    "\n",
    "This attention function takes in three inputs: the _queries_ $Q$, the _keys_ $K$, and the _values_ $V$.\n",
    "The names are not completely arbitrary here: we can think of the keys and values as key-value pairs in a Python dictionary.\n",
    "\n",
    "The queries and the keys are the two values compared to compute attention weights;\n",
    "in particular, the attention weights measure how relevant the keys are to the queries.\n",
    "The attention weights are then used in a weighted sum of the value vectors.\n",
    "Thus how relevant a key is to the query determines how strongly weighted the corresponding value is in the final context vector.\n",
    "\n",
    "For a given sequence, $Q$ will each be a $(t, d_k)$ matrix, $K$ will be a $(T, d_k)$ matrix, and $V$ will be a $(T, d_v)$ matrix;\n",
    "in all three, each row corresponds to a token embedding in the sequence.\n",
    "For instance, the queries will be represented by the matrix\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\:\\begin{matrix}\\xleftarrow{\\hspace{0.5em}} & d_k & \\xrightarrow{\\hspace{0.5em}}\\end{matrix} \\\\\n",
    "Q =\\;\\, &\\begin{bmatrix}\\leftarrow & \\vec{q}_1 & \\rightarrow \\\\ \\leftarrow & \\vec{q}_2 & \\rightarrow \\\\ & \\vdots & \\\\ \\leftarrow & \\vec{q}_t & \\rightarrow\\end{bmatrix}\\;\\;\\begin{matrix}\\Big\\uparrow \\\\ t \\\\ \\Big\\downarrow\\end{matrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "While the sequence lengths $T$ and $t$ may be the same, it is also possible that they differ - hence the distinction.\n",
    "\n",
    "As mentioned before, we will compute attention scores from $Q$ and $K$.\n",
    "In particular, we will matrix multiply $Q$ and the transpose of $K$ to calculate the dot product between each query vector and each key vector\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\begin{bmatrix}\n",
    "    \\;\\;\\; \\uparrow \\;\\; & \\quad \\uparrow \\; & & \\;\\, \\uparrow \\\\ \n",
    "    \\;\\;\\; \\vec{k}_1 \\;\\; & \\quad \\vec{k}_2 \\; & \\;\\; \\cdots & \\;\\;\\;\\, \\vec{k}_T \\;\\; \\\\ \n",
    "    \\;\\;\\; \\downarrow \\;\\; & \\quad \\downarrow \\; & & \\;\\, \\downarrow\n",
    "\\end{bmatrix} \\\\\n",
    "E = QK^\\intercal =\n",
    "\\begin{bmatrix}\n",
    "    \\leftarrow & \\vec{q}_1 & \\rightarrow \\vphantom{\\vec{q}_1\\cdot\\vec{k}_t} \\\\\n",
    "    \\leftarrow & \\vec{q}_2 & \\rightarrow \\vphantom{\\vec{q}_2\\cdot\\vec{k}_t} \\\\\n",
    "    & \\vdots & \\\\\n",
    "    \\leftarrow & \\vec{q}_t & \\rightarrow \\vphantom{\\vec{q}_T\\cdot\\vec{k}_t}\n",
    "\\end{bmatrix}\n",
    "&\\begin{bmatrix}\n",
    "    \\vec{q}_1\\cdot\\vec{k}_1 & \\vec{q}_1\\cdot\\vec{k}_2 & \\cdots & \\vec{q}_1\\cdot\\vec{k}_T \\\\\n",
    "    \\vec{q}_2\\cdot\\vec{k}_1 & \\vec{q}_2\\cdot\\vec{k}_2 & \\cdots & \\vec{q}_2\\cdot\\vec{k}_T \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\vec{q}_t\\cdot\\vec{k}_1 & \\vec{q}_t\\cdot\\vec{k}_2 & \\cdots & \\vec{q}_t\\cdot\\vec{k}_T \\\\\n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "    e_{1,1} & e_{1,2} & \\cdots & e_{1,T} \\vphantom{\\vec{q}_1\\cdot\\vec{k}_T} \\\\\n",
    "    e_{2,1} & e_{2,2} & \\cdots & e_{2,T} \\vphantom{\\vec{q}_2\\cdot\\vec{k}_T} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    e_{t,1} & e_{t,2} & \\cdots & e_{t,T} \\vphantom{\\vec{q}_T\\cdot\\vec{k}_T} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This gives us a $(t,T)$ matrix $E$, where the $(i,j)^\\text{th}$ element in $e$ is the attention score between the $i^\\text{th}$ query vector and the $j^\\text{th}$ key vector.\n",
    "\n",
    "Now, before we take the softmax of our attention scores to compute the attention weights, we will _scale_ each of the attention scores by a factor of $\\frac{1}{\\sqrt{d_k}}$.\n",
    "This is done to push the dot products into a reasonable range for the subsequent softmax - too negative of a dot product will results in a near-zero gradient and thus the model will train less effectively.\n",
    "\n",
    "\\begin{equation}\n",
    "E' = \\frac{1}{\\sqrt{d_k}}E\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "We will now take the softmax of $E'$ to get our attention weights, where the sum in softmax is done over the columns of $E'$,\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\alpha = \\operatorname{softmax}(E') =\n",
    "\\begin{bmatrix}\n",
    "    \\operatorname{softmax}\\begin{pmatrix}e'_{1,1} & e'_{1,2} & \\cdots & e'_{1,T}\\end{pmatrix} \\\\\n",
    "    \\operatorname{softmax}\\begin{pmatrix}e'_{2,1} & e'_{2,2} & \\cdots & e'_{2,T}\\end{pmatrix} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\operatorname{softmax}\\begin{pmatrix}e'_{t,1} & e'_{t,2} & \\cdots & e'_{t,T}\\end{pmatrix}\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "And, as we did before, we will use our attention weights as the coefficients in weighted sum of the value vectors - the rows of $V$ - and we can accomplish this with yet another matrix multiplication:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\begin{bmatrix}\n",
    "    \\xleftarrow{\\hspace{6em}} & \\vec{v}_1 & \\xrightarrow{\\hspace{6em}} \\\\\n",
    "    \\xleftarrow{\\hspace{6em}} & \\vec{v}_2 & \\xrightarrow{\\hspace{6em}} \\\\\n",
    "    & \\vdots &  \\\\\n",
    "    \\xleftarrow{\\hspace{6em}} & \\vec{v}_T & \\xrightarrow{\\hspace{6em}}\n",
    "\\end{bmatrix} \\\\\n",
    "C = \\alpha V =\n",
    "\\begin{bmatrix}\n",
    "    \\alpha_{1,1} & \\alpha_{1,2} & \\cdots & \\alpha_{1,T} \\vphantom{\\sum\\limits_{i=1}^T} \\\\\n",
    "    \\alpha_{2,1} & \\alpha_{2,2} & \\cdots & \\alpha_{2,T} \\vphantom{\\sum\\limits_{i=1}^T} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\alpha_{t,1} & \\alpha_{t,2} & \\cdots & \\alpha_{t,T} \\vphantom{\\sum\\limits_{i=1}^T} \\\\\n",
    "\\end{bmatrix}&\n",
    "\\begin{bmatrix}\n",
    "    \\sum\\limits_{i=1}^T \\alpha_{1,i}(\\vec{v}_i)_1 & \\sum\\limits_{i=1}^T \\alpha_{1,i}(\\vec{v}_i)_2 & \\cdots & \\sum\\limits_{i=1}^T \\alpha_{1,i}(\\vec{v}_i)_{d_v} \\\\\n",
    "    \\sum\\limits_{i=1}^T \\alpha_{2,i}(\\vec{v}_i)_1 & \\sum\\limits_{i=1}^T \\alpha_{2,i}(\\vec{v}_i)_2 & \\cdots & \\sum\\limits_{i=1}^T \\alpha_{2,i}(\\vec{v}_i)_{d_v} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\sum\\limits_{i=1}^T \\alpha_{t,i}(\\vec{v}_i)_1 & \\sum\\limits_{i=1}^T \\alpha_{t,i}(\\vec{v}_i)_2 & \\cdots & \\sum\\limits_{i=1}^T \\alpha_{t,i}(\\vec{v}_i)_{d_v}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "    \\leftarrow & \\vec{c}_1 & \\rightarrow \\vphantom{\\sum\\limits_{i=1}^T} \\\\\n",
    "    \\leftarrow & \\vec{c}_2 & \\rightarrow \\vphantom{\\sum\\limits_{i=1}^T} \\\\\n",
    "    & \\vdots &  \\\\\n",
    "    \\leftarrow & \\vec{c}_t & \\rightarrow \\vphantom{\\sum\\limits_{i=1}^T}\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This will give us a matrix $C$ that has shape-$(t, d_v)$, where each row is a context vector in the same sense as before.\n",
    "Because of we weight the value vectors by attention weights from the keys, it often makes sense to have $K$ and $V$ be the same;\n",
    "if we were to draw a parallel to our previous Seq2Seq model's attention mechanism, both $K$ and $V$ would have been $H^e$, while $Q$ would be $\\vec{h}{}^d_t$.\n",
    "\n",
    "All in all, we can write our attention as the function\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "C = \\operatorname{Attention}(Q, K, V) = \\operatorname{softmax}\\bigg(\\frac{QK^\\intercal}{\\sqrt{d_k}}\\bigg)V.\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a9faec",
   "metadata": {},
   "source": [
    "One significant extension of the idea of attention introduced in the transformer architecture was that of **multihead attention**.\n",
    "Put briefly, we will not just perform a single attention operation, but rather perform $h$ attention operations in parallel.\n",
    "Each of these attention operations is known as an attention head.\n",
    "By performing multiple attention operations at once, the model is able to learn a greater number of patterns in the data, having multiple opportunities to 'attend' to different parts of the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ba5b65",
   "metadata": {},
   "source": [
    "To start this, we will define three sets of learnable weight matrices: $\\big(W_q^{(i)}\\big)_{i=1}^h$, $\\big(W_k^{(i)}\\big)_{i=1}^h$, and $\\big(W_v^{(i)}\\big)_{i=1}^h$.\n",
    "Here, $h$ is the number of attention heads we choose to use.\n",
    "Each of the weight matrices $W_q^{(i)}$, $W_k^{(i)}$, and $W_v^{(i)}$ will have shape $(d, d_k)$, $(d, d_k)$, and $(d, d_v)$, respectively.\n",
    "\n",
    "\n",
    "As their names suggest, these weight matrices will be applied to $Q$, $K$, and $V$ to project the query, key, and value vectors into $d_k$ or $d_v$ dimensional space.\n",
    "While we can pick the values of $d_k$ and $d_v$ to be anything, we will set them to\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "d_k=d_v=\\bigg\\lfloor\\frac{d}{h}\\bigg\\rfloor.\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "This is done so that the computational cost of using a large number of attention heads is roughly the same as using a small number of heads;\n",
    "notice how these choices of $d_k$ and $d_v$ will have the total number of learnable parameters across a given set of $\\big(W_\\square^{(i)}\\big)_{i=1}^h$ matrices remain around $d^2$.\n",
    "\n",
    "Now, we will matrix multiply $Q$, $K$, and $V$ with the each of the corresponding weight matrices.\n",
    "So for the $i^\\text{th}$ attention head, we will have inputs\n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "Q^{(i)}=QW_q^{(i)} \\rightarrow (t,d)\\times(d,d_k) \\rightarrow (t,d_k) \\\\\n",
    "K^{(i)}=KW_k^{(i)} \\rightarrow (T,d)\\times(d,d_k) \\rightarrow (T,d_k) \\\\\n",
    "V^{(i)}=VW_v^{(i)} \\rightarrow (T,d)\\times(d,d_v) \\rightarrow (T,d_v)\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "These learnable weight matrices are what will allow the model to detect different patterns in the data for each attention head, as they can learn different projections for the queries, keys, and values depending on the patterns they pick up on.\n",
    "That being said each row of the new query, key, and value matrices still correspond to a different token from the sequence, and we can interpret them as the queries, keys, and values remapped to a semantically 'richer' embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adbeba4",
   "metadata": {},
   "source": [
    "From here we can apply the scaled dot product attention to each of the $h$ triples $(Q^{(i)}, K^{(i)}, V^{(i)})$, yielding $h$ output matrices of shape $(t,d_v)$\n",
    "\n",
    "\\begin{equation}\n",
    "C^{(i)}=\\operatorname{Attention}(Q^{(i)},K^{(i)},V^{(i)}).\n",
    "\\end{equation}\n",
    "\n",
    "However, since we only want a single matrix to be output from the attention mechanism, we will need to somehow combine the $h$ attention head outputs.\n",
    "We can do this by concatenating the columns of each of the outputs\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "C =\n",
    "\\begin{bmatrix}\n",
    "    \\leftarrow & {\\vec{c}_1}^{(1)} & \\rightarrow & \\leftarrow & {\\vec{c}_1}^{(2)} & \\rightarrow & \\cdots & \\leftarrow & {\\vec{c}_1}^{(h)} & \\rightarrow \\\\\n",
    "    \\leftarrow & {\\vec{c}_2}^{(1)} & \\rightarrow & \\leftarrow & {\\vec{c}_2}^{(2)} & \\rightarrow & \\cdots & \\leftarrow & {\\vec{c}_2}^{(h)} & \\rightarrow \\\\\n",
    "    & \\vdots & & & \\vdots & & \\vdots & & \\vdots & \\\\\n",
    "    \\leftarrow & {\\vec{c}_t}^{(1)} & \\rightarrow & \\leftarrow & {\\vec{c}_t}^{(2)} & \\rightarrow & \\cdots & \\leftarrow & {\\vec{c}_t}^{(h)} & \\rightarrow \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "This gives us the $\\big(t,h\\cdot\\big\\lfloor\\frac{d}{h}\\big\\rfloor\\big)$ matrix $C$, but we will ultimately want the output to be the same as if we had only one attention head.\n",
    "That is, we want our final attention output to be shape $(T,d)$.\n",
    "So what can we do?\n",
    "Well, as is usually the answer, we will matrix multiply $C$ by a $\\big(h\\cdot\\big\\lfloor\\frac{d}{h}\\big\\rfloor,d\\big)$ matrix of learnable weights $W_O$:\n",
    "\n",
    "\\begin{equation}\n",
    "C_\\text{out} = CW_O\n",
    "\\end{equation}\n",
    "\n",
    "The matrix $W_O$ will process the information in each from the attention heads and yield a matrix that summarizes all of the most important information picked up by the heads.\n",
    "Our final output, $C_\\text{out}$, will be a $(t, d)$ matrix - the same shape as the queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134ff3ae",
   "metadata": {},
   "source": [
    "This has been a lot so far, and we're going to have to add a little more.\n",
    "In our implementation, we will need to be careful to perform these operations in parallel not only over the attention heads, but also _over the batches of sequences_ that we will process.\n",
    "So we should take a step back for a moment to look at how all these matrix multiplications will manifest in our code, as things can get unwieldy pretty quickly.\n",
    "\n",
    "The inputs to our multihead attention will consist of a $N$-sized batch of length $t$/$T$ sequences, where each token in the sequence is a $d$ dimensional vector.\n",
    "That is, $Q$, $K$, and $V$ will be tensors with shapes\n",
    "\n",
    "```\n",
    "Q : (t, N, d)\n",
    "K : (T, N, d)\n",
    "V : (T, N, d)\n",
    "```\n",
    "\n",
    "For our multihead attention, we will also need to define the sets of learnable weight matrices.\n",
    "The individual weight matrices $W^{(i)}_q$, $W^{(i)}_k$, and $W^{(i)}_v$ will be shape $(d, d_k)$, $(d, d_k)$, and $(d, d_v)$, respectively.\n",
    "For our implementation, we choose $d_k=d_v=\\big\\lfloor\\frac{d}{h}\\big\\rfloor$.\n",
    "Since we have $h$ of each of these weight matrices - one for each attention head - we can pack them into tensors with shapes\n",
    "\n",
    "```\n",
    "Wq : (h, d, d // h) = (h, d, k)\n",
    "Wk : (h, d, d // h) = (h, d, k)\n",
    "Wv : (h, d, d // h) = (h, d, v)\n",
    "```\n",
    "\n",
    "We also need to define the weight matrix $W_O$ that takes a matrix with the $h$ context vectors from each attention head concatenated together and maps it to a matrix of $d$-dimensional vectors.\n",
    "That is, we need to define a weight matrix for $W_O$ with shape\n",
    "\n",
    "```\n",
    "Wo : (h * (d // h), d) = (h * v, d)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a969a03",
   "metadata": {},
   "source": [
    "Now that we have all the variables we will need, we can think about how to perform the particular operations described in the math above.\n",
    "\n",
    "In the multihead attention, the first step we need to take is to multiply $Q$, $K$, and $V$ by the respective weight matrices.\n",
    "We must do this before computing any attention scores so that, as the model trains, if can tune the weights in such a way that the subsequent attention weights find more meaningful relations in the data.\n",
    "\n",
    "To compute the matrix multiplications in parallel for each attention head, we will want to broadcast the matrix multiplication over the `h` dimension in the weight tensors.\n",
    "Similarly, because we are computing these attention weights in parallel for each sequence in our batch, we will want to broadcast the operation over the `N` dimension of the queries/keys/values.\n",
    "So, for each of the matrix multiplications, we will want to get outputs of shape\n",
    "\n",
    "```\n",
    "Q_i = Q x Wq : (t, N, d) x (h, d, k) -> (t, N, h, k)\n",
    "K_i = K x Wk : (T, N, d) x (h, d, k) -> (T, N, h, k)\n",
    "V_i = V x Wv : (T, N, d) x (h, d, v) -> (T, N, h, v)\n",
    "```\n",
    "\n",
    "Notice how the `d` dimension was 'contracted' over, such that it no longer appears in the output.\n",
    "This indicates that a matrix multiplication was done with respect to the size `d` dimensions.\n",
    "\n",
    "**Note:** for the next two operations detailed going forward, it is can be helpful to 'ignore' the `N` and `h` when thinking of how the matrix multiplications relate to the earlier formula.\n",
    "Since the matrix multiplication will be broadcast over these batch and head dimensions, the matrix multiplications detailed earlier are actually happening between matrices with rows of the $0^\\text{th}$ axis and columns of the $3^\\text{rd}$ axis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7790c008",
   "metadata": {},
   "source": [
    "From here, we go into computing attention weights for each attention head.\n",
    "The first step to computing the attention weights is computing the dot product similarity between the queries and the keys, via a matrix multiplication.\n",
    "As with the previous operations, this matrix multiplication will need to be broadcast over the batch and head dimensions in both `Q_i` and `K_i`.\n",
    "This means we will compute attention scores with a shape of\n",
    "\n",
    "\n",
    "```\n",
    "E = Q_i x K_i : (t, N, h, k) x (T, N, h, k) -> (t, N, h, T)\n",
    "```\n",
    "\n",
    "As before, notice how the `k` dimension from both `Q_i` and `K_i` dissappeared in the output, indicating that dot products were computed between all of the `k`-dimensional vectors in the two tensors.\n",
    "\n",
    "Now we divide `E` by $\\sqrt{d_k}$ and take the softmax over the size `T` dimension to find the attention weights `Œ±`.\n",
    "These two operations will not change the shape of the tensor, and so `Œ±` is a shape `(t, N, h, T)` tensor.\n",
    "\n",
    "The attention weights are then matrix multiplied with `V_i` to compute a weighted sum of the values for each attention head.n\n",
    "This means that we want to contract the `T` dimension in both `Œ±` and `V_i`, giving an output `C` with shape\n",
    "\n",
    "```\n",
    "C_i = Œ± x V_i : (t, N, h, T) x (T, N, h, v) -> (t, N, h, v)\n",
    "```\n",
    "\n",
    "At this point, we've finished computing context vectors for each of the attention heads;\n",
    "`C_i` representes the `t` context vectors of dimensionality `v` for each of the `h` heads and each of the `N` sequences in our batch.\n",
    "However for our final output, we want to 'summarize' the context vectors from each of the attention heads into a single context vector for each of the `t` tokens in the queries.\n",
    "To do this mathematically, we concatenated the columns of each of the $C^{(i)}$ outputs for the attention heads and matrix multiplied the result with $W_O$.\n",
    "In our Python code, to do this concatenation we can simply reshape the tensor `C_i` such that we combine the trailing two axes\n",
    "\n",
    "```\n",
    "C_i -> C :  (t, N, h, v) -> (t, N, h * v)\n",
    "```\n",
    "\n",
    "With the context vectors of each attention head concatenated, we can use `Wo` to project all of the `h * v` dimensional vectors into `d` dimensional vectors.\n",
    "This giving us `C_out`, a tensor with shape\n",
    "\n",
    "```\n",
    "C_out = C x Wo : (t, N, h * v) x (h * v, d) -> (t, N, d)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d44156",
   "metadata": {},
   "source": [
    "The comments in the `MultiheadAttention` class below provide instructions on how to implement the various matrix multiplications.\n",
    "The MyGrad function `einsum` may be of use for performing many of the necessary steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a55d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention:\n",
    "    def __init__(self, dim, n_head=3):\n",
    "        \"\"\" Initializes a multihead attention layer.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dim : int\n",
    "            The dimension of the input sequences, `d`. For one-hot\n",
    "            encodings, this is the size of the vocabulary.\n",
    "        n_head : int\n",
    "            The number of distinct attention heads to use.        \n",
    "        \"\"\"\n",
    "        # Assign `self.Wq` to a tensor drawn from the glorot-normal distribution.\n",
    "        # The tensor should be 3-dimensional, with shape (h, d, d // h)\n",
    "        self.Wq = glorot_normal(n_head, dim, dim // n_head) # <COGLINE>\n",
    "        \n",
    "        # Assign `self.Wk` to a tensor drawn from the glorot-normal distribution.\n",
    "        # The tensor should be 3-dimensional, with shape (h, d, d // h)\n",
    "        self.Wk = glorot_normal(n_head, dim, dim // n_head) # <COGLINE>\n",
    "        \n",
    "        # Assign `self.Wv` to a tensor drawn from the glorot-normal distribution.\n",
    "        # The tensor should be 3-dimensional, with shape (h, d, d // h)\n",
    "        self.Wv = glorot_normal(n_head, dim, dim // n_head) # <COGLINE>\n",
    "        \n",
    "        # Assign `self.Wo` to a MyNN dense class that takes a\n",
    "        # (t, N, h * [d // h]) array to a (t, N, d) array.\n",
    "        # The dense layer should not have a bias term, and the\n",
    "        # weights should be initialized from a glorot_normal distribution.\n",
    "        # <COGINST>\n",
    "        self.Wo = dense(n_head * (dim // n_head), dim, weight_initializer=glorot_normal, bias=False)\n",
    "        # </COGINST>\n",
    "    \n",
    "    def __call__(self, Q, K, V, mask=None):\n",
    "        \"\"\" Performs the full forward pass for the attention layer.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Q : Union[numpy.ndarray, mygrad.Tensor], shape=(t, N, d)\n",
    "            The queries. These are the length-d vectors that we will\n",
    "            re-represent in terms of value-vectors, based on query-key\n",
    "            overlaps.\n",
    "\n",
    "        K : Union[numpy.ndarray, mygrad.Tensor], shape=(T, N, d)\n",
    "            The keys. These are the vectors that have \"information drawn\n",
    "            from them\" in computing attention scores. Each key token will\n",
    "            be compared to each query token to find attention\n",
    "            scores that determine the relevance of the key to the query.\n",
    "\n",
    "        V : Union[numpy.ndarray, mygrad.Tensor], shape=(T, N, d)\n",
    "            The values. These are the vectors that we want to\n",
    "            weight according to the attention scores. Often times\n",
    "            this is the same as x_k.\n",
    "\n",
    "        mask : Optional[numpy.ndarray], dtype=bool, shape=(t, T)\n",
    "            An optional mask to apply on attention weights.\n",
    "            Values that are False are set to -1e14 before softmax\n",
    "            is applied to the attention weights.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        mygrad.Tensor\n",
    "            A weighted sum of the values V. The weights are found by\n",
    "            scoring the relevance of the keys to the queries.\n",
    "        \"\"\"\n",
    "        # Compute Q_i, K_i, and V_i using `self.Wq`, `self.Wk`, and `self.Wv`.\n",
    "        # Each of these should be 4-dimensional tensors,\n",
    "        # with shape (t or T, N, h, d // h).\n",
    "        #\n",
    "        # These tensors contain the Q_i, K_i, and V_i for each sequence in the\n",
    "        # batch and for each attention head we are computing.\n",
    "        # <COGINST>\n",
    "        Q_i = mg.einsum(\"tNd,hdk->tNhk\", Q, self.Wq, optimize=True)\n",
    "        K_i = mg.einsum(\"TNd,hdk->TNhk\", K, self.Wk, optimize=True)\n",
    "        V_i = mg.einsum(\"TNd,hdv->TNhv\", V, self.Wv, optimize=True)\n",
    "        # </COGINST>\n",
    "        \n",
    "        # Below, we will compute our scaled dot product attention with\n",
    "        # the projected queries, keys, and values.\n",
    "        #\n",
    "        # Matrix multiply Q_i and the transpose of K_i\n",
    "        # (only transposing the first and last axes),\n",
    "        # and store the result in the variable `E`.\n",
    "        # The matrix multiplication must be broadcast across\n",
    "        # the batch and attention head dimensions.\n",
    "        # The output should be shape (t, N, h, T).\n",
    "        #\n",
    "        # Then, divide `E` by sqrt(d_k),\n",
    "        # storing the result in the variable `E` again.\n",
    "        E = mg.einsum(\"tNhk,TNhk->tNhT\", Q_i, K_i) / np.sqrt(self.Wk.shape[-1]) # <COGLINE>\n",
    "        \n",
    "        # We will see the use of this optional masking later.\n",
    "        # The mask is a (t,T) array, and any index where the mask is\n",
    "        # False will be zero-ed out in the attention weights.\n",
    "        # The masking is broadcast over the batch and head dimensions.\n",
    "        if mask is not None:\n",
    "            E.transpose(1, 2, 0, 3)[:, :, ~mask.astype(np.bool_)] = -1e14\n",
    "        \n",
    "        # Apply softmax to to masked scores along the last axis,\n",
    "        # and save the attention weights to `self.a_ij`\n",
    "        self.a_ij = mg.nnet.softmax(E, axis=-1) # <COGLINE>\n",
    "        \n",
    "        # Matrix multiply the attention weights with V_i,\n",
    "        # and transpose the output as neccessary such that is is\n",
    "        # shape (T, N, h, d // h)\n",
    "        C = mg.einsum(\"tNhT,TNhv->tNhv\", self.a_ij, V_i) # <COGLINE>\n",
    "        \n",
    "        # We have now finished computing the each of the attention heads\n",
    "        # and now need to combine all of them into a single matrix.\n",
    "        #\n",
    "        # Reshape the (t, N, h, d // h) tensor \n",
    "        # into a (t, N, h * [d // h]) tensor.\n",
    "        # This is equivalent to concatenating the columns of the\n",
    "        # individual attention head outputs.\n",
    "        #\n",
    "        # Then apply the final dense layer Wo to return\n",
    "        # a shape (t, N, d) tensor\n",
    "        # <COGINST>\n",
    "        out = C.reshape(*C.shape[:2], -1)\n",
    "        return self.Wo(out)\n",
    "        # </COGINST>\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        \"\"\" A convenience function for getting all the parameters of our model. \"\"\"\n",
    "        return (self.Wq, self.Wk, self.Wv) + self.Wo.parameters # <COGLINE>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e30c21",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38859fba",
   "metadata": {},
   "source": [
    "Much like our Seq2Seq model, a transformer has both an encoder and decoder.\n",
    "The encoder is once again responsible for processing the source text, using **self-attention** to distill important relationships and patterns in the source text;\n",
    "the encoder is responsible for finding out which tokens in the source text are most closely related to one another.\n",
    "That is, we will use the batch of source sequences as all of the queries, keys, and values in a multihead attention.\n",
    "\n",
    "Before we implement our encoder though, there is one small detour we need to make."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca65ea99",
   "metadata": {},
   "source": [
    "### Two Modern Deep Learning Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65b8663",
   "metadata": {},
   "source": [
    "We come now to two techniques that have risen rapidly in deep learning - residual connections and layer normalization.\n",
    "\n",
    "Residual connections (also sometimes called skip connections) were introduced to enable 'deeper' models to be constructed.\n",
    "They aim to help the gradient 'flow' more easily during backpropagation, by establishing a direct connection between the beginning and end of a layer in a model.\n",
    "For any given layer, this is done by summing the input and output of the layer.\n",
    "Thus during backpropagation, the gradient for the input of the layer will consist directly of both the incoming gradient to the layer, as well as the gradient from the layer itself.\n",
    "This means that, if the gradient from the layer were to be very small, it would not as substantially impact the gradient of the input, which also sees directly the layer's incoming gradient.\n",
    "\n",
    "Layer normalization is an idea similar to the more commonly seen batch normalization, though it sees much more use in NLP where batch normalization can be tricky with variable length sequences.\n",
    "However, the core idea behind layer normalization is to reduce inter-layer variablility that can cause early layers to have outsized impacts on subsequent layers.\n",
    "Such a dynamic can lead to exploding or vanishing gradients, neither of which is desirable when training a model.\n",
    "\n",
    "Often times in deep learning research, residual connections and layer/batch normalization will be deployed as the general rule of thumb is that they help the training of a model by making the loss landscape easier to traverse.\n",
    "The original transformer architecture employs these tricks, and so we will as well.\n",
    "\n",
    "Below is a class `ResidualConnectionAndLayerNorm` that performs a residual connection followed by a layer normalization.\n",
    "This class will be utilized after each layer in our transformer.\n",
    "Calling it requires two arguments - `x_old` and `x_new` - which represent the input and output to the preceding transformer layer.\n",
    "Briefly study the code below, to get a sense for what this class will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de6e4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnectionAndLayerNorm:\n",
    "    def __init__(self, dim):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        dim : int\n",
    "        \"\"\"\n",
    "        self.g = mg.ones(dim)\n",
    "        self.b = mg.zeros(dim)\n",
    "        \n",
    "    def __call__(self, x_old, x_new):\n",
    "        \"\"\"\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x_old : Union[numpy.ndarray, mygrad.Tensor], shape-(..., d)\n",
    "        \n",
    "        x_new : mygrad.Tensor, shape-(..., d)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        mygrad.Tensor, shape-(..., d)\n",
    "        \"\"\"\n",
    "        x = x_old + x_new\n",
    "        \n",
    "        # normalize over the trailing axis and apply\n",
    "        # learnable scale and shift terms\n",
    "        mu = x.mean(axis=-1, keepdims=True)\n",
    "        sigma = x.std(axis=-1, keepdims=True)\n",
    "        return self.g * (x - mu) / (sigma + 1e-6) + self.b\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        \"\"\" A convenience function for getting all the parameters of our model. \"\"\"\n",
    "        return (self.g, self.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64401ad8",
   "metadata": {},
   "source": [
    "Now, back to the interesting parts of the transformer's encoder.\n",
    "Our transformer encoder will consist of two layers: a multihead self-attention layer, and a simple feedforward layer.\n",
    "The feedforward layer will act as a sort of 'interpreter' for the context vectors yielded by the self-attention, distilling any meaningful information into a richer, abstract representation.\n",
    "After both layers, we will apply a residual connection and layer normalization.\n",
    "All in all, the transformer encoder will be structured as\n",
    "\n",
    "\\begin{align}\n",
    "X_1 &= \\operatorname{MultiHeadAttention}(\\mathrm{src},\\, \\mathrm{src},\\, \\mathrm{src}) && \\text{Self-attention layer} \\\\\n",
    "X_2 &= \\operatorname{LayerNorm}(\\mathrm{src} + X_1) && \\text{Residual connection and layer normalization} \\\\\n",
    "X_3 &= \\operatorname{ReLU}\\!\\big(X_2W_1 + \\vec{b}_1\\big)W_2 + \\vec{b}_2 && \\text{Feedforward layer} \\\\\n",
    "\\mathrm{Enc}_\\text{out} &= \\operatorname{LayerNorm}(X_2 + X_3) && \\text{Residual connection and layer normalization}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad786f62",
   "metadata": {},
   "source": [
    "Below are two classes, `FeedForward` and `TransformerEncoder`.\n",
    "The `FeedForward` network will be a simple dense neural network using a ReLU activation function.\n",
    "Complete the `FeedForward` class using MyNN's `dense`, then fill out the `TransformerEncoder` class according to the equations above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20f377d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward:\n",
    "    def __init__(self, in_dim, h_dim, out_dim):\n",
    "        \"\"\" Initializes a simple feedforward layer.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        in_dim : int\n",
    "            The dimensionality of the input vectors.\n",
    "        \n",
    "        h_dim : int\n",
    "            The number of neurons in the hidden layer.\n",
    "        \n",
    "        out_dim : int\n",
    "            The dimensionality of the output vectors.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Initialize MyNN `dense` layers\n",
    "        # <COGINST>\n",
    "        self.W1 = dense(in_dim, h_dim, weight_initializer=glorot_normal)\n",
    "        self.W2 = dense(h_dim, out_dim, weight_initializer=glorot_normal)\n",
    "        # </COGINST>\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : Union[numpy.ndarray, mygrad.Tensor]\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        mygrad.Tensor\n",
    "        \n",
    "        \"\"\"\n",
    "        # Use a ReLU activation between the two dense layers\n",
    "        # <COGINST>\n",
    "        out = self.W1(x)\n",
    "        out = mg.nnet.relu(out)\n",
    "        return self.W2(out)\n",
    "        # </COGINST>\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        \"\"\" A convenience function for getting all the parameters of our model. \"\"\"\n",
    "        return self.W1.parameters + self.W2.parameters # <COGLINE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b896bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder:\n",
    "    def __init__(self, dim, h_dim, n_head=3):\n",
    "        \"\"\" Initializes a TransformerEncoder object.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dim : int\n",
    "            The dimensionality of both the input and output embeddings (`d`).\n",
    "            These are the same to accomodate the residual connections.\n",
    "        \n",
    "        h_dim : int\n",
    "            The number of neurons in the hidden layer of the feed forward\n",
    "            layer.\n",
    "        \n",
    "        n_head : int\n",
    "            The number of attention heads to apply on the self-attention layer.\n",
    "        \"\"\"\n",
    "        # <COGINST>\n",
    "        self.self_attn = MultiheadAttention(dim, n_head)\n",
    "        self.ff = FeedForward(dim, h_dim, dim)\n",
    "        self.norm1 = ResidualConnectionAndLayerNorm(dim)\n",
    "        self.norm2 = ResidualConnectionAndLayerNorm(dim)\n",
    "        # </COGINST>\n",
    "    \n",
    "    def __call__(self, src):\n",
    "        \"\"\" Performs a full forward pass of the Transformer encoder.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        src : Union[numpy.ndarray, mygrad.Tensor], shape=(T, N, d)\n",
    "            The batch of source sequences\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        mygrad.Tensor, shape=(t, N, d)\n",
    "        \"\"\"\n",
    "        # Apply the self-attention layer to the source sequence.\n",
    "        # That is, use the source sequence as the queries, keys,\n",
    "        # and targets for the attention layer.\n",
    "        # This will have the model learn intra-sequence relations\n",
    "        # for the source sequence.\n",
    "        X1 = self.self_attn(src, src, src) # <COGLINE>\n",
    "        \n",
    "        # Apply the first residual + layernorm\n",
    "        X2 = self.norm1(src, X1) # <COGLINE>\n",
    "        \n",
    "        # Apply the feedforward layer\n",
    "        X3 = self.ff(X2) # <COGLINE>\n",
    "        \n",
    "        # Apply the second residual + layernorm\n",
    "        return self.norm2(X2, X3) # <COGLINE>\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        \"\"\" A convenience function for getting all the parameters of our model. \"\"\"\n",
    "        # <COGINST>\n",
    "        return self.self_attn.parameters + self.ff.parameters + self.norm1.parameters + self.norm2.parameters\n",
    "        # </COGINST>        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115ef545",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a98e3b7",
   "metadata": {},
   "source": [
    "The transformer's decoder is a bit trickier than the encoder, because we are now going to also process the _target_ sequence.\n",
    "This might seem strange;\n",
    "wouldn't using the target sequence in training the model cause it to simply memorize the answers?\n",
    "There is, after all, a reason that we previously haven't used the target value as part of the model's inputs.\n",
    "\n",
    "Well for some NLP problems, it can make sense to use at least some of the target sequence as an input when training.\n",
    "Since language is inherently sequential and past tokens will typically inform future tokens, then when predicting a target token it can be reasonable to give the model access to the prior predicted tokens.\n",
    "We saw this idea in the Seq2Seq model, where the decoder output would determine the decoder's next input.\n",
    "Since the transformer does not process data sequentially, and instead relies on attention layers to pick up on relationships between tokens, we will pass the full target sequence in and process it as needed.\n",
    "\n",
    "In our transformer, the target sequence will be first passed through a self-attention layer.\n",
    "Of course, we can't let the attention weights have access to the entirety of the target sequence - then it would just memorize the target by looking at the future tokens!\n",
    "But for any given token, we _can_ compute attention weights for prior tokens from the target sequence.\n",
    "By doing some clever masking of the target sequence, we can 'hide' the future tokens that the model shouldn't be seeing.\n",
    "Let's take a look at a simplified example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b846531",
   "metadata": {},
   "source": [
    "Say that we have computed a self-attention weight of\n",
    "\n",
    "```python\n",
    "a_ij = np.array([[0.1, 0.7, 0.2],\n",
    "                 [0.4, 0.2, 0.4],\n",
    "                 [0.1, 0.8, 0.1]])\n",
    "```\n",
    "\n",
    "for our target sequence.\n",
    "This would mean that, when looking at the first token in the target sequence, the model places a very high weight on the second token.\n",
    "\n",
    "But when applying the transformer to new data, we won't necessarily have the target sequence available.\n",
    "So we'll have to generate it one token at a time, meaning we don't know the second token until after we have processed the first token!\n",
    "Thus when the model weights subsequent tokens in the target sequence, it is practically looking into the future to access information it shouldn't have.\n",
    "\n",
    "To get around this, we can 'mask' our attention weights to only allow information to be used it it comes from either the current token or any previous token.\n",
    "In other words, the attention weights for the first token can only be non-zero for the first token;\n",
    "for the second token, the attention weights can be non-zero for either the first or second token.\n",
    "\n",
    "We do this by setting the upper-right corner of the attention weights to a very large negative value (`-1e14`) before applying the softmax across the rows.\n",
    "If the attention scores, before softmax was applied, of the earlier `a_ij` weights were\n",
    "\n",
    "```python\n",
    "C = np.array([[-2.30258509, -0.35667494, -1.60943791],\n",
    "              [-0.91629073, -1.60943791, -0.91629073],\n",
    "              [-2.30258509, -0.22314355, -2.30258509]])\n",
    "```\n",
    "\n",
    "then we would mask the scores as:\n",
    "\n",
    "```python\n",
    "C = np.array([[-2.30258509e+00, -1.00000000e+14, -1.00000000e+14],\n",
    "              [-9.16290730e-01, -1.60943791e+00, -1.00000000e+14],\n",
    "              [-2.30258509e+00, -2.23143550e-01, -2.30258509e+00]])\n",
    "```\n",
    "\n",
    "Since softmax exponentiates the values in the array, these very negative values will become `0`, while the other non-zero values will still sum to `1`.\n",
    "Computing the masked self-attention weights would now yield:\n",
    "\n",
    "```python\n",
    "array([[1.        , 0.        , 0.        ],\n",
    "       [0.66666667, 0.33333333, 0.        ],\n",
    "       [0.1       , 0.8       , 0.1       ]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634dfba9",
   "metadata": {},
   "source": [
    "This masking mechanism was pre-built into the `MultiheadAttention` class.\n",
    "It will take in a boolean array, and will set any position where the mask is `False` to `-1e14`.\n",
    "So for a mask and attention scores of\n",
    "\n",
    "```python\n",
    "# (3, 3)\n",
    "mask = np.array([[ True, False,  True],\n",
    "                 [False,  True, False],\n",
    "                 [ True, False, False]])\n",
    "\n",
    "# (1, 1, 3, 3)\n",
    "C = np.array([[[[10,  2, -3],\n",
    "                [ 5, -6,  4],\n",
    "                [ 1, -5, 12]]]])\n",
    "```\n",
    "\n",
    "the masking would lead to a `C` of\n",
    "\n",
    "```\n",
    "array([[[[              10, -100000000000000,               -3],\n",
    "         [-100000000000000,               -6, -100000000000000],\n",
    "         [               1, -100000000000000, -100000000000000]]]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9172daec",
   "metadata": {},
   "source": [
    "Aside from this masking, the decoder will largely resemble the encoder.\n",
    "Of course, we will want to incorporate the output of the encoder as an input to the decoder.\n",
    "So we will use _two_ attention layers here: one as a masked self-attention layer on the target sequence, and one as an attention layer between the encoder output and the target self-attention output.\n",
    "The final tensor will be passed through a feedforward network, as in the encoder, as a means of 'interpreting' the results of the attention layers.\n",
    "Put more succinctly, our decoder will take the form of\n",
    "\n",
    "\\begin{align}\n",
    "X_1 &= \\operatorname{MaskedMultiHeadAttention}(\\mathrm{tgt},\\, \\mathrm{tgt},\\, \\mathrm{tgt}) && \\text{Masked self-attention layer} \\\\\n",
    "X_2 &= \\operatorname{LayerNorm}(\\mathrm{tgt} + X_1) && \\text{Residual connection and layer normalization} \\\\\n",
    "X_3 &= \\operatorname{MultiHeadAttention}(X_2,\\, \\mathrm{Enc}_\\text{out},\\, \\mathrm{Enc}_\\text{out}) && \\text{Encoder-decoder attention layer} \\\\\n",
    "X_4 &= \\operatorname{LayerNorm}(X_2 + X_3) && \\text{Residual connection and layer normalization} \\\\\n",
    "X_5 &= \\operatorname{ReLU}\\!\\big(X_4W_1 + \\vec{b}_1\\big)W_2 + \\vec{b}_2 && \\text{Feedforward layer} \\\\\n",
    "\\mathrm{Dec}_\\text{out} &= \\operatorname{LayerNorm}(X_4 + X_5) && \\text{Residual connection and layer normalization}\n",
    "\\end{align}\n",
    "\n",
    "Complete the `TransformerDecoder` class below, following the outline above.\n",
    "Make sure to only mask the self-attention layer for the target sequence, creating and passing in the appropriate mask array when the layer called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4730bb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder:\n",
    "    def __init__(self, dim, h_dim, n_head=3):\n",
    "        \"\"\" Initializes a TransformerDecoder object.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dim : int\n",
    "            The dimensionality of both the input and output embeddings (`d`).\n",
    "            These are the same to accomodate the residual connections.\n",
    "        \n",
    "        h_dim : int\n",
    "            The number of neurons in the hidden layer of the feed forward\n",
    "            layer.\n",
    "        \n",
    "        n_head : int\n",
    "            The number of attention heads to apply on both the\n",
    "            self-attention layer and the encoder-decoder attention layer.\n",
    "        \"\"\"\n",
    "        # Instantiate two attention layers:\n",
    "        # one as a self-attention on the target sequence\n",
    "        # and the other as an attention between the encoder output and the target sequence\n",
    "        # <COGINST>\n",
    "        self.self_attn = MultiheadAttention(dim, n_head)\n",
    "        self.enc_dec_attn = MultiheadAttention(dim, n_head)\n",
    "        # </COGINST>\n",
    "        \n",
    "        # Instantiate a feedforward layer and 3 residual+layernorms\n",
    "        # <COGINST>\n",
    "        self.ff = FeedForward(dim, h_dim, dim)\n",
    "        self.norm1 = ResidualConnectionAndLayerNorm(dim)\n",
    "        self.norm2 = ResidualConnectionAndLayerNorm(dim)\n",
    "        self.norm3 = ResidualConnectionAndLayerNorm(dim)\n",
    "        # </COGINST>\n",
    "    \n",
    "    def __call__(self, enc_out, tgt):\n",
    "        \"\"\"\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        enc_out : mygrad.Tensor, shape=(T, N, d)\n",
    "            The output of the transformer's encoder.\n",
    "        \n",
    "        tgt : Union[numpy.array, mygrad.Tensor], shape=(t, N, d)\n",
    "            The batch target sequence for translation\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        mygrad.Tensor, shape=(T, N, d)\n",
    "        \"\"\"       \n",
    "        # Create a mask for the target sequence self-attention layer\n",
    "        # that will result in the upper right corner of the attention\n",
    "        # scores being set to -1e14.\n",
    "        # The mask should be a (t, t) boolean NumPy array.\n",
    "        # <COGINST>\n",
    "        t = tgt.shape[0]\n",
    "        mask = np.tril(np.ones((t, t))).astype(np.bool_)\n",
    "        # </COGINST>\n",
    "        \n",
    "        # apply the masked self-attention layer to the target sequence\n",
    "        X1 = self.self_attn(tgt, tgt, tgt, mask=mask) # <COGLINE>\n",
    "        \n",
    "        # apply the first layernorm\n",
    "        X2 = self.norm1(tgt, X1) # <COGLINE>\n",
    "        \n",
    "        # apply the encoder-decoder attention layer\n",
    "        # the 'queries' should be the output of the target self-attention\n",
    "        # and the 'keys' and 'values' should be the output of the encoder\n",
    "        X3 = self.enc_dec_attn(X2, enc_out, enc_out) # <COGLINE>\n",
    "        \n",
    "        # apply the second layernorm\n",
    "        X4 = self.norm2(X2, X3) # <COGLINE>\n",
    "        \n",
    "        # apply the feedforward network\n",
    "        X5 = self.ff(X4) # <COGLINE>\n",
    "        \n",
    "        # apply the final layernorm\n",
    "        return self.norm3(X4, X5) # <COGLINE>\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        \"\"\" A convenience function for getting all the parameters of our model. \"\"\"\n",
    "        # <COGINST>\n",
    "        return (self.self_attn.parameters + self.enc_dec_attn.parameters + self.ff.parameters\n",
    "                + self.norm1.parameters + self.norm2.parameters + self.norm3.parameters)\n",
    "        # </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16099c5",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb6b8a1",
   "metadata": {},
   "source": [
    "We're almost ready to put together our transformer, but there's one last, very crucial, piece we're still missing.\n",
    "When we were working with RNNs, the architecture of the model itself - the fact that the RNN processed tokens sequentially - allowed the model to leverage positional information about the sequence.\n",
    "But so far, our transformer has no such way to learn positional information from the inputs;\n",
    "the transformer in its current state would be like a bag-of-words, losing all information about the locations of tokens in a sequence.\n",
    "\n",
    "To get around this, we will put our source and target sequences through a **positional encoding**.\n",
    "The positional encoding will yield a unique vector for each token in the sequence that corresponds only to its position in the sequence, and the dimensionality of the token's vector representation.\n",
    "Since the positional encoding only depends on the shapes of the input sequences, it can be thought of as a constant 'fingerprint' that we will add onto the sequences before passing them through the encoder and decoder;\n",
    "this allows us to provide positional information about tokens in the sequence to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12acb332",
   "metadata": {},
   "source": [
    "A class `PositonalEncoder` is included below.\n",
    "There are two variables required: `dim_input` and `seq_len`.\n",
    "`dim_input` corresponds to the dimension of the token embeddings, while `seq_len` is, well, the length of the sequence.\n",
    "The output is a shape `(seq_len, 1, dim_input)` array, where the singleton dimension is included so that the output is broadcast compatable with batches of sequences.\n",
    "\n",
    "Run the following cells to visualize the output of the positional encoder.\n",
    "Along the `dim_input` dimension are sinusoids of decreasing frequencies;\n",
    "notice how this makes it such that the `seq_len` dimension has a unique vector corresponding to each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a30631",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder:\n",
    "    def __init__(self, dim):\n",
    "        \"\"\" Initializes a positional encoder for a transformer model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dim : int\n",
    "            The dimensionality `d` of the embeddings of tokens in the\n",
    "            source and target sequences. For one-hot encodings, this\n",
    "            is just the size of the vocabulary.\n",
    "        \"\"\"\n",
    "        self.dim = dim\n",
    "        \n",
    "    def __call__(self, seq_len):\n",
    "        \"\"\" Computes a positional encoding 'fingerprint' for each token in a sequence.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        seq_len : int\n",
    "            The length of the sequence being encoded (`T`).\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray, shape: (T, 1, d)\n",
    "            Positional encodings for a sequence of length T with embeddings of\n",
    "            size d, made to be broadcastable with a batch of sequences.\n",
    "        \"\"\"\n",
    "        den = 10000 ** (-np.arange(0, self.dim, 2) / self.dim)\n",
    "        pos = np.arange(0, seq_len).reshape(seq_len, 1)\n",
    "        \n",
    "        out = np.zeros((seq_len, self.dim))\n",
    "        out[:, 0::2] = np.sin(pos * den)\n",
    "        out[:, 1::2] = np.cos(pos * den[:self.dim // 2])\n",
    "        \n",
    "        return out[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29c7e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 50 # token embedding size \n",
    "T = 20 # sequence length\n",
    "\n",
    "pos = PositionalEncoder(C)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# squeeze here to remove the batch dimension for plotting\n",
    "ax.imshow(pos(T).squeeze())\n",
    "\n",
    "ax.set_xlabel(\"Dimension of Token Embeddings\")\n",
    "ax.set_ylabel(\"Positional Encoding for\\nTokens in Sequence\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9401ae",
   "metadata": {},
   "source": [
    "Applying this positional encoding to the source and target sequences will simply be summing the sequence and the positional encoding.\n",
    "Since the output of the positional encoding has a shape of `(T, 1, C)`, the addition will be broadcast over the batch and every sequence in the batch will have the same positional encoding applied to it.\n",
    "This invariance in the positional encoding across sequences is what allows the model to learn it these encodings represent locations of tokens in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f576dd",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7003be",
   "metadata": {},
   "source": [
    "Now that we have all the necessary pieces, its finally time to roll out our full transformer model.\n",
    "Complete the `Transformer` class below, carefully reading the docstrings and following the instructions in the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b730599",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    def __init__(self, dim, h_dim, n_enc, n_dec, n_head=3):\n",
    "        \"\"\" Instantiates a full transformer model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dim : int\n",
    "            The dimensionality of tokens in the source and target sequences (`d`).\n",
    "        \n",
    "        h_dim : int\n",
    "            The number of neurons in each feed forward layer.\n",
    "        \n",
    "        n_enc : int\n",
    "            The number of encoder layers to apply sequentially.\n",
    "        \n",
    "        n_dec : int\n",
    "            The number of decoder layers to apply sequentially.\n",
    "        \n",
    "        n_head : int\n",
    "            The number of attention heads to use in each multi-head attention.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Instantiate a `PositionalEncoder` with\n",
    "        # the appropriate embedding dimension\n",
    "        self.pos_enc = PositionalEncoder(dim) # <COGLINE>\n",
    "        \n",
    "        # Create two lists, one of `TransformerEncoder`s and one of `TransformerDecoder`s,\n",
    "        # of lengths `n_enc` and `n_dec`. We will iterate over these lists to apply\n",
    "        # multiple encoder or decoder layers in `__call__`.\n",
    "        # <COGINST>\n",
    "        self.enc_layers = [TransformerEncoder(dim, h_dim, n_head) for _ in range(n_enc)]\n",
    "        self.dec_layers = [TransformerDecoder(dim, h_dim, n_head) for _ in range(n_dec)]\n",
    "        # </COGINST>\n",
    "        \n",
    "        # Instantiate a MyNN `dense` layer that will 'interpret' the\n",
    "        # final decoder output before the softmax probabilities are\n",
    "        # computed for each token.\n",
    "        #\n",
    "        # The dense layer should take `d` dimensional vectors to\n",
    "        # `d` dimensional vectors.\n",
    "        # Use glorot_normal as the weight initializer.\n",
    "        self.out_dense = dense(dim, dim, weight_initializer=glorot_normal) # <COGLINE>\n",
    "    \n",
    "    def __call__(self, src, tgt):\n",
    "        \"\"\" Performs a full transformer forward pass.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        src : Union[numpy.ndarray, mygrad.Tensor], shape=(T, N, d)\n",
    "            The batch of source sequences.\n",
    "        \n",
    "        tgt : Union[numpy.ndarray, mygrad.Tensor], shape=(t, N, d)\n",
    "            The batch of target sequences.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        mygrad.Tensor, shape=(t, N, d)\n",
    "        \"\"\"\n",
    "        # Compute the positional encoding for the source sequence.\n",
    "        # and sum it with the source sequence.\n",
    "        x = src + self.pos_enc(src.shape[0]) # <COGLINE>\n",
    "        \n",
    "        # Iterate over the list of encoder layers.\n",
    "        # Use the output of each as the input to the next layer.\n",
    "        # Make sure to save the final encoder layers' output\n",
    "        # for use in the decoder.\n",
    "        # <COGINST>\n",
    "        for enc in self.enc_layers:\n",
    "            x = enc(x)\n",
    "        enc_out = x\n",
    "        # </COGINST>\n",
    "        \n",
    "        # Compute the positional encoding for the target sequence.\n",
    "        # and sum it with the target sequence.\n",
    "        x = tgt + self.pos_enc(tgt.shape[0]) # <COGLINE>\n",
    "        \n",
    "        # Iterate over the list of decoder layers.\n",
    "        # Use the output of each as the input to the next layer.\n",
    "        # <COGINST>\n",
    "        for dec in self.dec_layers:\n",
    "            x = dec(enc_out, x)\n",
    "        # </COGINST>\n",
    "        \n",
    "        # Apply the final dense layer and return the output\n",
    "        return self.out_dense(x) # <COGLINE>\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        \"\"\" A convenience function for getting all the parameters of our model. \"\"\"\n",
    "        # <COGINST>\n",
    "        params = sum((enc.parameters for enc in self.enc_layers), ())\n",
    "        params += sum((dec.parameters for dec in self.dec_layers), ())\n",
    "        \n",
    "        return params + self.out_dense.parameters\n",
    "        # </COGINST>\n",
    "    \n",
    "    def save_parameters(self, filename):\n",
    "        with open(filename, \"wb\") as f:\n",
    "            np.savez(f, *(t.data for t in model.parameters))\n",
    "    \n",
    "    def load_param_weights(self, filename):\n",
    "        with np.load(filename) as f:\n",
    "            for param, arr in zip(self.parameters, f.values()):\n",
    "                param.data = arr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4031ee9",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc41f4a",
   "metadata": {},
   "source": [
    "Instatiate a nogging plot to track the loss and accuracy of the transformer as it trains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b61d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from noggin import create_plot\n",
    "plotter, fig, ax = create_plot([\"loss\", \"accuracy\"]) # <COGLINE>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4235e8",
   "metadata": {},
   "source": [
    "Instatiate the `Transformer` and an `Adam` optimizer.\n",
    "\n",
    "Using the default parameters for the optimizer is good.\n",
    "Starting with a feedforward hidden dimension of `30`, `2` encoder layers, `2` decoder layers, and `4` attention heads is a reasonable starting place for the model;\n",
    "after completing the notebook, try adjusting these values and seeing how it impacts the performance and the learned attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0b250e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <COGINST>\n",
    "d = train_src_one_hot.shape[-1]\n",
    "\n",
    "model = Transformer(d, 30, 2, 2, n_head=4)\n",
    "optimizer = Adam(model.parameters)\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5ac107",
   "metadata": {},
   "source": [
    "Write your training loop below.\n",
    "Remember, the $0^\\text{th}$ dimension of the data corresponds to the sequence length, while the $1^\\text{st}$ dimension corresponds to the batch of sequences.\n",
    "\n",
    "Use a `softmax_crossentropy` loss, and remember the necessary input shapes for the function.\n",
    "\n",
    "Start by training the model for a single epoch, with a batch size of $10$;\n",
    "feel free to revist these values as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a72b990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <COGINST>\n",
    "mg.turn_memory_guarding_off()\n",
    "\n",
    "batch_size = 10\n",
    "num_epochs = 1\n",
    "dataset_size = train_src_one_hot.shape[1]\n",
    "\n",
    "for epoch_cnt in range(num_epochs):\n",
    "    idxs = np.arange(dataset_size)\n",
    "    np.random.shuffle(idxs)\n",
    "\n",
    "    for batch_cnt in range(0, dataset_size // batch_size):\n",
    "        # adjusting the lr seems to help speed up\n",
    "        # convergence a bit but it isn't really necessary\n",
    "        if batch_cnt == 7500:\n",
    "            optimizer = Adam(model.parameters, learning_rate=1e-5)\n",
    "        \n",
    "        batch_indices = idxs[batch_cnt * batch_size : (batch_cnt + 1) * batch_size]\n",
    "\n",
    "        # since the data are shape (T, N, ...), we need to\n",
    "        # index into the 1st dimension with `batch_indices`\n",
    "        src_batch = train_src_one_hot[:, batch_indices]\n",
    "        tgt_batch = train_tgt_one_hot[:, batch_indices]\n",
    "        truth_batch = train_truth_inds[:, batch_indices]\n",
    "\n",
    "        pred = model(src_batch, tgt_batch)\n",
    "\n",
    "        loss = softmax_crossentropy(pred.reshape(-1, pred.shape[-1]), truth_batch.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        acc = np.mean(np.all(np.argmax(pred, axis=-1) == truth_batch, axis=0))\n",
    "\n",
    "        plotter.set_train_batch({\"loss\":loss.item(), \"accuracy\":acc}, batch_size=batch_size)\n",
    "\n",
    "        if batch_cnt % 1000 == 0 and batch_cnt > 0:\n",
    "            plotter.set_train_epoch()\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e2df31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "filename = \"transformer_weights.npz\"\n",
    "# model.save_parameters(filename)\n",
    "loaded_model = model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af68139d",
   "metadata": {},
   "source": [
    "## Cracking the Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8e4659",
   "metadata": {},
   "source": [
    "Now that we have trained our transformer up to a pretty good accuracy, what do we do when we get a new encypted text?\n",
    "Well, we can use our model to decode it!\n",
    "\n",
    "Of course, unlike during training, we no longer have access to the full target text to pass in as an input.\n",
    "So we will have to iteratively generate the target sequence, in much the same way as our previous Seq2Seq model did.\n",
    "\n",
    "Complete the cell below that will perform greedy decoding.\n",
    "As in the Seq2Seq model, we will generate the target sequence one token at a time, 'greedily' picking the class with the largest predicted score to be the next token in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ca1a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = Transformer(d, 30, 2, 2, n_head=4)\n",
    "loaded_model.load_param_weights(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9bfcdf",
   "metadata": {},
   "source": [
    "Let's make a function that enables us to decrypt a message using our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bda32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You don't need to complete any code here\n",
    "\n",
    "def one_hot_encoding(text) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    adds both a start and end token\n",
    "    \"\"\"\n",
    "    seq_len = len(text)\n",
    "    # +2 to seq_len to add in start/end tokens, +2 to dimension to account for new start/end tokens\n",
    "    # start = [... 0 1 0], end = [... 0 0 1]\n",
    "    out = np.zeros((seq_len+2, len(ALPHA)+2), dtype=np.float32)\n",
    "    \n",
    "    inds = [IND_LOOKUP[x] for x in text]\n",
    "    out[range(1, seq_len+1), inds] = 1\n",
    "    out[0, -2] = 1\n",
    "    out[-1, -1] = 1\n",
    "    \n",
    "    return out\n",
    "\n",
    "@mg.no_autodiff\n",
    "def decrypt_with_model(encrypted_message: str, model: Transformer):\n",
    "    \"\"\"Given an encrypted message, returns the model's decrypted (decoded) message\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    encrypted_message : str\n",
    "        A message encrypted with via Vigen√®re cipher with the key 'clap'\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    decrypted : str\n",
    "        The model's decoded output -- a decrypted version of the string\n",
    "    \n",
    "    model: Transformer\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> decrypt_with_model('jpl qkwctwdojzwocbeo zu')\n",
    "    hello world how are you\n",
    "    \"\"\"\n",
    "    assert len(encrypted_message) < 50, \\\n",
    "    \"Remove at your own peril -- this can be slow to compute because our decoder method is 'brute force'\"\n",
    "\n",
    "    # Rather than simply use our model's forward pass T-1 times to generate our\n",
    "    # prediction, we will just call the encoder pass once and then call the\n",
    "    # decoder T-1 times to generate the decoded sequence. This will save us\n",
    "    # from calling our encoder redundantly\n",
    "    \n",
    "    # A shape-(T, 1, 29) one-hot version of our encrypted input\n",
    "    # where 29 is: 26 letters + space + start & stop special characters\n",
    "    one_hot_encrypted_msg = np.concatenate([one_hot_encoding(encrypted_message)[1:, None]], axis=1)\n",
    "    \n",
    "    # We only need to process our encoder's outputs once on the full input sequence\n",
    "    x = one_hot_encrypted_msg + model.pos_enc(one_hot_encrypted_msg.shape[0])\n",
    "    for enc in model.enc_layers:\n",
    "        x = enc(x)\n",
    "    enc_out = x\n",
    "    \n",
    "    # Now we will run our decoder T-1 times (T includes an <END> character)\n",
    "    # We will pass in the [1, (i - 1)] decorder outputs to produce the ith decoder output\n",
    "\n",
    "    # Create a shape-(T, 1, 29) tensor of zeros, which will store each of our decoder's\n",
    "    # outputs as they are generated\n",
    "    T = len(one_hot_encrypted_msg)\n",
    "    decoded_one_hot = np.zeros_like(one_hot_encrypted_msg)\n",
    "    decoded_one_hot[0, :, -2] = 1 # start all sequence with a start token\n",
    "\n",
    "    for i in range(T - 1):\n",
    "        # Pass in decoder outputs [1, (i - 1)] to produce the ith decoder output\n",
    "        tgt = decoded_one_hot[:i+1]\n",
    "        x = tgt + model.pos_enc(tgt.shape[0]) # <COGLINE>\n",
    "\n",
    "        for dec in model.dec_layers:\n",
    "            x = dec(enc_out, x)\n",
    "\n",
    "        pred = model.out_dense(x)\n",
    "\n",
    "        ind = pred[-1].argmax(axis=-1)\n",
    "        # store ith prediction as one-hot\n",
    "        decoded_one_hot[i+1, range(len(ind)), ind] = 1  \n",
    "    \n",
    "    # We have produced our model's decoded outputs, which are\n",
    "    # stored in a one-hot representation. Now we convert these\n",
    "    # to integer indices and look up the predicted characters\n",
    "    # so that we can return a final decrypted string\n",
    "    \n",
    "    # define our alphabet, with: ` ` = space, ~ = start, ! = stop\n",
    "    ALPHA_STA_STO = string.ascii_lowercase + \" ~!\"\n",
    "\n",
    "    decoded_str = []\n",
    "\n",
    "    # iterate over batch dimension\n",
    "    for seq_one_hot in decoded_one_hot.transpose(1, 0, 2):\n",
    "        # index into alphabet based on one-hot encodings for sequence\n",
    "        seq_text = \"\".join([ALPHA_STA_STO[i] for i in seq_one_hot.argmax(axis=-1)])\n",
    "        decoded_str.append(seq_text)\n",
    "    return decoded_str[0][1:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082eb151",
   "metadata": {},
   "source": [
    "Try playing around with the input message. Is the model better for shorter inputs, medium inputs, or longer inputs (warning: do not enter more than ~50 characters)?\n",
    "\n",
    "Why might our model not be very robust? In what ways was our training limited in variety?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65468de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_message = \"hello world how are you\"\n",
    "\n",
    "encrypted_message = cipher(input_message, key=\"clap\")\n",
    "\n",
    "out = decrypt_with_model(encrypted_message, loaded_model)\n",
    "\n",
    "print(f\" original message: {input_message}\")\n",
    "print(f\"encrypted message: {encrypted_message}\")\n",
    "print(f\"decrypted message: {out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6deafa04",
   "metadata": {},
   "source": [
    "Awesome, we can now decode any new encoded messages that we get (to a reasonable accuracy)!\n",
    "\n",
    "Finally, let's visualize all our attention heads.\n",
    "Fill in the following cell with the names attributes in your transformer, then run the subsequent cells to plot the attention heads.\n",
    "The code will plot the attention heads for the $0^\\text{th}$ sequence in the last batch processed by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edd3c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model to the trained `Transformer` model\n",
    "\n",
    "# set to the name of the list of `TransformerEncoder`s in the `Transformer` (as a string)\n",
    "enc_layers = \"enc_layers\"\n",
    "\n",
    "# set to the name of the list of `TransformerDecoder`s in the `Transformer` (as a string)\n",
    "dec_layers = \"dec_layers\"\n",
    "\n",
    "# set to the name of the self-attention `MultiHeadAttention` in the `TransformerEncoder` (as a string)\n",
    "enc_self_attn = \"self_attn\"\n",
    "\n",
    "# set to the name of the self-attention `MultiHeadAttention` in the `TransformerDecoder` (as a string)\n",
    "dec_self_attn = \"self_attn\"\n",
    "\n",
    "# set to the name of the encoder-decoder `MultiHeadAttention` in the `TransformerDecoder` (as a string)\n",
    "dec_enc_dec_attn = \"enc_dec_attn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320c32dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can query our model with an input encyrpted phrase and visualize the attention\n",
    "# heads of its various layers as it was decoding (decrypting) the output\n",
    "#\n",
    "# Specifically, these are the attention masks as its decoder produces its\n",
    "# final output\n",
    "\n",
    "query_phrase = \"coggies are the best\"  # Try changing this\n",
    "encrypted_message = cipher(query_phrase, key=\"clap\")\n",
    "decrypt_with_model(encrypted_message, loaded_model)\n",
    "\n",
    "\n",
    "enc_layers_ls = getattr(loaded_model, enc_layers)\n",
    "\n",
    "n_layers = len(enc_layers_ls)\n",
    "n_heads = len(getattr(enc_layers_ls[0], enc_self_attn).Wq)\n",
    "\n",
    "fig, ax = plt.subplots(n_layers, n_heads)\n",
    "ax = np.array(ax).reshape(n_layers, n_heads)\n",
    "\n",
    "for i, layer in enumerate(enc_layers_ls):\n",
    "    a_ij = getattr(layer, enc_self_attn).a_ij[:, 0] # shape: (T, h, t)\n",
    "    for j in range(a_ij.shape[1]):\n",
    "        head_aij = a_ij[:, j]\n",
    "        ax[i, j].imshow(head_aij);\n",
    "        if j == 1:\n",
    "            ax[i, j].set_title(f\"'{query_phrase}': enc self-attn layer - {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5885264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_layers_ls = getattr(loaded_model, dec_layers)\n",
    "\n",
    "n_layers = len(dec_layers_ls)\n",
    "n_heads = len(getattr(dec_layers_ls[0], dec_self_attn).Wq)\n",
    "\n",
    "fig, ax = plt.subplots(n_layers, n_heads)\n",
    "ax = np.array(ax).reshape(n_layers, n_heads)\n",
    "\n",
    "for i, layer in enumerate(dec_layers_ls):\n",
    "    a_ij = getattr(layer, dec_self_attn).a_ij[:, 0] # shape: (T, h, t)\n",
    "    for j in range(a_ij.shape[1]):\n",
    "        head_aij = a_ij[:, j]\n",
    "        ax[i, j].imshow(head_aij);\n",
    "        if j == 1:\n",
    "            ax[i, j].set_title(f\"'{query_phrase}': self-attn layer - {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9035333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = len(dec_layers_ls)\n",
    "n_heads = len(getattr(dec_layers_ls[0], dec_enc_dec_attn).Wq)\n",
    "\n",
    "fig, ax = plt.subplots(n_layers, n_heads)\n",
    "ax = np.array(ax).reshape(n_layers, n_heads)\n",
    "\n",
    "for i, layer in enumerate(dec_layers_ls):\n",
    "    a_ij = getattr(layer, dec_enc_dec_attn).a_ij[:, 0] # shape: (T, h, t)\n",
    "    for j in range(a_ij.shape[1]):\n",
    "        head_aij = a_ij[:, j]\n",
    "        ax[i, j].imshow(head_aij);\n",
    "        \n",
    "        if j == 1:\n",
    "            ax[i, j].set_title(f\"'{query_phrase}': enc-dec intra-attn layer - {i}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python",
   "notebook_metadata_filter": "nbsphinx,-kernelspec"
  },
  "nbsphinx": {
   "execute": "never"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
