{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "425b43c0",
   "metadata": {},
   "source": [
    "# Language Models\n",
    "\n",
    "This notebook explores using character-level n-grams to model language. We will learn how to train models by analyzing a body of text, and then use them for a fun task: generating new language in the style of a model.\n",
    "\n",
    "A brief review of Python strings: recall that strings are sequence-objects. This means that they behave like tuples: they support indexing/slicing and can be iterated over (which occurs character-by-character). Naturally, the length of a string reflects the number of characters in a string. Special characters, like the line-break `\\n`, is a single character.\n",
    "\n",
    "Python provides excellent, efficient string-methods. Initialize some string, `x = \"moo\"`. And then use tab-completing (`x.<tab>`) to view the list of built-in string methods. Be sure to make use of these whenever appropriate.\n",
    "Here are a couple of useful resources for working with strings:\n",
    "\n",
    "- [Basic tutorial for working with strings](https://docs.python.org/3/tutorial/introduction.html#strings)\n",
    "- [List of string methods](https://docs.python.org/3/library/stdtypes.html#string-methods) (It is a good idea to have this page open in a separate window; we will be making frequent use of the various string methods!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3cdd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d7ea96",
   "metadata": {},
   "source": [
    "We will need to make use of this `unzip` function. Try playing with this function to help build your intuition for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d1917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip(pairs):\n",
    "    \"\"\"\n",
    "    \"unzips\" of groups of items into separate tuples.\n",
    "    \n",
    "    Example: pairs = [(\"a\", 1), (\"b\", 2), ...] --> ((\"a\", \"b\", ...), (1, 2, ...))\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pairs : Iterable[Tuple[Any, ...]]\n",
    "        An iterable of the form ((a0, b0, c0, ...), (a1, b1, c1, ...))\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[Tuples[Any, ...], ...]\n",
    "       A tuple containing the \"unzipped\" contents of `pairs`; i.e. \n",
    "       ((a0, a1, ...), (b0, b1, ...), (c0, c1), ...)\n",
    "    \"\"\"\n",
    "    return tuple(zip(*pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd12907f",
   "metadata": {},
   "source": [
    "## Section 1: Most frequent letters (in English)\n",
    "\n",
    "Ever wonder why the bonus round of Wheel of Fortune automatically gives the contestant the letters R, S, T, L, N, and E (before letting them choose an additional 3 consonants and 1 vowel)? Or wonder why the letters J, X, Q, and Z are worth so much in Scrabble?\n",
    "\n",
    "Let's find out by analyzing a particular corpus of English text: Wikipedia. Thanks to Evan Jones for providing a clean text-only version of top Wikipedia articles (based on the Wikipedia \"release version\" project): http://www.evanjones.ca/software/wikipedia2text.html\n",
    "\n",
    "Load the entire contents of \"wikipedia2text-extracted.txt\" into a single string. Because some of these articles contain non-[ASCII](http://www.asciitable.com/) characters (for instance, some Chinese characters), you will need to open the file in binary-read mode: `mode='rb'`. Instead of reading in a typical string, this will read in a `bytes` object, which is simply your machine's memory-encoding for the characters. To make a long story short, you can simply call the method `decode` on this bytes-instance to decode the bytes into a familiar string. E.g. \n",
    "\n",
    "```python\n",
    "with open(path_to_wikipedia, \"rb\") as f:\n",
    "    # decoding the bytes into a string\n",
    "    wikipedia = f.read().decode()\n",
    "```\n",
    "After decoding from bytes to a string, **make all of the characters in the string lowercase** (you do not need a for-loop for this!)\n",
    "\n",
    "Confirm that there are over 63 million characters total, in the string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14dd1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cogworks_data.language import get_data_path\n",
    "\n",
    "path_to_wikipedia = get_data_path(\"wikipedia2text-extracted.txt\")\n",
    "\n",
    "with open(path_to_wikipedia, \"rb\") as f:\n",
    "    wikipedia = f.read().decode()  # <COGSTUB> read in and decode the file\n",
    "    wikipedia = wikipedia.lower()  # <COGSTUB> make all the characters lowercase\n",
    "\n",
    "# print out the number of characters in `wikipedia\n",
    "# <COGINST>    \n",
    "print(str(len(wikipedia)) + \" character(s)\")\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f289fcc4",
   "metadata": {},
   "source": [
    "Print out the first 500 characters of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a017f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <COGINST>\n",
    "print(wikipedia[:500])\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4038652",
   "metadata": {},
   "source": [
    "### 1.1 Count letters in text\n",
    "\n",
    "We want to count the occurence of every letter-character in our corpus. First, count up the occurence of every character (including punctuation and special characters, like `\\n`).\n",
    "\n",
    "Hint: Python has a `Counter` object in its `collections` module. You should be able to produce the count for every character in the file in one line. This should take roughly 5 seconds. Using a for-loop will take roughly 10x longer!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673af9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counter = Counter(wikipedia)  # <COGSTUB>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1342db8",
   "metadata": {},
   "source": [
    "Now we want a list of character-count tuples sorted in descending order of count. However, we want to filter out all of the non-letter characters. Thus our list should have a length of 26 (since we cast all of the letters to be lower-case):\n",
    "\n",
    "```\n",
    "[('e', 6091134),\n",
    " ('t', 4456786),\n",
    " ('a', 4365050),\n",
    " ('i', 3866921),\n",
    " ('n', 3740382),\n",
    " ('o', 3676394),\n",
    " ...\n",
    " ```\n",
    "\n",
    "Note that a simple way to access all lowercase letters in the English alphabet, other than typing out each character manually, is:\n",
    "\n",
    "```python\n",
    ">>> import string\n",
    ">>> string.ascii_lowercase\n",
    "'abcdefghijklmnopqrstuvwxyz'\n",
    "```\n",
    "\n",
    "If you made use of the `Counter` class, then there is a nice instance-method that you can make use of that takes care of sorting the character-count tuples so that the most-common occurences come first. You will need to filter out the non-letter characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f8af3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "set_of_letters = set(string.ascii_lowercase)  # <COGSTUB>\n",
    "\n",
    "# create a sorted list of tuples -- (char, count) -- sorted in descending count-order\n",
    "char_count_tuples = [(char, cnt) for char, cnt in counter.most_common() if char in set_of_letters]  # <COGSTUB>\n",
    "\n",
    "print(char_count_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e937a2c7",
   "metadata": {},
   "source": [
    "Create a variable called `freqs`, which is also a list of tuples, but instead containing character-count pairs, it contains character-frequency pairs. **Frequency is the ratio of the letter-count to the total number of letters (not characters) in the corpus**. It should end up looking something like:\n",
    "\n",
    "    [('e', 0.12081350306248849),\n",
    "     ('t', 0.088397321263964282),\n",
    "     ('a', 0.0865778000521603),\n",
    "     ...\n",
    "     ('q', 0.0010429083984244488)]\n",
    "     \n",
    "You should **not** iterate over the entire corpus to get the total letter count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725d812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = sum(cnt for _, cnt in char_count_tuples)  # <COGSTUB>\n",
    "freqs = [(char, cnt/total_count) for char, cnt in char_count_tuples]  # <COGSTUB>\n",
    "\n",
    "print(freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b516d98b",
   "metadata": {},
   "source": [
    "Confirm that the frequencies total to 1 (within numerical precision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70efe880",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(freq for _, freq in freqs)) # <COGLINE>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e17146",
   "metadata": {},
   "source": [
    "### 1.2 Plot letter frequency histogram\n",
    "\n",
    "Using \n",
    "```\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.bar(characters, frequencies, align='center', alpha=0.5)\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Frequency of Letters in English')\n",
    "```\n",
    "Look up the documentation for `ax.bar` (use shift-tab in your Jupyter notebook!) to plot a bar-graph of the characters and their frequencies. The x-axis should order the characters in decreasing frequency.\n",
    "\n",
    "Be sure to provide a title and a descriptive label for the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a20104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <COGINST>\n",
    "fig, ax = plt.subplots()\n",
    "characters, frequencies = unzip(freqs)\n",
    "\n",
    "ax.bar(characters, frequencies, align='center', alpha=0.5)\n",
    "\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Frequency of Letters in English')\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ab008b",
   "metadata": {},
   "source": [
    "## Section 2: Most frequent words (in English)\n",
    "\n",
    "Let's move up a level from characters and look at the distribution the English words. Returning to the full wikipedia corpus, which we have stored as a lower-cased string. Let's tokenize the corpus: separating the string into individual words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c428674e",
   "metadata": {},
   "source": [
    "### 2.1 (Simple) Tokenization\n",
    "\n",
    "For now we'll just apply a simple tokenization scheme: splitting our string on any whitespace (spaces, tabs, newlines, etc.) characters.\n",
    "\n",
    "Without using a for-loop, produce a list of the \"tokens\" from the corpus, and print out the first 10 tokens along with the total number of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed6812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <COGINST>\n",
    "tokens = wikipedia.split()\n",
    "print(len(tokens))\n",
    "print(tokens[:10])\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a3fdb2",
   "metadata": {},
   "source": [
    "As you did above with characters, count the occurences of all of the different words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82d94e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word -> count\n",
    "word_counts = Counter(tokens)  # <COGSTUB>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fece6d5",
   "metadata": {},
   "source": [
    "Finally, display the top 20 most-common words and their associated occurences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78726c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <COGINST>\n",
    "word_counts.most_common()[:20]\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b0e22e",
   "metadata": {},
   "source": [
    "What do you notice about this list? Do you see any people, places, or other distinguishing words in towards the top of this list? Can you discern anything about the content of the articles from these words (other than the fact that they predominantly contain English)? \n",
    "\n",
    "These abundant \"glue\" words, **referred to as \"stop words\" in NLP applications**, are ubiquitous to modern English. They provide the necessary glue needed for a coherent grammar, but do not provide actual meaning to text. Often, we will want to filter them out, so that we can get at the \"meaningful\" words in a corpus.\n",
    "\n",
    "See https://en.wikipedia.org/wiki/Stop_words for more details. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b34432",
   "metadata": {},
   "source": [
    "## 3 Creating an n-gram language model\n",
    "\n",
    "An **n-gram** is a contiguous sequence of n characters from a piece of text.\n",
    "For example `\"cat in\"` is a $6$-gram, `\"th\"` is a $2$-gram, etc..\n",
    "\n",
    "Inspired by Yoav Goldberg blog, which in turn was inspired by a [post from Andrej Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) that you saw in the Perspective on Machine Learning module, we will train our own n-gram language model.\n",
    "\n",
    "**Character-based n-gram language models aim to guess the next letter based on seeing the previous (n-1) letters.** (The assumption that the probability of seeing a letter only depends on a certain finite number of previous letters is an example of a Markov assumption. See https://en.wikipedia.org/wiki/Markov_property for more information.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ca41e3",
   "metadata": {},
   "source": [
    "Just as we did above, we will want to take a counter of `letter -> count` pairs, and convert them to `letter -> frequency` pairs. Using your code from earlier as reference, provide the body for the following function. Do **not** filter any characters this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73b3b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(counter):\n",
    "    \"\"\" Convert a `letter -> count` counter to a list \n",
    "    of (letter, frequency) pairs, sorted in descending order of \n",
    "    frequency.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    counter : collections.Counter\n",
    "        letter -> count\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[Tuple[str, float]]\n",
    "       A list of tuples: (letter, frequency) pairs in order\n",
    "       of descending-frequency\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from collections import Counter\n",
    "    >>> letter_count = Counter({\"a\": 1, \"b\": 3})\n",
    "    >>> letter_count\n",
    "    Counter({'a': 1, 'b': 3})\n",
    "\n",
    "    >>> normalize(letter_count)\n",
    "    [('b', 0.75), ('a', 0.25)]\n",
    "    \"\"\"\n",
    "    # <COGINST>\n",
    "    total = sum(counter.values())\n",
    "    return [(char, cnt/total) for char, cnt in counter.most_common()]\n",
    "    # </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18356ec",
   "metadata": {},
   "source": [
    "In the following, we will want to make some serious use of Python's `collections` module. Not only do we want to use the `Counter` class again, we also will want to use a [defaultdict](http://www.pythonlikeyoumeanit.com/Module2_EssentialsOfPython/DataStructures_III_Sets_and_More.html#Default-Dictionary). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35455eb",
   "metadata": {},
   "source": [
    "Now we'll create the function to actually analyze the n-grams (a length-n sequence of characters) that occur in a text:\n",
    " - For each distinct sequence of n-1 characters, we will keep a tally of the character that follows that sequence.\n",
    " - After counting is done, we'll normalize the char_count_tuples for each history to convert to frequencies, which we can interpret as probabilities.\n",
    " - At the beginning of a document we'll pad the text with a dummy character, \"~\". We will pre-pend $n-1$ of these to the beginning of our text.\n",
    "\n",
    "Our \"model\" simply keeps track of all length-(n-1) histories in a given text and the associated tallies of the various characters that follow each history.\n",
    "\n",
    "Thus our model can be thought of as two nested dictionaries:  \n",
    "\n",
    "```\n",
    "model: {(n-1) character history -> {nth-letter -> count}}\n",
    "```\n",
    "\n",
    "Here's an illustration of the process for analyzing the text \"cacao\" in terms of 3-grams:\n",
    "\n",
    "    history is \"~~\", next char is \"c\", increment model[\"~~\"][\"c\"]\n",
    "    history is \"~c\", next char is \"a\", increment model[\"~c\"][\"a\"]\n",
    "    history is \"ca\", next char is \"c\", increment model[\"ca\"][\"c\"]\n",
    "    history is \"ac\", next char is \"a\", increment model[\"ac\"][\"a\"]\n",
    "    history is \"ca\", next char is \"o\", increment model[\"ca\"][\"o\"]\n",
    "    history is \"ao\", next char does not exists. End process\n",
    "\n",
    "\n",
    "Here's an illustration of the process for analyzing the text \"cacao\" in terms of 4-grams:\n",
    "\n",
    "    history is \"~~~\", next char is \"c\", increment model[\"~~~\"][\"c\"]\n",
    "    history is \"~~c\", next char is \"a\", increment model[\"~~c\"][\"a\"]\n",
    "    history is \"~ca\", next char is \"c\", increment model[\"~ca\"][\"c\"]\n",
    "    history is \"cac\", next char is \"a\", increment model[\"cac\"][\"a\"]\n",
    "    history is \"aca\", next char is \"o\", increment model[\"aca\"][\"o\"]\n",
    "    history is \"cao\", next char does not exists. End process\n",
    "    \n",
    "\n",
    "\n",
    "So we will want to our model to be a default dictionary, whose default value is an empty `Counter` instance. Thus any time we encounter a new history, our model will create an empty counter for that history.\n",
    "\n",
    "I.e. `model = defaultdict(Counter)`. Thus `model[history]` will return the counter for that history. You can then update that counter with the character that comes after that history: `model[history][char] += 1`\n",
    "\n",
    "To \"train\" our language model is to simply populate its counters for all of the histories in a given text. Complete the following `train_lm` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4686dd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def train_lm(text, n):\n",
    "    \"\"\" Train character-based n-gram language model.\n",
    "        \n",
    "    This will learn: given a sequence of n-1 characters, what the probability\n",
    "    distribution is for the n-th character in the sequence.\n",
    "\n",
    "    For example if we train on the text:\n",
    "        text = \"cacao\"\n",
    "\n",
    "    Using a n-gram size of n=3, then the following dict would be returned.\n",
    "    See that we *normalize* each of the char_count_tuples for a given history\n",
    "\n",
    "        {'ac': [('a', 1.0)],\n",
    "         'ca': [('c', 0.5), ('o', 0.5)],\n",
    "         '~c': [('a', 1.0)],\n",
    "         '~~': [('c', 1.0)]}\n",
    "\n",
    "    Tildas (\"~\") are used for padding the history when necessary, so that it's \n",
    "    possible to estimate the probability of a seeing a character when there \n",
    "    aren't (n - 1) previous characters of history available.\n",
    "\n",
    "    So, according to this text we trained on, if you see the sequence 'ac',\n",
    "    our model predicts that the next character should be 'a' 100% of the time.\n",
    "\n",
    "    For generating the padding, recall that Python allows you to generate \n",
    "    repeated sequences easily: \n",
    "       `\"p\" * 4` returns `\"pppp\"`\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    text: str \n",
    "        A string (doesn't need to be lowercased).\n",
    "        \n",
    "    n: int\n",
    "        The length of n-gram to analyze.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, List[Tuple[str, float]]]\n",
    "        \n",
    "        {n-1 history -> [(letter, normalized count), ...]}\n",
    "        \n",
    "        A dictionary that maps histories (strings of length (n-1)) to lists of (char, prob) \n",
    "        pairs, where prob is the probability (i.e frequency) of char appearing after \n",
    "        that specific history.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> train_lm(\"cacao\", 3)\n",
    "    {'ac': [('a', 1.0)],\n",
    "     'ca': [('c', 0.5), ('o', 0.5)],\n",
    "     '~c': [('a', 1.0)],\n",
    "     '~~': [('c', 1.0)]}\n",
    "    \"\"\"\n",
    "\n",
    "    raw_lm = defaultdict(Counter) # <COGSTUB> history -> {char -> count}\n",
    "    history = \"~\" * (n - 1)  # <COGSTUB> length n - 1 history\n",
    "    \n",
    "    # count number of times characters appear following different histories\n",
    "    #\n",
    "    # for char in text ...\n",
    "    #    1. Increment language model's count, given current history and character\n",
    "    #    2. Update history\n",
    "\n",
    "    # <COGINST>\n",
    "    for char in text:\n",
    "        raw_lm[history][char] += 1\n",
    "        # slide history window to the right by one character\n",
    "        history = history[1:] + char\n",
    "    # </COGINST>\n",
    "    \n",
    "    # create the finalized language model – a dictionary with: history -> [(char, freq), ...]\n",
    "    lm = {history : normalize(counter) for history, counter in raw_lm.items()}  # <COGSTUB>\n",
    "    \n",
    "    return lm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebec8a67",
   "metadata": {},
   "source": [
    "Test train_lm() on \"cacao\", using n=3. You should get the same result as in the docstring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690583ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <COGINST>\n",
    "lm = train_lm(\"cacao\", 3)\n",
    "lm\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8285168",
   "metadata": {},
   "source": [
    "Now let's test our function on more serious example: a small snippet of text from \"The Cat in the Hat\" by Dr. Seuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dc4017",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"The sun did not shine, it was too wet to play, \n",
    "so we sat in the house all that cold, cold wet day. \n",
    "I sat there with Sally. We sat here we two \n",
    "and we said 'How we wish we had something to do.'\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68233f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <COGINST>\n",
    "lm3 = train_lm(text, 3)\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf66a3e",
   "metadata": {},
   "source": [
    "The following should show a probability of 1.0 for the letter \"T\", since that's the only starting letter that the model has ever seen (i.e., with no history, indicated by \"~~\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2074e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm3[\"~~\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7688be07",
   "metadata": {},
   "source": [
    "Similarly, the following should show a probability of 1.0 for \"h\", since that's the only letter the model has seen after a history of \"~T\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca12267",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm3[\"~T\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a1835b",
   "metadata": {},
   "source": [
    "This last example should give a probability distribution of the characters \"e\", \"a\", \" \", and \"i\", since those four characters all were observed to follow \"th\" in the text (with \"e\" occurring most often)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97731992",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm3[\"th\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b671bd",
   "metadata": {},
   "source": [
    "## 4 Generating text\n",
    "\n",
    "A fun thing to do with language models is to generate random text in the style of the model by generating letters using the learned probability distributions. \n",
    "\n",
    "First we'll create a function to randomly draw a single letter given a particular history, based on the probabilities stored in our language model.\n",
    "\n",
    "Hint: `np.random.choice(choices, p=probabilities)` will return an element from choices according to the specified probabilities. For example, `np.random.choice([\"a\", \"b\"], [0.25, 0.75])` will return an \"a\" 25% of the time and a \"b\" 75% of the time.\n",
    "\n",
    "Complete the following function. You will want to make use of the `unzip` function defined above to perform the following:\n",
    "\n",
    "\n",
    "```[(char0, prob0), (char1, prob1), ...] -> ((char0, char1, ...), (prob0, prob1, ...))```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590160a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_letter(lm, history):\n",
    "    \"\"\" Randomly picks letter according to probability distribution associated with \n",
    "    the specified history, as stored in your language model.\n",
    "\n",
    "    Note: returns dummy character \"~\" if history not found in model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lm: Dict[str, List[Tuple[str, float]]] \n",
    "        The n-gram language model. \n",
    "        I.e. the dictionary: history -> [(char, freq), ...]\n",
    "\n",
    "    history: str\n",
    "        A string of length (n-1) to use as context/history for generating \n",
    "        the next character.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The predicted character. '~' if history is not in language model.\n",
    "    \"\"\"\n",
    "    # <COGINST>\n",
    "    if not history in lm:\n",
    "        return \"~\"\n",
    "    letters, probs = unzip(lm[history])\n",
    "    i = np.random.choice(letters, p=probs)\n",
    "    return i\n",
    "    # </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ade6e1f",
   "metadata": {},
   "source": [
    "The following should generate \"e\", \"a\", \" \", or \"i\", since those are the only four characters that followed \"th\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997891a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw from `generate_letter` 100 times, keep only the unique outcomes\n",
    "set(generate_letter(lm3, \"th\") for i in range(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e34fa7",
   "metadata": {},
   "source": [
    "The following generates several possible next characters to get a sense of the distribution. \"e\" should appear more frequently than the other characters on average since it has a higher probability of following \"th\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bdf5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "[generate_letter(lm3, \"th\") for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d73ac3",
   "metadata": {},
   "source": [
    "Finally, we'll generate whole sequences of text according to the language model. The approach will be to start with no history, i.e. $n - 1$ `~`'s, generate a random letter, update the history, and repeat. In our example,\n",
    "\n",
    "    history \"~~\" will generate 'T' (since only one possibility)\n",
    "    history \"~T\" will generate 'h' (since only one possibility)\n",
    "    history \"Th\" will generate 'e' (since only one possibility)\n",
    "    history \"he\" could generate 'r' (out of multiple possibilities)\n",
    "    history \"er\" will generate 'e' (since only one possibility)\n",
    "\n",
    "and so on. The text generated so far would be \"There\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd92360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(lm, n, nletters=100):\n",
    "    \"\"\" Randomly generates `nletters` of text by drawing from \n",
    "    the probability distributions stored in a n-gram language model \n",
    "    `lm`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lm: Dict[str, List[Tuple[str, float]]]\n",
    "        The n-gram language model. \n",
    "        I.e. the dictionary: history -> [(char, freq), ...]\n",
    "    \n",
    "    n: int\n",
    "        Order of n-gram model.\n",
    "    \n",
    "    nletters: int\n",
    "        Number of letters to randomly generate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Model-generated text. Should contain `nletters` number of\n",
    "        generated characters. The pre-pended ~'s are not to be included. \n",
    "    \"\"\"\n",
    "    # <COGINST>\n",
    "    history = \"~\" * (n - 1)\n",
    "    text = []\n",
    "    for i in range(nletters):\n",
    "        c = generate_letter(lm, history)\n",
    "        text.append(c)\n",
    "        history = history[1:] + c\n",
    "    return \"\".join(text)  \n",
    "    # </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf5e08f",
   "metadata": {},
   "source": [
    "The following will generate 40 characters according to the 3-gram language model trained on the beginning of \"The Cat in the Hat\". It won't be very pretty... partly because of the short history length and also the small amount of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b83c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(lm3, 3, 40))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578b7c18",
   "metadata": {},
   "source": [
    "## 5 Generating \"Shakespeare\"\n",
    "\n",
    "Lastly, we'll have some fun trying to generate text in the style of Shakespeare.\n",
    "\n",
    "The next cell loads in Andrej Karpathy's shakespeare_input.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc836e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_shakespeare = get_data_path(\"shakespeare_input.txt\")\n",
    "\n",
    "with open(path_to_shakespeare, \"r\") as f:\n",
    "    shakespeare = f.read()\n",
    "print(str(len(shakespeare)) + \" character(s)\")\n",
    "chars = set(shakespeare)\n",
    "print(f\"'~' is a good pad character: {'~' not in chars}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe5f88e",
   "metadata": {},
   "source": [
    "Now experiment with training models for various values of n (e.g., 3, 5, and 11) and generate some text (maybe 500 characters or so). You should find the 3-gram model to be a very bad speller; the 5-gram model a better speller, but not making much sense; and the 11-gram model looking pretty good (which is amazing considering the simplicity of the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f209f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <COGINST>\n",
    "t0 = time.time()\n",
    "lm3 = train_lm(shakespeare, 3)\n",
    "t1 = time.time()\n",
    "print(\"elapsed = \" + str(t1 - t0) + \"s\")\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7785475",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(lm3, 3, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797bc38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <COGINST>\n",
    "t0 = time.time()\n",
    "lm5 = train_lm(shakespeare, 5)\n",
    "t1 = time.time()\n",
    "print(\"elapsed = \" + str(t1 - t0) + \"s\")\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4b6a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(lm5, 5, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7798637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <COGINST>\n",
    "t0 = time.time()\n",
    "lm11 = train_lm(shakespeare, 11)\n",
    "t1 = time.time()\n",
    "print(\"elapsed = \" + str(t1 - t0) + \"s\")\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0d66a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(lm11, 11, 500))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f468e24",
   "metadata": {},
   "source": [
    "Idea for bonus fun: find some other text on the Internet to train models on, e.g., song lyrics, books from a particular author, etc."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python",
   "notebook_metadata_filter": "nbsphinx,-kernelspec"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "nbsphinx": {
   "execute": "never"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
