{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e82a224a",
   "metadata": {},
   "source": [
    "# Notebook of Failure \n",
    "\n",
    "(aka \"Even Odd Problem with Feed Forward Networks\")\n",
    "\n",
    "In this notebook, we'll attempt to solve the following problem: Given an input vector of zeros and ones, predict `1` if the number of ones in the vector is even, and predict `0` if the number of ones in the vector is odd.\n",
    "\n",
    "Sounds easy enough, right? :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918cb387",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c795b761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mygrad as mg\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd617872",
   "metadata": {},
   "source": [
    "## Create even/odd dataset\n",
    "\n",
    "Create a function that returns numpy arrays `x` and `y`, where:\n",
    "* `x` is an NumPy array of shape $(N, T)$ where each element is 0 or 1 with equal probability\n",
    "* `y` is an NumPy array of shape $(N,)$ where $y_i$ is 1 if number of 1s in row $i$ is even and 0 otherwise\n",
    "\n",
    "$N$ is the size of our batch and $T$ is the length of each vector of zeros and ones.\n",
    "\n",
    "For example, `generate_dataset(4, 8)` produces four vectors, each containing a length-8 sequence of zeros and ones. For `x`, it might produce:\n",
    "```python\n",
    "[[1. 0. 1. 1. 0. 1. 1. 1.]\n",
    " [0. 1. 1. 1. 0. 1. 0. 1.]\n",
    " [0. 1. 0. 1. 0. 1. 0. 0.]\n",
    " [0. 1. 1. 0. 0. 1. 1. 1.]]\n",
    "```\n",
    "\n",
    "Then the corresponding truth, `y`, would be:\n",
    "\n",
    "```python\n",
    "[1 0 0 0]\n",
    "```\n",
    "Note that `y` needs to have dtype `np.int` to be used with MyGrad/MyNN cross entropy loss.\n",
    "\n",
    "Also note that it's possible for rows to appear more than once, but this gets less and less probable as the the number of columns increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c979d326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(N, T):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    N : int\n",
    "        The number of even/odd sequences to generate\n",
    "        \n",
    "    T : int\n",
    "        The length of each even/odd sequence\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[numpy.ndarray, numpy.ndarray], shapes=(N, T) & (T,)\n",
    "        A tuple containing:\n",
    "            - the batch of even/odd sequences; shape-(N, T)\n",
    "            - the integer label for each sequence: 1 if even, 0 if odd; shape-(N,)\n",
    "        \"\"\"\n",
    "    # <COGINST>\n",
    "    x = np.random.choice([0, 1], (N, T)).astype(np.float_)\n",
    "    y = (x.sum(axis=1) % 2 == 0).astype(np.int_)\n",
    "    return x, y\n",
    "    # </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000ca17d",
   "metadata": {},
   "source": [
    "Test your `generate_dataset`. Generate four sequences, each sequence being length-8. Manually tally each sequence and verify that each label is correct for the even/oddness of each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf0ea48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <COGINST>\n",
    "x, y = generate_dataset(4, 8)\n",
    "print(x)\n",
    "print(y)\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba799626",
   "metadata": {},
   "source": [
    "Now generate a dataset with 10000 rows and 32 columns, and split the data/labels evenly into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446feef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <COGINST>\n",
    "x, y = generate_dataset(10000, 32)\n",
    "num_train = int(len(y) * 0.5)\n",
    "xtrain = x[:num_train,:]\n",
    "ytrain = y[:num_train]\n",
    "xtest = x[num_train:,:]\n",
    "ytest = y[num_train:]\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdddc6f",
   "metadata": {},
   "source": [
    "Print out the shapes of your train/test sequence-data/labels to verify that they match with your expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f5035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <COGINST>\n",
    "print(xtrain.shape, ytrain.shape)\n",
    "xtrain[0,:], ytrain[0] # look at one training example sequence and corresponding label\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c110d4bf",
   "metadata": {},
   "source": [
    "## Define MyNN model\n",
    "\n",
    "Initially try using a two-layer neural network (one hidden layer of ReLU units):\n",
    "\n",
    "\\begin{equation}\n",
    "f(W_{1}, W_{2}, b_{1}, b_{2};\\;X) = \\mathrm{softmax}(\\mathrm{ReLU}(XW_{1} + b_{1})W_{2} + b_{2})\n",
    "\\end{equation}\n",
    "\n",
    "with cross entropy loss.\n",
    "\n",
    "For convenience, use MyGrad's `softmax_crossentropy` loss. This means that the MyNN model doesn't need to apply the softmax activation in its forward pass (because `softmax_crossentropy` will do it for us), i.e.,\n",
    "\n",
    "\\begin{equation}\n",
    "f(W_{1}, W_{2}, b_{1}, b_{2};\\;X) = \\mathrm{ReLU}(XW_{1} + b_{1})W_{2} + b_{2}\n",
    "\\end{equation}\n",
    "\n",
    "Ultimately, we will have our neural network produce **two** classification scores: $p_{odd}$ and $p_{even}$\n",
    "\n",
    "Use `from mygrad.nnet.initializers import normal`, and specify `normal` as the weight initializer for your dense layers. A layer size of 100 for your first layer, $W_{1}$, is a reasonable start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077a9b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mynn.layers.dense import dense\n",
    "from mynn.optimizers.sgd import SGD\n",
    "\n",
    "from mygrad.nnet.activations import relu\n",
    "from mygrad.nnet.initializers import normal\n",
    "from mygrad.nnet.losses import softmax_crossentropy\n",
    "\n",
    "# Define your MyNN-model\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, dim_in, num_hidden, dim_out):\n",
    "        # <COGINST>\n",
    "        self.dense1 = dense(dim_in, num_hidden, weight_initializer=normal)\n",
    "        self.dense2 = dense(num_hidden, dim_out, weight_initializer=normal)\n",
    "        # </COGINST>\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\" The model's forward pass functionality.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : Union[numpy.ndarray, mygrad.Tensor], shape=(N, T)\n",
    "            The batch of size-N.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        mygrad.Tensor, shape=(N, 2)\n",
    "            The model's predictions for each of the N pieces of data in the batch.\n",
    "        \"\"\"\n",
    "        # <COGINST>\n",
    "        return self.dense2(relu(self.dense1(x)))\n",
    "        # </COGINST>\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        \"\"\" A convenience function for getting all the parameters of our model. \"\"\"\n",
    "        # <COGINST>\n",
    "        return self.dense1.parameters + self.dense2.parameters\n",
    "        # </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b2ea48",
   "metadata": {},
   "source": [
    "Now initialize model and optimizer. Try using 100 units in hidden layer. For your optimizer, try the SGD with a `learning_rate` of 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad82a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <COGINST>\n",
    "num_hidden = 100\n",
    "model = Model(32, num_hidden, 2)\n",
    "optim = SGD(model.parameters, learning_rate=0.1)\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff64170",
   "metadata": {},
   "source": [
    "Now, create an accuracy function to compare your predictions to your labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f539d26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, truth):\n",
    "    \"\"\"\n",
    "    Returns the mean classification accuracy for a batch of predictions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    predictions : Union[numpy.ndarray, mg.Tensor], shape=(N, 2)\n",
    "        The scores for 2 classes, for a batch of N data points\n",
    "        \n",
    "    truth : numpy.ndarray, shape=(N,)\n",
    "        The true labels for each datum in the batch: each label is an\n",
    "        integer in [0, 1]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "    \"\"\"\n",
    "    return np.mean(np.argmax(predictions, axis=1) == truth) # <COGLINE>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b920237",
   "metadata": {},
   "source": [
    "Now set up a noggin plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11d2523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from noggin import create_plot\n",
    "plotter, fig, ax = create_plot(metrics=[\"loss\", \"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dcdd06",
   "metadata": {},
   "source": [
    "Time to train your model!. You can try setting `batch_size = 100` and training for 700 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c6d839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <COGINST>\n",
    "batch_size = 100\n",
    "num_epochs = 700\n",
    "# </COGINST>\n",
    "\n",
    "for epoch_cnt in range(num_epochs):\n",
    "    idxs = np.arange(len(xtrain))\n",
    "    np.random.shuffle(idxs)  \n",
    "    \n",
    "    for batch_cnt in range(0, len(xtrain) // batch_size):\n",
    "        # random batch of our training data\n",
    "        # <COGINST>\n",
    "        batch_indices = idxs[batch_cnt * batch_size : (batch_cnt + 1) * batch_size]\n",
    "        batch = xtrain[batch_indices]\n",
    "        truth = ytrain[batch_indices]\n",
    "        # </COGINST>\n",
    "\n",
    "        # perform the forward pass on our batch\n",
    "        prediction = model(batch) # <COGLINE>\n",
    "        \n",
    "        # calculate the loss\n",
    "        loss = softmax_crossentropy(prediction, truth) # <COGLINE>\n",
    "        \n",
    "        # perform backpropagation\n",
    "        loss.backward() # <COGLINE>\n",
    "        \n",
    "        # update your parameters\n",
    "        optim.step() # <COGLINE>\n",
    "        \n",
    "        # calculate the accuracy\n",
    "        acc = accuracy(prediction, truth) # <COGLINE>\n",
    "        \n",
    "        plotter.set_train_batch({\"loss\" : loss.item(), \"accuracy\" : acc}, batch_size=batch_size)\n",
    "    \n",
    "    for batch_cnt in range(0, len(xtest) // batch_size):\n",
    "        idxs = np.arange(len(xtest))\n",
    "        batch_indices = idxs[batch_cnt * batch_size : (batch_cnt + 1) * batch_size]\n",
    "        batch = xtest[batch_indices]\n",
    "        truth = ytest[batch_indices]\n",
    "        \n",
    "        with mg.no_autodiff:\n",
    "            prediction = model(batch)\n",
    "            acc = accuracy(prediction, truth)\n",
    "        \n",
    "        plotter.set_test_batch({\"accuracy\" : acc}, batch_size=batch_size)\n",
    "    \n",
    "    plotter.set_train_epoch()\n",
    "    plotter.set_test_epoch()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b895b754",
   "metadata": {},
   "source": [
    "Inspect the final train and test accuracy of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2ccd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(model(xtrain), ytrain), accuracy(model(xtest), ytest) # <COGLINE>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa94cef",
   "metadata": {},
   "source": [
    "Considering these accuracies, was the network successful or an utter failure?\n",
    "\n",
    "When using the suggested initial setup, it looks like this model is essentially memorizing the training set while learning absolutely nothing that's generalizable. Why do you think this is such a hard problem for this typical neural network architecture to learn? Would dropout on the input layer help? Could convolutions help? Discuss with your neighbors!\n",
    "\n",
    "Now try experimenting with training set size, number of columns in data, number of layers, layer sizes, activations functions, regularization (weight_decay), optimizer, etc. to try to improve performance on test set."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python",
   "notebook_metadata_filter": "nbsphinx,-kernelspec"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "nbsphinx": {
   "execute": "never"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
