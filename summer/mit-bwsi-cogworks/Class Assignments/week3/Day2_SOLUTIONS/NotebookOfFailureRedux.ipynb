{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaba7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from mynn.layers.dense import dense\n",
    "from mynn.optimizers.adam import Adam\n",
    "\n",
    "from mygrad.nnet.losses import softmax_crossentropy\n",
    "from mygrad.nnet.initializers import glorot_normal\n",
    "from mygrad.nnet.activations import relu\n",
    "\n",
    "import mygrad as mg\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc7d8f5",
   "metadata": {},
   "source": [
    "# Notebook of Failure Redux\n",
    "\n",
    "(aka sequence reversal problem with RNNs)\n",
    "\n",
    "As we previously saw, a RNN is natural model architecture to use when working with sequential data, such as text.\n",
    "And when working with language, a natural problem that arises is that of _translation_.\n",
    "In this notebook, we will explore a simple machine translation problem: reversing a sequence of digits.\n",
    "E.g.\n",
    "\n",
    "\\begin{align}\n",
    "[2, 4, 7, 1, 3] &\\longrightarrow [3, 1, 7, 4, 2]\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "Sounds easy enough, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756944c9",
   "metadata": {},
   "source": [
    "## Generating the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64976fe",
   "metadata": {},
   "source": [
    "As we did with classifying sequences that had identical halves, we will represent each digit in our sequence with a one-hot encoding.\n",
    "Unlike our previous sequences though, we will being to apply special start and end tokens to denote the beginning and end of our sequences.\n",
    "In particular, we will have a start token correspond to `10`, and and end token correspond to `11`.\n",
    "So when we one-hot encode our new 'vocabulary', we will have the following shape-(12,) vectors:\n",
    "\n",
    "\\begin{align}\n",
    "0 &\\longrightarrow [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \\\\\n",
    "&\\;\\;\\vdots \\\\\n",
    "9 &\\longrightarrow [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0] \\\\\n",
    "\\langle\\text{Start}\\rangle=10 &\\longrightarrow [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0] \\\\\n",
    "\\langle\\text{End}\\rangle=11 &\\longrightarrow [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] \\\\\n",
    "\\end{align}\n",
    "\n",
    "Note that we will not be making use of the `<START>` token in our data generation for this simple-cell RNN model, but we will use it when we create a \"sequence-to-sequence\" model in the coming notebook.\n",
    "\n",
    "For our simple RNN model, we will only need to utilize the end token when generating our data.\n",
    "So for a sequence `[4, 7, 5, 1]`, we will want to create a one-hot encoding of\n",
    "\n",
    "```python\n",
    "[[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]  # 4 \n",
    " [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]  # 7\n",
    " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  # 5\n",
    " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  # 1\n",
    " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]] # <End> (11)\n",
    "```\n",
    "\n",
    "and have target labels of `[1, 5, 7, 4, 11]`, which is the sequence in reverse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049fc1ea",
   "metadata": {},
   "source": [
    "Additionally, to make the learning process more efficient, we will want to make batches of sequences to pass to our model during training.\n",
    "\n",
    "For a batch-size $N$ an original digit-sequence length of $T-1$, we will have the input to our model will be a shape $(T, N, C=12)$ array of one-hot encodings.\n",
    "While this differs from how we have handled batches in the past, where the $0^\\text{th}$ dimension was associated with the $N$ samples in the batch, notice that we will ultimately compute the same dot products due to how NumPy broadcasts matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbb1110",
   "metadata": {},
   "outputs": [],
   "source": [
    "T, N, C = 2, 3, 4\n",
    "seq_then_batch = np.arange(T * N * C).reshape(T, N, C)\n",
    "batch_then_seq = seq_then_batch.transpose(1, 0, 2)\n",
    "\n",
    "arr = np.arange(C * 5).reshape(C, 5)\n",
    "\n",
    "# shape-(T, N, C) x shape-(C, 5) -> shape-(T, N, 5)\n",
    "seq_then_batch = seq_then_batch @ arr\n",
    "\n",
    "# shape-(N, T, C) x shape-(C, 5) -> shape-(N, T, 5)\n",
    "batch_then_seq = batch_then_seq @ arr\n",
    "\n",
    "np.all(seq_then_batch == batch_then_seq.transpose(1, 0, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639d9dd8",
   "metadata": {},
   "source": [
    "With the two considerations from above kept in mind, write a function `generate_batch` that takes in a minimum and maximum possible sequence length and a batch size that will:\n",
    "\n",
    "1. Randomly generate a digit-sequence length, $T-1$, between the provided minimum and maximum.\n",
    "2. Randomly generate a `batch_size`-number of sequences of digits. The batch-size is represented by $N$.\n",
    "3. Create a dtype-`float32` array of one-hot encodings of the sequences of digits.\n",
    "4. Append the one-hot encoding of the `<END>` token to each sequence in the batch.\n",
    "5. Creates target sequences of digits by simply reversing the randomly generated integers and appending an end token.\n",
    "6. Returns a tuple containing:\n",
    "    1. the one-hot encodings of the original batch of sequences\n",
    "    2. the reversed, target sequences (as sequences of integers, not one-hot encodings)\n",
    "    3. the original randomly generated batch of sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051a113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(seq_len_min=1, seq_len_max=20, batch_size=10):\n",
    "    \"\"\"\n",
    "    Generates a batch of sequences and corresponding one-hot encodings.\n",
    "    \n",
    "    Each digit-sequence has a length of T-1 (not including the <END> token),\n",
    "    where the value for T-1 is randomly generated.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    seq_len_min : int, optional (default=1)\n",
    "       The smallest permissable length of the pattern,\n",
    "       excluding start and end tokens\n",
    "       \n",
    "    seq_len_max : int, optional (default=20)\n",
    "       The longest permissable length of the pattern,\n",
    "       excluding start and end tokens\n",
    "       \n",
    "    batch_size : int, optional (default=10)\n",
    "        The number of sequences to generate\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray]\n",
    "        1. the one-hot encoded sequences, including an end token; shape-(T, N, 12)\n",
    "        2. the target sequences, including an end token; shape-(T, N)\n",
    "        3. the original sequences of digits; shape-(T-1, N)\n",
    "    \"\"\"\n",
    "\n",
    "    # Randomly generate a sequence length in [seq_len_min, seq_len_max]\n",
    "    # This is the value T-1\n",
    "    T_1 = np.random.randint(seq_len_min, seq_len_max + 1)  # <COGLINE>\n",
    "\n",
    "    # Randomly generate the shape-(T-1, N) sequence of digits [0-9]. This\n",
    "    # represents the N integer-valued sequences of length T-1 that our model\n",
    "    # will be translating\n",
    "    #\n",
    "    # E.g. If T-1 = 3 and N = 2, this might produce:\n",
    "    #\n",
    "    #  array([[3, 9],\n",
    "    #         [2, 5],\n",
    "    #         [9, 3]])\n",
    "    # seq-0: 3, 2, 9\n",
    "    # seq-1: 9, 5, 3\n",
    "    #\n",
    "    # Assign this to the variable `digits`\n",
    "    digits = np.random.randint(0, 10, (T_1, batch_size))  # <COGLINE>\n",
    "\n",
    "    # Create an array of zeros to fill with one-hot encodings of sequences.\n",
    "    # This should have a shape of (T, N, 12) and a dtype of float-32.\n",
    "    #\n",
    "    # The sequence length is T because the source sequence\n",
    "    # needs to include the end token (but not start)\n",
    "    #\n",
    "    # Call this array `one_hot_x`\n",
    "    one_hot_x = np.zeros((T_1 + 1, batch_size, 12), dtype=np.float32)  # <COGLINE>\n",
    "\n",
    "    # Use `digits` to populate `one_hot_x` with the appropriate one-hot encodings.\n",
    "    #\n",
    "    # You can achieve this either via advanced indexing:\n",
    "    # one_hot_x[np.arange(T_1).reshape(-1, 1), np.arange(batch_size), digits] = 1\n",
    "    # \n",
    "    # Or by using a for-loop:\n",
    "    # for ind in np.ndindex(digits.shape):\n",
    "    #     one_hot_x[ind + (digits[ind],)] = 1\n",
    "    # <COGINST>\n",
    "    one_hot_x[np.arange(T_1).reshape(-1, 1), np.arange(batch_size), digits] = 1\n",
    "    # </COGINST>\n",
    "\n",
    "    # In `one_hot_x`, at the last token position for all batches, set the <END> token's one-hot encoding\n",
    "    #\n",
    "    # Hint: Recall that `one_hot_x` has a shape of (T, N, 12).\n",
    "    # We want to access the T-th sequence entry and the 12th encoding position in this array\n",
    "    # for all N batches using basic indexing. And we want to set all elements in this selected\n",
    "    # subarray to 1.\n",
    "    one_hot_x[-1, :, -1] = 1  # <COGLINE>\n",
    "\n",
    "    # Create the \"target\" sequences, which are simply the reversed input sequences.\n",
    "    # This should be a shape-(T, N) array, where the T-th row is filled with `11`, \n",
    "    # which is the <END> token.\n",
    "    # <COGINST>\n",
    "    ends = np.full(batch_size, 11).reshape(1, -1)\n",
    "    y = np.concatenate([digits[::-1], ends], axis=0)\n",
    "    # </COGINST>\n",
    "\n",
    "    # Return the appropriate arrays - in accordance with the docstring.\n",
    "    return one_hot_x, y, digits  # <COGLINE>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75becdb",
   "metadata": {},
   "source": [
    "Run the following cell to test your `generate_batch` implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e53389",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, seq = generate_batch()\n",
    "\n",
    "assert np.all(y[-2::-1] == seq) # check target labels are original sequences reversed\n",
    "assert np.all(y[-1] == 11) # check target labels include end token\n",
    "assert np.all(np.argmax(x[:-1], axis=-1) == seq) # check correct one-hot encodings of sequences\n",
    "assert np.all(np.argmax(x[-1], axis=-1) == 11) # check correct one-hot encoding of end token\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04618205",
   "metadata": {},
   "source": [
    "## Defining our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d4ed25",
   "metadata": {},
   "source": [
    "Define your RNN class below.\n",
    "Because our data have dim-0 as the sequence dimension and dim-1 as the batch dimension, we can easily iterate over all of the sequences in our batch at once.\n",
    "Only slight modifications will need to be made in order to accomodate batches of data.\n",
    "\n",
    "As before, use `glorot_normal` to initialize your learnable weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766f3870",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \"\"\"Implements a simple-cell RNN that produces both outputs and hidden descriptors.\"\"\"\n",
    "    def __init__(self, dim_input, dim_recurrent, dim_output):\n",
    "        \"\"\" Initializes all layers needed for RNN\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dim_input: int \n",
    "            Dimensionality of data passed to RNN (C)\n",
    "        \n",
    "        dim_recurrent: int\n",
    "            Dimensionality of hidden state in RNN (D)\n",
    "        \n",
    "        dim_output: int\n",
    "            Dimensionality of output of RNN (K)\n",
    "        \"\"\"\n",
    "        # <COGINST>\n",
    "        self.fc_x2h = dense(dim_input, dim_recurrent, weight_initializer=glorot_normal)\n",
    "        self.fc_h2h = dense(dim_recurrent, dim_recurrent, weight_initializer=glorot_normal, bias=False)\n",
    "        self.fc_h2y = dense(dim_recurrent, dim_output, weight_initializer=glorot_normal)\n",
    "        # </COGINST>\n",
    "    \n",
    "    \n",
    "    def __call__(self, x, h=None):\n",
    "        \"\"\" Performs the full forward pass for the RNN.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x: Union[numpy.ndarray, mygrad.Tensor], shape=(T, N, C)\n",
    "            The one-hot encodings for the each sequence in the batch\n",
    "        \n",
    "        h: Optional[Union[numpy.ndarray, mygrad.Tensor]], shape=(1, N, D)\n",
    "            An optional initial hidden dimension state h_0.\n",
    "            If None, initialize an array of zeros.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[y, h]\n",
    "            y: mygrad.Tensor, shape=(T, N, K)\n",
    "                The final classification scores for each RNN step\n",
    "            h: mygrad.Tensor, shape=(T, N, D)\n",
    "                The hidden states computed at each RNN step, excluding the initial state h_0\n",
    "        \"\"\"\n",
    "        # <COGINST>\n",
    "        # the only change needed to accomodate batches of data is adding `x.shape[1]`\n",
    "        # as dim-1 of the initial h_t if none is provided\n",
    "        h_t = np.zeros((1, x.shape[1], self.fc_h2h.weight.shape[0]), dtype=np.float32) if h is None else h\n",
    "        h = []\n",
    "        \n",
    "        for x_t in x:\n",
    "            h_t = relu(self.fc_x2h(x_t[np.newaxis]) + self.fc_h2h(h_t))\n",
    "            h.append(h_t)\n",
    "        \n",
    "        h = mg.concatenate(h, axis=0)\n",
    "        \n",
    "        return self.fc_h2y(h), h\n",
    "        # </COGINST>\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        \"\"\" A convenience function for getting all the parameters of our model.\n",
    "        \n",
    "        This can be accessed as an attribute, via `model.parameters` \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[Tensor, ...]\n",
    "            A tuple containing all of the learnable parameters for our model\n",
    "        \"\"\"\n",
    "        return self.fc_x2h.parameters + self.fc_h2h.parameters + self.fc_h2y.parameters # <COGLINE>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75b622d",
   "metadata": {},
   "source": [
    "Create a noggin plot below to track the loss and accuracy of the model as it trains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8c1b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from noggin import create_plot\n",
    "plotter, fig, ax = create_plot([\"loss\", \"accuracy\"]) # <COGLINE>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5057f6ea",
   "metadata": {},
   "source": [
    "Initialize the RNN and an Adam optimizer.\n",
    "As we are trying to predict a sequence of digits, each output $y_t$ should contain a classification score for each possible predicted token â€“ all digits and the start and end tokens.\n",
    "Thus the dimension of the output $K$ must be the same as the dimension of the input $C$.\n",
    "\n",
    "As before, a `dim_recurrent` of $D=50$ and the default Adam parameters are reasonable starting choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d524f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <COGINST>\n",
    "model = RNN(dim_input=12, dim_recurrent=50, dim_output=12)\n",
    "optimizer = Adam(model.parameters)\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d6a871",
   "metadata": {},
   "source": [
    "Write the training loop below.\n",
    "Use the default `batch_size` of $10$ to start, with `seq_len_min=1` and `seq_len_max=20`.\n",
    "Train for `20000` iterations.\n",
    "\n",
    "As before, we will use `softmax_crossentropy` as our loss function.\n",
    "However, MyGrad's implementation of `softmax_crossentropy` requires the first input `x` to be shape `(N, C)` and the second input `y_true` to be shape `(N,)`.\n",
    "Since we are working with batchs of sequences, the output of our model will be shape `(T, N, C)` and our truth values will be shape `(T, N)`.\n",
    "To make our inputs compatible with `softmax_crossentropy`, we can reshape `x` and `y_true` to `(T * N, C)` and `(T * N,)`, respectively.\n",
    "This will allow us to compute and average the per-token softmax-crossentropy loss, without having to redefine our own version of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf3f4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <COGINST>\n",
    "batch_size=10\n",
    "plot_every=500\n",
    "\n",
    "for k in range(20000):\n",
    "    x, target, sequence = generate_batch(batch_size=batch_size)\n",
    "\n",
    "    # unlike the simple cell RNN used for classification,\n",
    "    # we want to keep the full output for our loss\n",
    "    # as we want to translate each digit to a new one\n",
    "    output, _ = model(x)\n",
    "    \n",
    "    loss = softmax_crossentropy(output.reshape(-1, 12), target.reshape(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    acc = np.mean(np.argmax(output, axis=-1) == target)\n",
    "\n",
    "    plotter.set_train_batch({\"loss\":loss.item(), \"accuracy\":acc}, batch_size=batch_size, plot=False)\n",
    "    \n",
    "    if k % plot_every == 0 and k > 0:\n",
    "        plotter.set_train_epoch()\n",
    "# </COGINST>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d0a873",
   "metadata": {},
   "source": [
    "Run the following code to evaluate your model. What do you see and are the results surprising to you? Discuss with a neighbor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6288e311",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_total = defaultdict(int)\n",
    "length_correct = defaultdict(int)\n",
    "\n",
    "with mg.no_autodiff:\n",
    "    for i in range(100000):\n",
    "        if i % 10000 == 0:\n",
    "            print(f\"i = {i}\")\n",
    "        x, target, sequence = generate_batch(1, 20, 1)\n",
    "\n",
    "        output, _ = model(x)\n",
    "\n",
    "        length_total[sequence.size] += 1\n",
    "        if np.all(np.argmax(output, axis=-1) == target):\n",
    "            length_correct[sequence.size] += 1\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x, y = [], []\n",
    "for i in range(1, 20):\n",
    "    x.append(i)\n",
    "    y.append(length_correct[i] / length_total[i])\n",
    "ax.plot(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bf6f01",
   "metadata": {},
   "source": [
    "While the model appears to do very well for length-1 sequences, it seems to fail quite miserably on any larger sequence.\n",
    "\n",
    "Let's think about why this might be for a little.\n",
    "Because the RNN sequentially iterates over the data, it can only ever have information about preceding tokens when making a predication.\n",
    "But in our toy problem, we need to have information about _succeeding_ tokens in order to make informed predictions.\n",
    "\n",
    "For length-1 sequences, the model simply needs to return $\\vec{x}_1$ as $\\vec{y}_1$, since the reverse of a length-1 sequence is itself.\n",
    "The RNN can do this quite easily, as every time it predicts a $\\vec{y}_t$, it has information about $\\vec{x}_t$ as well as about all of $(\\vec{x}_i)_{i=0}^{t-1}$ via the hidden state.\n",
    "\n",
    "However for a length-2 sequence, when predicting $\\vec{y}_0$, the RNN needs to know $\\vec{x}_1$.\n",
    "The sequential structure of the RNN simply means this is impossible!\n",
    "While the RNN could successfully predict the second half of the sequence, $\\vec{y}_1$ (because it now knows about the first half of the sequence, $\\vec{x}_0$) it means that the roughly $0.1\\%$ accuracy for length-2 sequences is no better than simply guessing the first digit!\n",
    "\n",
    "Simply put, the fact that the RNN processes sequences from sequentially from start to finish means that it is incapable of solving our toy problem!\n",
    "It also means that it is an ill-suited model for real translation applications, where word order is not necessarily the same in two languages.\n",
    "In the next notebook, we will look at how we can use RNNs as a building block for a more suitable type of model for translation."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python",
   "notebook_metadata_filter": "nbsphinx,-kernelspec"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "nbsphinx": {
   "execute": "never"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
